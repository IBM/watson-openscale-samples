{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "cc9cf9b7-d191-4afe-b437-0b0532f51e08",
            "metadata": {
                "id": "a17834dc-5eb2-4b21-b78a-ce1f0bbbeca3"
            },
            "source": [
                "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7544829a-7251-40bc-bff1-ead8bf5af668",
            "metadata": {
                "id": "b0f41ee3-8baa-4417-a57f-69d5a7541aba"
            },
            "source": [
                "# IBM Watson OpenScale - Onboard models for monitoring using scored training data table and a sample csv"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "01a7207a-42da-48ad-bdf6-f655d4fd9dea",
            "metadata": {
                "id": "ac7a3b74-e05e-4827-a95f-2e52ecd1affb"
            },
            "source": [
                "This notebook must be run in the Python 3.10 runtime environment. It requires Watson OpenScale service credentials.\n",
                "\n",
                "The notebook demonstrates how to onboard a model (which stores its runtime data in a remote DB2 database) for monitoring in IBM Watson OpenScale. Use the notebook to enable quality, drift v2, fairness and explainability monitoring. Before you can run the notebook, you must have the following resources:\n",
                "\n",
                "1. Sample CSV file\n",
                "2. Scored training data table (existing) in IBM DB2\n",
                "3. Feedback, Payload, Explanations Queue and Result tables details (either existing or to be created) in an IBM DB2."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "198fb3f4-feb6-415f-8fc8-49af129fa066",
            "metadata": {
                "id": "222b2373-4e44-48ee-8a3d-b21e66f9562a"
            },
            "source": [
                "## Contents\n",
                "\n",
                "1. [Setup](#setup)\n",
                "2. [Provide path to sample csv file containing training data](#path-to-csv)\n",
                "3. [Provide Storage Details](#backend-storage)\n",
                "4. [Provide Table Details](#table-details)\n",
                "5. [Provide model details](#model_details)\n",
                "2. [Provide Spark Compute Engine Details](#spark)\n",
                "5. [Connect to IBM Watson OpenScale Instance](#connect-openscale)\n",
                "6. [Connect service provider in IBM Watson OpenScale Instance](#create-service-provider)\n",
                "7. [Onboard model for monitoring in IBM Watson OpenScale Instance](#create-subscription)\n",
                "9. [Enable services to monitor model](#enable-monitors)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9467a5e4-fbe5-4ab6-86c9-c7490fdcb4dc",
            "metadata": {
                "id": "0b971e5f-62b9-4705-9a02-17b471eb3876"
            },
            "source": [
                "## Setup <a name=\"setup\"></a>\n",
                "\n",
                "### Installing Required Libraries\n",
                "\n",
                "First import some of the packages you need to use. After you finish installing the following software packages, restart the kernel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "612f0b22-fd9a-47af-a113-9bfff2fb37af",
            "metadata": {
                "id": "a6ae2496-8c4b-4412-a480-e12d888d6426"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "env: PIP_DISABLE_PIP_VERSION_CHECK=1\n"
                    ]
                }
            ],
            "source": [
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "%env PIP_DISABLE_PIP_VERSION_CHECK=1\n",
                "\n",
                "# Note: Restart kernel after the dependencies are installed\n",
                "%pip install --upgrade ibm-watson-openscale\n",
                "%pip install \"ibm_wos_utils~=5.3.0\"\n",
                "\n",
                "# When this notebook is to be run on RT 24.1 environment, \n",
                "# uncomment the below line and execute.\n",
                "# !pip install --upgrade \"matplotlib~=3.10.1\" transformers numexpr bottleneck opencv-python transformers numexpr bottleneck opencv-python --no-cache | tail -n 1\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b894c7ab-3267-408e-b5d7-5d7c2f412505",
            "metadata": {
                "id": "78dd67b4-60a9-41c0-a522-53d6b2bd6392"
            },
            "source": [
                "## Provide path to sample csv file containing model input and output including label column <a name=\"path-to-csv\">\n",
                "This csv file is required to understand model input and output columns and their data-types. Provide path location of csv file here.\n",
                "\n",
                "Please note if you are executing this notebook in IBM Watson Studio, first upload the csv file to project and use provided code snippet to download it to local directory of this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "540bd8e7-4f1a-47fe-ad6c-7643ee973a5e",
            "metadata": {
                "id": "9eb0bc3a-370c-4c20-bd6d-ffe7d601b836"
            },
            "outputs": [],
            "source": [
                "# # Download \"sample_csv\" from project to local directory\n",
                "# from ibm_watson_studio_lib import access_project_or_space\n",
                "# wslib = access_project_or_space()\n",
                "# wslib.download_file(\"sample_csv\")\n",
                "sample_csv = \"\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "364f7949",
            "metadata": {
                "id": "06b73bdf-b06d-41f0-8884-ec6dfd0e0ba3"
            },
            "source": [
                "## Provide Backend Storage Details <a name=\"backend-storage\"></a>\n",
                "\n",
                "IBM Watson OpenScale services monitors models by analyzing runtime data, i.e., the data model is making predictions on. To do this analysis, most of the services require access to this runtime data (also called payload data). In addition, some of the services may require access to manually labelled runtime data (also called feedback data). Hence, user needs to store such data in some backend storage and connect this storage to IBM Watson OpenScale.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6d707beb-b2f8-48a5-a2b5-11823371ae32",
            "metadata": {
                "id": "2fe7cf0c-c067-4d15-a602-852acf855a04"
            },
            "source": [
                "### Provide DB2 connection details\n",
                "\n",
                "| Parameter | Description | Possible Value(s) |\n",
                "| :- | :- | :- |\n",
                "| type | Describes the type of storage being used. For DB2, this must be set to `jdbc`. | `jdbc` |\n",
                "| jdbc_url | Connection string for jdbc. Example: `jdbc:db2://jdbc_host:jdbc_port/database_name` | |\n",
                "| jdbc_driver | Class name of the JDBC driver to use to connect. Example: for DB2 use `com.ibm.db2.jcc.DB2Driver` | |\n",
                "| use_ssl | Boolean value to indicate whether to use SSL while connecting | `True` or `False` |\n",
                "| certificate | SSL Certificate [Base64 encoded string] of the JDBC Connection. Ignored if `use_ssl` is `False`. | |\n",
                "| location_type | Identifies the type of location for connection to use. For DB2, this must be set to `jdbc`. | `jdbc` |\n",
                "| username | Username of the JDBC Connection | |\n",
                "| password | Password of the JDBC Connection | |\n",
                "| database | Name of database hosting training data table | |\n",
                "| schema | Name of schema hosting training data table | |\n",
                "| table | Name of training data table | |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "74ea7fef-f46b-477a-ba4e-7395742294f0",
            "metadata": {
                "id": "8beac01d-5b09-4619-809f-d7319e8a1a10"
            },
            "outputs": [],
            "source": [
                "datawarehouse_details = {\n",
                "    \"type\": \"jdbc\",\n",
                "    \"connection\": {\n",
                "        \"jdbc_url\": \"\",\n",
                "        \"jdbc_driver\": \"\",\n",
                "        \"use_ssl\": \"\",\n",
                "        \"certificate\": \"\"\n",
                "    },\n",
                "    \"credentials\": {\n",
                "        \"username\": \"\",\n",
                "        \"password\": \"\"\n",
                "    }\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "69e1d363-94e9-4588-b4d9-bdacb14c331c",
            "metadata": {
                "id": "5d7671ff-9e09-4e09-ad9b-ea5077e0c7a3"
            },
            "source": [
                "## Provide details of different tables<a name=\"table-details\"></a>\n",
                "\n",
                "IBM Watson OpenScale services require different tables to perform their analysis. Depending on which services you have enabled, provide details of the corresponding tables.\n",
                "Tables are:\n",
                "\n",
                "| Table | Description |\n",
                "| :- | :- |\n",
                "| Payload Table | Hosts the runtime data predicted by model. Required for detecting fairness and drift v2 in runtime data. |\n",
                "| Feedback Table | Hosts the manually labelled runtime data (also called feedback data) predicted by model. Required for tracking quality of monitor by analyzing feedback data. |\n",
                "| Explain Queue Table | Hosts the data for which explanations are required to be generated. This can be same as payload table.|\n",
                "| Explain Results Table | Hosts the explanations generated for records in explain queue table. |\n",
                "| Scored Training Data Table | Contains the details of table containing scored training data. If you dont have this table available, Please refer to [this notebook](https://github.ibm.com/aiopenscale/api-client-utils/blob/master/notebooks/batch/4.6/jdbc/common_configuration_notebook_simplified_jdbc.ipynb) for creating Scored Training Data Table. Scored training data table should be available in the DATABASE. |\n",
                "\n",
                "For each of the table, following information is required:\n",
                "\n",
                "| Parameter | Description |\n",
                "| :- | :- |\n",
                "| database | Name of the database hosting the schema. |\n",
                "| schema | Name of the schema hosting the table. |\n",
                "| table | Name of the table. |\n",
                "| auto_create | Boolean value identifying if the table already exists or has to be created via IBM Watson OpenScale. |\n",
                "| partition_column | The column to help Spark read and write data using multiple workers in your JDBC storage. This will help improve the performance of your Spark jobs. |\n",
                "| num_partition | An integer value which defines maximum number of partitions that Spark can divide the data into. In JDBC, it also means the maximum number of connections that Spark can make to the JDBC store for reading/writing data. |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "f84261cb-2e3a-4b5d-9d18-ab56e5790497",
            "metadata": {
                "id": "18567449-d84a-423f-a3bc-5c7bde3e02f0"
            },
            "outputs": [],
            "source": [
                "DATABASE_NAME=\"\"\n",
                "SCHEMA_NAME=\"\"\n",
                "\n",
                "# Scored trainin data table information \n",
                "scored_training_data_table = { \n",
                "    \"data\": {\n",
                "        \"auto_create\": False, #set it to False if table already exists\n",
                "        \"database\": DATABASE_NAME,\n",
                "        \"schema\": SCHEMA_NAME,\n",
                "        \"table\": \"\"\n",
                "    },\n",
                "    \"parameters\":{\n",
                "        \"partition_column\": \"\",\n",
                "        \"num_partitions\": \"\"\n",
                "    }\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de8cc192-5e8d-4f74-9e7b-aff4e24509df",
            "metadata": {
                "id": "62f68cf4-4399-4598-84a3-8822040a2efb"
            },
            "outputs": [],
            "source": [
                "### Recommended number of partitions calculation\n",
                "# num_partitions_recommended = 12\n",
                "\n",
                "# if spark_parameters:\n",
                "#     executors = spark_parameters.get(\"max_num_executors\", 2)\n",
                "#     cores = spark_parameters.get(\"executor_cores\", 2)\n",
                "#     num_partitions_recommended = 2 * executors * cores\n",
                "    \n",
                "# print(\"{} is the recommended value for number of partitions in your data. \\\n",
                "# Please change this value as per your data.\".format(num_partitions_recommended))\n",
                "\n",
                "# Payload table information\n",
                "payload_table = {\n",
                "    \"data\": {\n",
                "        \"auto_create\": True, #set it to False if table already exists\n",
                "        \"database\": DATABASE_NAME,\n",
                "        \"schema\": SCHEMA_NAME,\n",
                "        \"table\": \"\"\n",
                "    },\n",
                "    \"parameters\":{\n",
                "        \"partition_column\": \"\",\n",
                "        \"num_partitions\": \"\"\n",
                "    }\n",
                "}\n",
                "\n",
                "# Feedback table information\n",
                "feedback_table = {\n",
                "    \"data\": {\n",
                "        \"auto_create\": True, #set it to False if table already exists\n",
                "        \"database\": DATABASE_NAME,\n",
                "        \"schema\": SCHEMA_NAME,\n",
                "        \"table\": \"\"\n",
                "    },\n",
                "    \"parameters\":{\n",
                "        \"partition_column\": \"\",\n",
                "        \"num_partitions\": \"\"\n",
                "    }\n",
                "}\n",
                "\n",
                "# The below tables are required by monitors\n",
                "\n",
                "#Explanation Result table\n",
                "#Set this table information if Explain is enabled\n",
                "explain_result_table = {\n",
                "    \"data\": {\n",
                "        \"auto_create\": True, #set it to False if table already exists\n",
                "        \"database\": DATABASE_NAME,\n",
                "        \"schema\": SCHEMA_NAME,\n",
                "        \"table\": \"\"\n",
                "    }\n",
                "}\n",
                "\n",
                "#Explanation Queue table\n",
                "#Set this table information if Explain is enabled\n",
                "explain_queue_table = {\n",
                "    \"data\": {\n",
                "        \"auto_create\": True, #set it to False if table already exists\n",
                "        \"database\": DATABASE_NAME,\n",
                "        \"schema\": SCHEMA_NAME,\n",
                "        \"table\": \"\"\n",
                "    },\n",
                "    \"parameters\":{\n",
                "        \"partition_column\": \"\",\n",
                "        \"num_partitions\": \"\"\n",
                "    }\n",
                "    \n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddaf3f64-9a79-4381-be3b-ef1ddafa459b",
            "metadata": {
                "id": "6edceec7-5b85-49be-a6fa-8f3790afb9a6"
            },
            "source": [
                "## Provide Model Details <a name=\"model-details\"></a>\n",
                "\n",
                "| Parameter | Description | Possible Value(s) |\n",
                "| :- | :- | :- |\n",
                "| label_column | The column which contains the target field (also known as label column or the class label). | |\n",
                "| model_type | Enumeration classifying if your model is a binary or a multi-class classifier or a regressor. | `binary`, `multiclass`, `regression` |\n",
                "| prediction | The column containing the model output. This should be of the same data type as the label column. | |\n",
                "| probability | The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models. | |\n",
                "| url | scoring url for the deployed model.| |\n",
                "| token | scoring token for the deployed model. This is required only for Azure ML studio model | |\n",
                "| feature_columns | Columns identified as features by model. If user is not providing this, it will be inferred from the input csv file. | A list of column names, `None` |\n",
                "| categorical_columns | Feature columns identified as categorical by model. If user is not providing this, it will be inferred from the input csv file. | A list of column names,  `None` |\n",
                "\n",
                "## Select IBM Watson OpenScale services\n",
                "\n",
                "| Parameter | Description | Possible Value(s) |\n",
                "| :- | :- | :- |\n",
                "| enable_quality | Boolean value to allow generation of common configuration details needed if quality alone is selected | `True` or `False` |\n",
                "| enable_fairness | Boolean value to allow generation of fairness specific data distribution needed for configuration | `True` or `False` |\n",
                "| enable_drift_v2 | Boolean value to allow generation of Drift v2 Archive. | `True` or `False` |\n",
                "| enable_explainability | Boolean value to allow generation of explainability configuration and perturbations | `True` or `False` |\n",
                "| parameters | Provide the parameters for a monitor that needs to get enabled, | |\n",
                "| thresholds | Provide the thresholds for faireness and quality monitor if that monitor needs to get enabled | |\n",
                "| enable_online_learning | It is set to `True` to generate the stats and scored perturbations online. |`True` or `False` |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1673214e-bae5-4444-b574-90016172d60d",
            "metadata": {
                "id": "823f5ddf-89fe-46fc-a8d2-bdca42312a88"
            },
            "outputs": [],
            "source": [
                "model_info = {\n",
                "    \"model_type\": \"\",\n",
                "    \"label_column\": \"\",\n",
                "    \"prediction\": \"\",\n",
                "    \"probability\": \"\",\n",
                "    \"feature_columns\": [\"\"],\n",
                "    \"categorical_columns\": [\"\"],\n",
                "    \"scoring\":{\n",
                "        \"url\":\"\",\n",
                "        \"token\":\"\"\n",
                "    }\n",
                "}\n",
                "\n",
                "monitors_config = {\n",
                "    \"fairness_configuration\": {\n",
                "        \"enabled\": True,\n",
                "        \"parameters\":{\n",
                "        },\n",
                "        \"thresholds\": [\n",
                "        ]\n",
                "    },\n",
                "    \"quality_configuration\": {\n",
                "        \"enabled\": True,\n",
                "        \"parameters\" : {\n",
                "        },\n",
                "        \"thresholds\" : [\n",
                "        ]\n",
                "    },\n",
                "    \"explainability_configuration\":{\n",
                "        \"enabled\": True,\n",
                "        \"parameters\":{\n",
                "            \"enable_online_learning\": True,\n",
                "            # Set below params to enable global explanation. Available from Cloud Pak for Data 4.6.4 onwards.\n",
                "            #\"global_explanation\": {\n",
                "            #    \"enabled\": True,\n",
                "            #    \"explanation_method\": \"lime\", # The explanation method\n",
                "            #    \"training_data_sample_size\": 1000, # [Optional] The sample size of records to be used for generating training data global explanation. If not specified entire training data is used.\n",
                "            #    \"sample_size\": 1000, # [Optional] The sample size of records to be used for generating payload data global explanation. If not specified entire data in the payload window is used.\n",
                "            #}\n",
                "        }\n",
                "    },\n",
                "    \"drift_v2_configuration\":{\n",
                "        \"enabled\": True,\n",
                "        \"parameters\": {\n",
                "            \"train_archive\": True,\n",
                "            \"feature_importance\": [], # required field\n",
                "            \"most_important_features\":[],\n",
                "            \"important_input_metadata_columns\": [] # <- Add this if input metadata drift to be calculated and meta columns are available\n",
                "        }\n",
                "    }\n",
                "    \n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41ff7e12-eaf1-4dd3-9f30-830f836aef48",
            "metadata": {
                "id": "725151c7-273a-44e6-821e-1254a7d2a3e3"
            },
            "source": [
                "## Provide Spark Connection Details <a name=\"spark\"></a>\n",
                "\n",
                "To generate configuration for monitoring models in IBM Watson OpenScale, a spark compute engine is required. It can be either IBM Analytics Engine or your own Spark Cluster. Provide details of any one of them in this section.\n",
                "\n",
                "Please note, if you are using your own Spark cluster, checkout IBM Watson OpenScale documentation on how to setup spark manager API to enable interface for use with IBM Watson OpenScale services.\n",
                "\n",
                "### Parameters for IBM Analytics Engine\n",
                "If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
                "\n",
                "| Parameter | Description | Possible Value(s) |\n",
                "| :- | :- | :- |\n",
                "| display_name | Display Name of the Spark instance in IBM Analytics Engine | |\n",
                "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For IBM IAE, this must be set to `cpd_iae`. | `cpd_iae` |\n",
                "| endpoint | Spark Jobs Endpoint for IBM Analytics Engine | |\n",
                "| volume | IBM Cloud Pak for Data storage volume name | |\n",
                "| username | IBM Cloud Pak for Data username | |\n",
                "| apikey | IBM Cloud Pak for Data API key | |\n",
                "\n",
                "### Parameters for Remote Spark Cluster\n",
                "If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
                "\n",
                "| Parameter | Description | Possible Value(s) |\n",
                "| :- | :- | :- |\n",
                "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For Remote Spark, this must be set to `custom`. | `custom` |\n",
                "| endpoint | Endpoint URL where the Spark Manager Application is running | |\n",
                "| username | Username to connect to Spark Manager Application | |\n",
                "| password | Password to connect to Spark Manager Application | |\n",
                "\n",
                "\n",
                "### Provide Spark Resource Settings [Optional]\n",
                "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `{}` if no customisation is required.\n",
                "\n",
                "| Parameter | Description |\n",
                "| :- | :- |\n",
                "| max_num_executors | Maximum Number of executors to launch for this session |\n",
                "| min_executors | Minimum Number of executors to launch for this session |\n",
                "| executor_cores | Number of cores to use for each executor |\n",
                "| executor_memory | Amount of memory (in GBs) to use per executor process |\n",
                "| driver_cores | Number of cores to use for the driver process |\n",
                "| driver_memory | Amount of memory (in GBs) to use for the driver process |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "373015d8-decf-460b-875f-0c940f3afe4a",
            "metadata": {
                "id": "df35f3ad-d0a8-40cc-98fc-6943034bdf77"
            },
            "outputs": [],
            "source": [
                "spark_connection_info = {\n",
                "    \"connection\": {\n",
                "        \"endpoint\": \"\",\n",
                "        \"location_type\": \"\",\n",
                "        \"display_name\": \"\",\n",
                "        \"volume\": \"\",\n",
                "        \"instance_id\":\"\"\n",
                "    },\n",
                "    \"credentials\": {\n",
                "        \"username\": \"\",\n",
                "        \"password\": \"\",\n",
                "        \"apikey\": \"\"\n",
                "    }\n",
                "}\n",
                "\n",
                "\"\"\"\n",
                "Example:\n",
                "\n",
                "spark_settings = {\n",
                "    # max_num_executors: Maximum Number of executors to launch for this session\n",
                "    \"max_num_executors\": \"2\",\n",
                "    \n",
                "    # min_executors: Minimum Number of executors to launch for this session\n",
                "    \"min_executors\": \"1\",\n",
                "    \n",
                "    # executor_cores: Number of cores to use for each executor\n",
                "    \"executor_cores\": \"2\",\n",
                "    \n",
                "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
                "    \"executor_memory\": \"2\",\n",
                "    \n",
                "    # driver_cores: Number of cores to use for the driver process\n",
                "    \"driver_cores\": \"2\",\n",
                "    \n",
                "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
                "    \"driver_memory\": \"1\"\n",
                "}\n",
                "\"\"\"\n",
                "spark_settings = {}\n",
                "\n",
                "spark_connection_info[\"spark_settings\"] = spark_settings"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4abe1a19-bb16-42e7-b5f3-ff5b43356bed",
            "metadata": {
                "id": "b856cc3a-b4e3-4c84-8411-6f5119ccef5f"
            },
            "source": [
                "## Connect to IBM Watson OpenScale instance <a name=\"connect-openscale\"></a>\n",
                "\n",
                "Following information is required to connect to IBM Watson OpenScale instance:\n",
                "\n",
                "| Parameter | Description |\n",
                "| :- | :- |\n",
                "| url | Base url of your Cloud Pak for Data cluster hosting IBM Watson OpenScale instance. |\n",
                "| username | Username to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. |\n",
                "| password | Password to connect to your IBM Watson OpenScale instance in  Cloud Pak for Data cluster. One of `password` or `api_key` must be provided. |\n",
                "| api_key | API Key to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. One of `password` or `api_key` must be provided. |\n",
                "| service_instance_id | Id of your IBM Watson OpenScale Instance |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "183e1085-91db-4a0b-ae63-2fa71bbb1597",
            "metadata": {
                "id": "54ff3c97-07f9-491c-bde5-6fee70c7c431"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "3.0.45.2\n"
                    ]
                }
            ],
            "source": [
                "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
                "from ibm_watson_openscale import APIClient\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "service_instance_id = \"\" #Default is 00000000-0000-0000-0000-000000000000\n",
                "service_credentials = {\n",
                "    \"url\": \"\",\n",
                "    \"username\": \"\",\n",
                "    \"password\": \"\"\n",
                "    #     \"apikey\":\"\"\n",
                "}\n",
                "\n",
                "authenticator = CloudPakForDataAuthenticator(\n",
                "    url=service_credentials['url'],\n",
                "    username=service_credentials['username'],\n",
                "    password=service_credentials['password'],\n",
                "    #     apikey=service_credentials['apikey'],\n",
                "    disable_ssl_verification=True\n",
                ")\n",
                "\n",
                "client = APIClient(\n",
                "    service_url=service_credentials['url'],\n",
                "    service_instance_id=service_instance_id,\n",
                "    authenticator=authenticator\n",
                ")\n",
                "\n",
                "print(client.version)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25af9880-8d90-4726-a768-511548255ef2",
            "metadata": {
                "id": "a064ae6a-6f63-4658-b3db-29f8708702bd"
            },
            "source": [
                "## Configure Machine Learning Provider in IBM Watson OpenScale instance <a name=\"create-service-provider\"></a>\n",
                "\n",
                "Before configuring model for monitoring in IBM Watson OpenScale, you need to connect your machine learning provider with IBM Watson OpenScale instance. Since, we are configuring a model for monitoring which has its runtime data located remotely to IBM Watson OpenScale, we'll create a custom machine learning provider in given instance.\n",
                "\n",
                "Following details are required:\n",
                "\n",
                "| Parameter | Description |\n",
                "| :- | :- |\n",
                "| name | Name of the machine learning provider being configured. This can be any string value. |\n",
                "| description | Description for the machine learning provider being configured. |\n",
                "| service_type | Identifies type of the machine learning provider. In this case, this value must be `ServiceTypes.CUSTOM_MACHINE_LEARNING` |\n",
                "| credentials | Stores username and password to connect to machine learning provider. |\n",
                "| deployment_space_id | Identifies the space where the model is deployed. |\n",
                "| operational_space_id | Defines the classification of machine learning provider. Possible values are `pre-production` and `production`. |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "3ea48ba8-6cc1-4234-8021-84958508b954",
            "metadata": {
                "id": "4fe8463f-7d75-43e9-89e5-07105dfce259"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "=================================================================================\n",
                        "\n",
                        " Waiting for end of adding service provider 019638e2-5e3a-703e-a39f-fce0404f5fea \n",
                        "\n",
                        "=================================================================================\n",
                        "\n",
                        "\n",
                        "\n",
                        "active\n",
                        "\n",
                        "-----------------------------------------------\n",
                        " Successfully finished adding service provider \n",
                        "-----------------------------------------------\n",
                        "\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<HTML>\n",
                            "        <body>\n",
                            "            <h3>Service Providers</h3>\n",
                            "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
                            "                <th style='border: 1px solid #dddddd'>instance_id</th><th style='border: 1px solid #dddddd'>status</th><th style='border: 1px solid #dddddd'>name</th><th style='border: 1px solid #dddddd'>service_type</th><th style='border: 1px solid #dddddd'>created_at</th><th style='border: 1px solid #dddddd'>id</th>\n",
                            "                <tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>GCR-Driftv2-Batch-Sample-CSV-DB2</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 09:58:01.562000+00:00</td><td style='border: 1px solid #dddddd'>019638e2-5e3a-703e-a39f-fce0404f5fea</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>Final-Batch-Sample-CSV-Hive</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 09:10:20.770000+00:00</td><td style='border: 1px solid #dddddd'>019638b6-b74a-7a7a-a7a0-8ec34ef17fa4</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>GCR-Driftv2-Batch-Sample-CSV-JDBC</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 08:16:41.875000+00:00</td><td style='border: 1px solid #dddddd'>01963885-9988-7328-9e2d-8c2f1aca6025</td></tr><tr><td style='border: 1px solid #dddddd'>99999999-9999-9999-9999-999999999999</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>service-provider-space-f18c065b-6096-448c-b1aa-80bd23f1cecb</td><td style='border: 1px solid #dddddd'>watson_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 07:58:34.729000+00:00</td><td style='border: 1px solid #dddddd'>01963875-0294-7954-90f5-2606051356c8</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_APIKEY_CLOUD_WITHOUTAPI_PREPROD</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:07.777000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-6096-7a89-8be6-628d39f2bef2</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_APIKEY_CLOUD_WITHOUTAPI</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:07.574000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-5fb8-7c3c-abc4-1a84f67a91c6</td></tr><tr><td style='border: 1px solid #dddddd'>4ca7ec1c-6b35-48ef-a45c-0434c6985058</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>MRM_WMLV4_CLOUD_PREPROD</td><td style='border: 1px solid #dddddd'>watson_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:07.415000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-5e59-7b4d-b912-3a27e1e18391</td></tr><tr><td style='border: 1px solid #dddddd'>4ca7ec1c-6b35-48ef-a45c-0434c6985058</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>MRM_WMLV4_CLOUD_PROD</td><td style='border: 1px solid #dddddd'>watson_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:04.460000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-50f0-7b96-b640-cddd7685e822</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_HLS_PREPROD</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:57:59.876000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-41ba-7b9c-8ee2-2ce5df750412</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_BATCH_PROD</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:57:59.780000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-4159-791d-b775-1035fe606be9</td></tr>\n",
                            "            </table>\n",
                            "        </body>\n",
                            "        </HTML>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: First 10 records were displayed.\n"
                    ]
                }
            ],
            "source": [
                "# [OPTIONAL] Delete existing service provider with the same name as provided\n",
                "\n",
                "# SERVICE_PROVIDER_NAME = \"\"\n",
                "# service_providers = client.service_providers.list().result.service_providers\n",
                "# for provider in service_providers:\n",
                "#     if provider.entity.name == SERVICE_PROVIDER_NAME:\n",
                "#         client.service_providers.delete(service_provider_id=provider.metadata.id)\n",
                "#         break\n",
                "\n",
                "# Add Service Provider\n",
                "from ibm_watson_openscale.supporting_classes.enums import ServiceTypes\n",
                "# from ibm_watson_openscale.base_classes.watson_open_scale_v2 import CustomCredentials\n",
                "\n",
                "added_service_provider_result = client.service_providers.add(\n",
                "        name=\"\",\n",
                "        description=\"\",\n",
                "        service_type=ServiceTypes.CUSTOM_MACHINE_LEARNING,\n",
                "        credentials={},\n",
                "        deployment_space_id = \"\",\n",
                "        operational_space_id=\"\",\n",
                "        background_mode=False\n",
                "    ).result\n",
                "\n",
                "service_provider_id = added_service_provider_result.metadata.id\n",
                "\n",
                "client.service_providers.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a33d4818-60ba-4740-8b2e-56ac847b92e8",
            "metadata": {
                "id": "57b7720f-68cb-454e-98ae-aa86a17065c7"
            },
            "source": [
                "## Onboard model for monitoring in IBM Watson OpenScale instance <a name=\"create-subscription\"></a>\n",
                "\n",
                "When you configure a model for monitoring in IBM Watson OpenScale instance, a corresponding subscription is created for this model. Following details are required:\n",
                "\n",
                "| Parameter | Description |\n",
                "| :- | :- |\n",
                "| subscription_name | Name of the subscription to use. This can be any string value typically identifying model being monitored. |\n",
                "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
                "| service_provider_id | Id of the machine learning provider instance created in IBM Watson OpenScale. |\n",
                "| model_info | Details of the model to be monitored |\n",
                "| sample_csv | Path to the csv file containing scored training data |\n",
                "| spark_credentials | Connection details of Spark compute engine to use for analysis by different IBM Watson OpenScale services. |\n",
                "| payload_table | Details of the payload table to be used with this subscription. |\n",
                "| feedback_table | Details of the feedback table to be used with this subscription. |\n",
                "| scored_training_data_table | Details of the scored training data table to be used with this subscription. |\n",
                "| managed_by | To identify whether the subscription is `system` managed (Model transactions are stored in the OpenScale database and evaluated using OpenScale computing resources) or `self` managed (Model transactions are stored in a your own data warehouse and evaluated by your Spark analytics engine.) . This function is not supporting system managed subscriptions as of now. |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "b829a86f-69c6-443b-9af2-69275b9c9d70",
            "metadata": {
                "id": "ee68a0a1-1226-4b95-b876-9b509731d8bb"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Warning] Missing scoring url : scoring url is mandatory if explain needs to be enabled\n",
                        "Creating integrated system for Spark\n",
                        "Integrated system 019638e2-802c-7d22-b435-dcbe108556b8 created \n",
                        "Creating integrated system for Hive/DB2\n",
                        "Hive/Db2 Integrated system 019638e2-80f7-7782-a51b-355488ad6875 created\n",
                        "Updating schemas ...\n",
                        "09:58:11 preparing\n",
                        "Schemas update completed.\n",
                        "Updating data-sources ...\n",
                        "Data-sources update complete.\n",
                        "Subscription is created. Id is : 019638e2-819e-790b-8f31-f18fd1a0c7cb\n",
                        "Subscription is being activated, please wait for state to be active before using it further.\n"
                    ]
                }
            ],
            "source": [
                "subscription_id = client.subscriptions.create_subscription_using_training_data(\n",
                "    subscription_name=\"My SDK Batch Subscription-db2\",\n",
                "    datamart_id=service_instance_id,\n",
                "    service_provider_id=service_provider_id,\n",
                "    model_info=model_info,\n",
                "    sample_csv = sample_csv,\n",
                "    spark_credentials=spark_connection_info,\n",
                "    data_warehouse_connection = datawarehouse_details,\n",
                "    payload_table=payload_table,\n",
                "    feedback_table=feedback_table,\n",
                "    scored_training_data_table = scored_training_data_table,\n",
                "    managed_by=\"self\"\n",
                ")\n",
                "\n",
                "# print(\"Subscription id is {}\".format(subscription_id))\n",
                "\n",
                "# Wait for the subscription to get in active state and to create the \n",
                "# required tables in the background before moving onto enabling monitors\n",
                "\n",
                "# import time\n",
                "# from datetime import datetime\n",
                "\n",
                "# subscription_status = None\n",
                "# while subscription_status not in (\"active\", \"error\"):\n",
                "#     subscription_status = client.subscriptions.get(subscription_id).result.entity.status.state\n",
                "#     if subscription_status not in (\"active\", \"error\"):\n",
                "#         print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)\n",
                "#         time.sleep(15)\n",
                "        \n",
                "# print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b475546b-1db0-4066-bfc8-e684c0977a92",
            "metadata": {
                "id": "fed8e39b-2238-455d-bb38-5e4602a2de5a"
            },
            "source": [
                "## Enable different services to monitor model <a name=\"enable-monitors\"></a>\n",
                "\n",
                "Depending on the services enabled in `monitors_config`, different services are enabled in given subscription. There services are called monitors.\n",
                "\n",
                "Following details are required:\n",
                "\n",
                "| Parameter | Description |\n",
                "| :- | :- |\n",
                "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
                "| subscription_id | Id of the subscription created for given model in IBM Watson OpenScale instance. |\n",
                "| monitors_config | Details of the monitores that needs to get configured. |\n",
                "| explain_queue_table | Details of the explain queue table to be used with this subscription. |\n",
                "| explain_results_table | Details of the explain results table to be used with this subscription. |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a9cb5d36-ee6a-4cd1-9968-bb7d4513293b",
            "metadata": {
                "id": "9fbf9baf-3912-415b-b55a-6be21dfe450b"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Enabling Drift V2....\n",
                        "{'drift_v2': '019638e3-82c2-7dc2-adc4-aaaa0d701642'}\n"
                    ]
                }
            ],
            "source": [
                "instance_ids = client.monitor_instances.enable_monitor_using_training_data(\n",
                "                  datamart_id = service_instance_id,\n",
                "                  subscription_id = subscription_id,\n",
                "                  monitors_config = monitors_config,\n",
                "                  explain_queue_table = explain_queue_table,\n",
                "                  explain_results_table = explain_result_table)\n",
                "\n",
                "print(instance_ids)\n",
                "\n",
                "## Track each monitor instance status\n",
                "# for key, value in instance_ids.items():\n",
                "#     monitor_instance_status = None\n",
                "\n",
                "#     while monitor_instance_status not in (\"active\", \"error\"):\n",
                "#         monitor_instance_details = client.monitor_instances.get(monitor_instance_id=value).result\n",
                "#         monitor_instance_status = monitor_instance_details.entity.status.state\n",
                "#         if monitor_instance_status not in (\"active\", \"error\"):\n",
                "#             print(datetime.now().strftime(\"%H:%M:%S\"), monitor_instance_status)\n",
                "#             time.sleep(30)\n",
                "\n",
                "#     print(key, monitor_instance_status)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71ca1292-9c58-4160-95c1-a3087f79f73d",
            "metadata": {
                "id": "866266fc-8d24-49f9-9d9b-d6cd4cf09eed"
            },
            "source": [
                "## Congratulations!\n",
                "\n",
                "All the monitors have been enabled. It will take some time for monitors to get into active state. You can track the status of each monitor separately by using above code snippet.\n",
                "\n",
                "Once, all monitors are active, load data into payload or feedback table and either run on-demand evaluations or wait for scheduled evaluations to complete for each monitor. You can check more details in [Watson OpenScale Dashboard](https://url-to-your-cp4d-cluster/aiopenscale)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "50bb679d",
            "metadata": {
                "id": "8caf84cc-e9a9-4582-b74b-0a3ccd789aac"
            },
            "source": [
                "## Helper Methods"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26bc2b09",
            "metadata": {
                "id": "f8a77f8b-d673-4eb9-b3b0-4ce943ca635d"
            },
            "source": [
                "### Cleanup subscription and its related artefacts\n",
                "Crawls through subscription json and identifies entities to be deleted. Currently, following entities are identified and deleted:\n",
                "- Analytics Engine integrated system\n",
                "- Data Warehouse Connection integrated system(s)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "c7cac8d9",
            "metadata": {
                "id": "001e893a-b0fc-42b7-a5fa-6bed436a9662"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found integrated system for analytics engine with type: spark\n",
                        "Found integrated system for data source with type: payload\n",
                        "Integrated Systems to delete: ['bd6d2c78-c317-434a-b763-bbcdf700a85b', 'de2e563a-cb13-4454-9e92-60961b1daba8']\n",
                        "\n",
                        "\n",
                        "===============================================================================\n",
                        "\n",
                        " Waiting for end of deleting subscription 28ac2561-7d46-4e0f-bb87-189b0f652385 \n",
                        "\n",
                        "===============================================================================\n",
                        "\n",
                        "\n",
                        "\n",
                        "finished\n",
                        "\n",
                        "---------------------------------------------\n",
                        " Successfully finished deleting subscription \n",
                        "---------------------------------------------\n",
                        "\n",
                        "\n",
                        "Deleting integrated system with id: bd6d2c78-c317-434a-b763-bbcdf700a85b\n",
                        "Deleting integrated system with id: de2e563a-cb13-4454-9e92-60961b1daba8\n",
                        "Cleanup Complete!!!\n"
                    ]
                }
            ],
            "source": [
                "# # Uncomment and update following if you are running this at a later point of time or \n",
                "# # separate from this notebook with no subscription id and wos client session\n",
                "\n",
                "# from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
                "# from ibm_watson_openscale import APIClient\n",
                "\n",
                "# import warnings\n",
                "# warnings.filterwarnings('ignore')\n",
                "\n",
                "# service_instance_id = \"<SERVICE_INSTANCE_ID>\" #Default is 00000000-0000-0000-0000-000000000000\n",
                "# service_credentials = {\n",
                "#     \"url\": \"<to_be_edited>\",\n",
                "#     \"username\": \"<to_be_edited>\",\n",
                "#     \"password\": \"<to_be_edited>\",\n",
                "# #     \"apikey\":\"<to_be_edited>\"\n",
                "# }\n",
                "\n",
                "# authenticator = CloudPakForDataAuthenticator(\n",
                "#     url=service_credentials['url'],\n",
                "#     username=service_credentials['username'],\n",
                "#     password=service_credentials['password'],\n",
                "# #     apikey=service_credentials['apikey'],\n",
                "#     disable_ssl_verification=True\n",
                "# )\n",
                "\n",
                "# client = APIClient(\n",
                "#     service_url=service_credentials['url'],\n",
                "#     service_instance_id=service_instance_id,\n",
                "#     authenticator=authenticator\n",
                "# )\n",
                "\n",
                "# print(client.version)\n",
                "\n",
                "# subscription_id = \"<to_be_edited>\"\n",
                "\n",
                "subscription_details = client.subscriptions.get(\n",
                "    subscription_id=subscription_id).result.to_dict()\n",
                "subscription_entity = subscription_details.get(\"entity\", {})\n",
                "\n",
                "integrated_systems_id = []\n",
                "\n",
                "# add analytics engine integrated system id\n",
                "analytics_engine = subscription_entity.get(\"analytics_engine\", {})\n",
                "if analytics_engine and analytics_engine.get(\"integrated_system_id\"):\n",
                "    print(\"Found integrated system for analytics engine with type: {}\".format(\n",
                "        analytics_engine.get(\"type\")))\n",
                "    integrated_systems_id.append(analytics_engine.get(\"integrated_system_id\"))\n",
                "\n",
                "# add data source integrated system ids\n",
                "data_sources = subscription_entity.get(\"data_sources\", [])\n",
                "for data_source in data_sources:\n",
                "    if not data_source.get(\"connection\"):\n",
                "        continue\n",
                "\n",
                "    if not data_source.get(\"connection\").get(\"integrated_system_id\"):\n",
                "        continue\n",
                "\n",
                "    integrated_system_id = data_source.get(\"connection\").get(\"integrated_system_id\")\n",
                "    if integrated_system_id in integrated_systems_id:\n",
                "        continue\n",
                "\n",
                "    print(\"Found integrated system for data source with type: {}\".format(\n",
                "        data_source.get(\"type\")))\n",
                "    integrated_systems_id.append(integrated_system_id)\n",
                "    \n",
                "print(\"Integrated Systems to delete: {}\".format(integrated_systems_id))\n",
                "    \n",
                "# delete subscription\n",
                "client.subscriptions.delete(\n",
                "    subscription_id=subscription_id,\n",
                "    background_mode=False)\n",
                "\n",
                "# wait time for subscription delete to complete\n",
                "import time\n",
                "time.sleep(30)\n",
                "\n",
                "# delete all integrated systems\n",
                "for integrated_system_id in integrated_systems_id:\n",
                "    print(\"Deleting integrated system with id: {}\".format(integrated_system_id))\n",
                "    client.integrated_systems.delete(integrated_system_id)\n",
                "    \n",
                "    # wait time for integrated system delete to complete\n",
                "    time.sleep(10)\n",
                "    \n",
                "print(\"Cleanup Complete!!!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.11",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        },
        "vscode": {
            "interpreter": {
                "hash": "37b82c9850f337b3f8e26ef3a35bf87c9f2cfa1e4ad2c96ec00819afd6ebf7e9"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
