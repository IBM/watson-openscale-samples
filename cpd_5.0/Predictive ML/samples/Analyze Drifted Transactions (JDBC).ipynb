{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for analyzing payload transactions causing drift for JDBC storage\n",
    "\n",
    "This notebook helps users of IBM Watson OpenScale to analyze payload transactions that are causing drift - both drop in accuracy and drop in data consistency. \n",
    "\n",
    "The notebook is designed to give users a jump start in their analysis of the payload transactions. It is by no means a comprehensive analysis. \n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) to be able to proceed with the analysis. \n",
    "\n",
    "PS: This notebook is designed to analyse one drift monitor run at a time for a given subscription.\n",
    "\n",
    "**Contents:**\n",
    "1. [Pre-requisites](#Pre-requisites)\n",
    "2. [Installing Dependencies](#Installing-Dependencies)\n",
    "3. [User Inputs](#User-Inputs)\n",
    "4. [Setting up Services](#Setting-up-Services)\n",
    "5. [Measurement Summary](#Measurement-Summary)\n",
    "6. [Counts from Drifted Transactions Table](#Counts-from-Drifted-Transactions-Table)\n",
    "7. [Analyse Transactions Causing Drop in Accuracy](#Analyse-Transactions-Causing-Drop-in-Accuracy)\n",
    "    * [Get all transactions causing drop in data accuracy](#Get-all-transactions-causing-drop-in-data-accuracy)\n",
    "    * [Get all transactions causing drop in accuracy in given range of drift model confidence](#Get-all-transactions-causing-drop-in-accuracy-in-given-range-of-drift-model-confidence)\n",
    "8. [Analyse Transactions Causing Drop in Accuracy and Drop in Data Consistency](#Analyse-Transactions-Causing-Drop-in-Accuracy-and-Drop-in-Data-Consistency)\n",
    "    * [Get all transactions causing drop in accuracy and drop in data consistency](#Get-all-transactions-causing-drop-in-accuracy-and--drop-in-data-consistency)\n",
    "    * [Get all transactions causing drop in accuracy and drop in data consistency in given range of drift model confidence](#Get-all-transactions-causing-drop-in-accuracy-and-drop-in-data-consistency-in-given-range-of-drift-model-confidence)\n",
    "9. [Analyse Transactions Causing Drop in Data Consistency](#Analyse-Transactions-Causing-Drop-in-Data-Consistency)\n",
    "    * [Get all transactions causing drop in data consistency](#Get-all-transactions-causing-drop-in-data-consistency)\n",
    "    * [Get all transactions violating a data constraint](#Get-all-transactions-violating-a-data-constraint)\n",
    "    * [Get all transactions where a column is causing drop in data consistency](#Get-all-transactions-where-a-column-is-causing-drop-in-data-consistency)\n",
    "    * [Explain categorical distribution constraint violations](#Explain-categorical-distribution-constraint-violations)\n",
    "    * [Explain numeric range constraint violations](#Explain-numeric-range-constraint-violations)\n",
    "    * [Explain cat-numeric range constraint violations](#Explain-cat-numeric-range-constraint-violations)\n",
    "    * [Explain cat-cat distribution constraint violations](#Explain-cat-cat-distribution-constraint-violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the required jars to connect to the JDBC storage.\n",
    "2. If running locally - place the jars in the `jars` directory of your spark installation.\n",
    "3. If running from a project in Watson Studio - upload the jars as Data Assets to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5737-H76\n",
    "# Copyright IBM Corp. 2021, 2024\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = \"jdbc-1.1.9\"\n",
    "\n",
    "# Version history\n",
    "\n",
    "# jdbc-1.1.9 : Upgrade ibm-wos-utils to 5.0.0\n",
    "# jdbc-1.1.8 : Upgrade ibm-wos-utils to 4.8.0\n",
    "# jdbc-1.1.7 : Upgrade ibm-wos-utils to 4.7.0\n",
    "# jdbc-1.1.6 : Install pyspark as a pre-requisite; Add clarification for JDBC_SSL_CERTIFICATE\n",
    "# jdbc-1.1.5 : Upgrade ibm-wos-utils to 4.5.0\n",
    "# jdbc-1.1.4 : Upgrade ibm-wos-utils to 4.1.1 (scikit-learn has been upgraded to 1.0.2)\n",
    "# jdbc-1.1.3 : Upgrade ibm-wos-utils to 4.0.34\n",
    "# jdbc-1.1.2 : Upgrade ibm-wos-utils to 4.0.31\n",
    "# jdbc-1.1.1 : Add comment about conda install for zLinux environments; Upgrade ibm-wos-utils to 4.0.25\n",
    "# jdbc-1.1   : Add partition column information for the payload and drifted transactions table\n",
    "# 1.0        : First public release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PYTHON = sys.executable\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts --upgrade tabulate ibm-watson-openscale pyspark | tail -n 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For IBM Watson OpenScale Cloud Pak for Data version 5.0.x, use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==1.3.1 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=1.3.1\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts \"ibm-wos-utils~=5.0.0\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "\n",
    "The following inputs are required:\n",
    "\n",
    "1. **IBM_CPD_ENDPOINT:** The URL representing the IBM Cloud Pak for Data service endpoint.\n",
    "2. **IBM_CPD_USERNAME:** IBM Cloud Pak for Data username used to obtain a bearer token.\n",
    "3. **IBM_CPD_PASSWORD:** IBM Cloud Pak for Data password used to obtain a bearer token.\n",
    "4. **JDBC_HOST:** Hostname of the JDBC Connection\n",
    "5. **JDBC_PORT:** Port of the JDBC Connection\n",
    "6. **JDBC_USE_SSL:** Boolean Flag to indicate whether to use SSL while connecting.\n",
    "7. **JDBC_SSL_CERTIFICATE:** Path to SSL Certificate file. Ignored if JDBC_USE_SSL is False.\n",
    "    - If running on local Jupyter, please provide the absolute path for the SSL Certificate\n",
    "    - If running in a Watson Studio environment, upload the SSL Certificate as an asset and provide it's path. e.g.  `JDBC_SSL_CERTIFICATE = \"/project_data/data_asset/my_cert.arm\"`\n",
    "8. **JDBC_DRIVER:** Class name of the JDBC driver to use to connect e.g. for DB2 use the default value \"com.ibm.db2.jcc.DB2Driver\"\n",
    "9. **JDBC_USERNAME:** Username of the JDBC Connection\n",
    "10. **JDBC_PASSWORD:** Password of the JDBC Connection\n",
    "11. **JDBC_DATABASE_NAME:** Name of the JDBC Database to connect.\n",
    "12. **ANALYSIS_INPUT_PARAMETERS:** Analysis Input Parameters to be copied from IBM Watson OpenScale UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM Cloud Pak for Data credentials\n",
    "IBM_CPD_ENDPOINT = \"<The URL representing the IBM Cloud Pak for Data service endpoint.>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username used to obtain a bearer token.>\"\n",
    "IBM_CPD_PASSWORD = \"<IBM Cloud Pak for Data password used to obtain a bearer token.>\"\n",
    "\n",
    "# JDBC\n",
    "JDBC_HOST = \"<Hostname of the JDBC Connection>\"\n",
    "JDBC_PORT = \"<Port of the JDBC Connection>\"\n",
    "JDBC_USE_SSL = False\n",
    "JDBC_SSL_CERTIFICATE = \"<Path to SSL Certificate file. Ignored if JDBC_USE_SSL is False.>\"\n",
    "JDBC_DRIVER = \"com.ibm.db2.jcc.DB2Driver\"\n",
    "JDBC_USERNAME = \"<Username of the JDBC Connection>\"\n",
    "JDBC_PASSWORD = \"<Password of the JDBC Connection>\"\n",
    "JDBC_DATABASE_NAME = \"<Name of the JDBC Database to connect.>\"\n",
    "\n",
    "# NUM_PARTITIONS decide the number of simultaneous connections Spark will make to your JDBC instance.\n",
    "# e.g. DB2 on Cloud Free Plan has a maximum limit of 15 simultaneous connections\n",
    "# This is the default value used if no value is set in the data sources in the subscription.\n",
    "NUM_PARTITIONS = \"10\"\n",
    "\n",
    "# FETCH_SIZE determines how many rows to fetch per round trip.\n",
    "FETCH_SIZE = \"100\"\n",
    "\n",
    "# Analysis Input Parameters to be copied from UI\n",
    "# Please make sure that the quotes around the key-values \n",
    "# are correct after copying from UI\n",
    "ANALYSIS_INPUT_PARAMETERS = {\n",
    "    \"data_mart_id\": \"<data_mart_id>\",\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"monitor_instance_id\": \"<monitor_instance_id>\",\n",
    "    \"measurement_id\": \"<measurement_id>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAMART_ID = ANALYSIS_INPUT_PARAMETERS.get(\"data_mart_id\")\n",
    "SUBSCRIPTION_ID = ANALYSIS_INPUT_PARAMETERS.get(\"subscription_id\")\n",
    "MONITOR_INSTANCE_ID = ANALYSIS_INPUT_PARAMETERS.get(\"monitor_instance_id\")\n",
    "MEASUREMENT_ID = ANALYSIS_INPUT_PARAMETERS.get(\"measurement_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:db2://{}:{}/{}\".format(JDBC_HOST, JDBC_PORT, JDBC_DATABASE_NAME)\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": JDBC_USERNAME,\n",
    "    \"password\": JDBC_PASSWORD,\n",
    "    \"driver\": JDBC_DRIVER,\n",
    "    \"fetchsize\": FETCH_SIZE\n",
    "}\n",
    "\n",
    "if JDBC_USE_SSL:\n",
    "    connection_properties[\"sslConnection\"] = \"true\"\n",
    "    connection_properties[\"sslCertLocation\"] = JDBC_SSL_CERTIFICATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import APIClient\n",
    "\n",
    "from ibm_wos_utils.drift.batch.util.constants import ConstraintName\n",
    "from ibm_wos_utils.joblib.utils.analyze_notebook_utils import (\n",
    "    explain_catcat_distribution_constraint,\n",
    "    explain_categorical_distribution_constraint,\n",
    "    explain_catnum_range_constraint, explain_numeric_range_constraint,\n",
    "    get_column_query, get_drift_archive_contents,\n",
    "    get_table_details_from_subscription, show_constraints_by_column,\n",
    "    show_dataframe, show_last_n_drift_measurements, get_record_timestamp_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "        .setAppName(\"Analyze Drifted Transactions\")\\\n",
    "\n",
    "# Uncomment the following line if running this notebook from a Watson Studio Project.\n",
    "# Here, db2jcc4.jar is the DB2 specific jar required to run this notebook.\n",
    "# conf = conf.set(\"spark.jars\", \"/project_data/data_asset/db2jcc4.jar\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = CloudPakForDataAuthenticator(\n",
    "        url=IBM_CPD_ENDPOINT,\n",
    "        username=IBM_CPD_USERNAME,\n",
    "        password=IBM_CPD_PASSWORD,\n",
    "        disable_ssl_verification=True\n",
    "    )\n",
    "wos_client = APIClient(authenticator=authenticator, service_url=IBM_CPD_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not DATAMART_ID or not SUBSCRIPTION_ID:\n",
    "    raise Exception(\"DATAMART_ID and SUBSCRIPTION_ID are required to proceed.\")\n",
    "\n",
    "subscription = wos_client.subscriptions.get(subscription_id=SUBSCRIPTION_ID).result\n",
    "monitor_instance = wos_client.monitor_instances.list(data_mart_id=DATAMART_ID, target_target_id=SUBSCRIPTION_ID, monitor_definition_id=\"drift\").result.monitor_instances[0]\n",
    "model_drift_enabled = monitor_instance.entity.parameters.get(\"model_drift_enabled\", False)\n",
    "data_drift_enabled = monitor_instance.entity.parameters.get(\"data_drift_enabled\", False)\n",
    "\n",
    "if not MONITOR_INSTANCE_ID:\n",
    "    MONITOR_INSTANCE_ID = monitor_instance.metadata.id\n",
    "    \n",
    "drift_archive = wos_client.monitor_instances.download_drift_model(monitor_instance_id=MONITOR_INSTANCE_ID).result.content\n",
    "schema, ddm_properties, constraints_set = get_drift_archive_contents(drift_archive, model_drift_enabled, data_drift_enabled)\n",
    "_, payload_schema_name, payload_table_name, payload_partition_column, payload_num_partitions = get_table_details_from_subscription(subscription, \"payload\", NUM_PARTITIONS)\n",
    "_, drift_schema_name, drift_table_name, drift_partition_column, drift_num_partitions = get_table_details_from_subscription(subscription, \"drift\", NUM_PARTITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies heavily on filtering transactions in the Drifted Transactions table based on three columns: `run_id`, `is_model_drift` and `is_data_drift`. \n",
    "\n",
    "It is, therefore, recommended that you create an index for these columns, if not done already as part of the common configuration notebook. You can use the following DDL to create the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_string = \"CREATE INDEX \\\"{1}_index\\\" ON \\\"{0}\\\".\\\"{1}\\\" (\\\"run_id\\\", \\\"is_model_drift\\\", \\\"is_data_drift\\\")\".format(drift_schema_name, drift_table_name)\n",
    "print(ddl_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MEASUREMENT_ID:\n",
    "    print(\"Please pick a measurement to analyze from the following list:\")\n",
    "    \n",
    "show_last_n_drift_measurements(10, wos_client, SUBSCRIPTION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not selected MEASUREMENT_ID so far, please enter a measurement ID\n",
    "# from the above cell's output to analyze.\n",
    "\n",
    "# MEASUREMENT_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MEASUREMENT_ID:\n",
    "    raise Exception(\"MEASUREMENT_ID is required to proceed.\")\n",
    "\n",
    "measurement = wos_client.monitor_instances.measurements.get(measurement_id=MEASUREMENT_ID, monitor_instance_id=MONITOR_INSTANCE_ID).result\n",
    "measurement_data = measurement.entity.sources[0].data\n",
    "MONITOR_RUN_ID = measurement.entity.run_id\n",
    "MONITOR_RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy and drop in data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IBM Watson OpenScale analyzed {} transactions between {} and {} for drift. Here's a summary.\".format(measurement_data[\"transactions_count\"], measurement_data[\"start\"], measurement_data[\"end\"]))\n",
    "\n",
    "if model_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing drop in accuracy.\".format(measurement_data[\"drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))\n",
    "\n",
    "if data_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing drop in data consistency.\".format(measurement_data[\"data_drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))\n",
    "    \n",
    "if model_drift_enabled and data_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing both drop in accuracy and drop in data consistency.\".format(measurement_data[\"model_data_drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy - percent bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_drift_enabled:\n",
    "    rows_df = pd.DataFrame(measurement_data[\"drifted_transactions\"][\"drift_model_confidence_count\"])\n",
    "    rows_df = rows_df[[\"lower_limit\", \"upper_limit\", \"count\"]]\n",
    "    rows_df.columns = [\"Drift Model Confidence - Lower Limit\", \"Drift Model Confidence - Upper Limit\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in data consistency - feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_drift_enabled:\n",
    "    rows_df = pd.Series(measurement_data[\"data_drifted_transactions\"][\"features_count\"])\\\n",
    "                .sort_values(ascending=False).to_frame()\n",
    "    rows_df.reset_index(inplace=True)\n",
    "    rows_df.columns = [\"Feature Column\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy - constraints list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_drift_enabled:\n",
    "    rows_df = pd.Series(measurement_data[\"data_drifted_transactions\"][\"constraints_count\"])\\\n",
    "                .sort_values(ascending=False).to_frame()\n",
    "    rows_df.reset_index(inplace=True)\n",
    "    rows_df.columns = [\"Constraint Name\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts from Drifted Transactions Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take advantage of multiple workers in Spark while reading information from the JDBC table, the partition information is being added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"(select min(\\\"{0}\\\") \\\"rtmin\\\", max(\\\"{0}\\\") \\\"rtmax\\\" from \\\"{1}\\\".\\\"{2}\\\" where \\\"run_id\\\" = '{3}')\".format(drift_partition_column, drift_schema_name, drift_table_name, MONITOR_RUN_ID)\n",
    "print(sql)\n",
    "\n",
    "result = spark.read.jdbc(url=jdbc_url,\\\n",
    "                 table=sql,\\\n",
    "                 properties=connection_properties).collect()[0]\n",
    "\n",
    "\n",
    "drift_connection_properties = {}\n",
    "drift_connection_properties.update(connection_properties)\n",
    "drift_connection_properties[\"partitionColumn\"] = drift_partition_column\n",
    "drift_connection_properties[\"lowerBound\"] = str(result.rtmin)\n",
    "drift_connection_properties[\"upperBound\"] = str(result.rtmax)\n",
    "drift_connection_properties[\"numPartitions\"] = str(drift_num_partitions)\n",
    "\n",
    "drift_connection_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drift_table_df = spark.read.jdbc(url=jdbc_url,\\\n",
    "                                 table=\"\\\"{}\\\".\\\"{}\\\"\".format(drift_schema_name, drift_table_name),\\\n",
    "                                 properties=drift_connection_properties)\n",
    "\n",
    "drift_table_df = drift_table_df.where(drift_table_df.run_id == MONITOR_RUN_ID)\n",
    "drift_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_timestamp_column = get_record_timestamp_column(subscription)\n",
    "\n",
    "# Convert ISO format timestamp to DB2 SQL compatible format\n",
    "start = measurement_data[\"start\"].replace(\"T\", \" \")\n",
    "end = measurement_data[\"end\"].replace(\"T\", \" \")\n",
    "\n",
    "sql = \"(select min(\\\"{0}\\\") \\\"rtmin\\\", max(\\\"{0}\\\") \\\"rtmax\\\" from \\\"{1}\\\".\\\"{2}\\\" where \\\"{3}\\\" >= '{4}' and \\\"{3}\\\" <= '{5}')\".format(payload_partition_column, payload_schema_name, payload_table_name, record_timestamp_column, start, end)\n",
    "print(sql)\n",
    "\n",
    "result = spark.read.jdbc(url=jdbc_url,\\\n",
    "                 table=sql,\\\n",
    "                 properties=connection_properties).collect()[0]\n",
    "\n",
    "\n",
    "payload_connection_properties = {}\n",
    "payload_connection_properties.update(connection_properties)\n",
    "payload_connection_properties[\"partitionColumn\"] = drift_partition_column\n",
    "payload_connection_properties[\"lowerBound\"] = str(result.rtmin)\n",
    "payload_connection_properties[\"upperBound\"] = str(result.rtmax)\n",
    "payload_connection_properties[\"numPartitions\"] = str(drift_num_partitions)\n",
    "\n",
    "payload_connection_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_table_df = spark.read.jdbc(url=jdbc_url,\\\n",
    "                                 table=\"\\\"{}\\\".\\\"{}\\\"\".format(payload_schema_name, payload_table_name),\\\n",
    "                                 properties=payload_connection_properties)\n",
    "\n",
    "payload_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Total number of drifted transactions: {}\".format(drift_table_df.count()))\n",
    "print(\"Total number of model drift transactions: {}\".format(drift_table_df.where(\"is_model_drift\").count()))\n",
    "print(\"Total number of data drift transactions: {}\".format(drift_table_df.where(\"is_data_drift\").count()))\n",
    "print(\"Total number of model + data drift transactions: {}\".format(drift_table_df.where(\"is_model_drift\").where(\"is_data_drift\").count()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Transactions Causing Drop in Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in data accuracy\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy.\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy in given range of drift model confidence\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drift Model Confidence Lower Limit\n",
    "dm_conf_lower = 0.5\n",
    "# Drift Model Confidence Upper Limit\n",
    "dm_conf_upper = 1.0\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(drift_table_df.drift_model_confidence.between(dm_conf_lower,dm_conf_upper))\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy where drift model confidence is between {} and {}\".format(count, dm_conf_lower, dm_conf_upper))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Transactions Causing Drop in Accuracy and Drop in Data Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy and  drop in data consistency\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing both drop in accuracy and drop in data consistency\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy and drop in data consistency in given range of drift model confidence\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drift Model Confidence Lower Limit\n",
    "dm_conf_lower = 0.5\n",
    "# Drift Model Confidence Upper Limit\n",
    "dm_conf_upper = 1.0\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .where(drift_table_df.drift_model_confidence.between(dm_conf_lower,dm_conf_upper))\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy and drop in data consistency where drift model confidence is between {} and {}\".format(count, dm_conf_lower, dm_conf_upper))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Transactions Causing Drop in Data Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in data consistency\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .select([\"scoring_id\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in data consistency\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions violating a data constraint\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_name = ConstraintName.CATEGORICAL_DISTRIBUTION_CONSTRAINT\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "        .where(\"is_data_drift\")\\\n",
    "        .where(F.col(constraint_name.value).like(\"%1%\"))\\\n",
    "        .select([\"scoring_id\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are violating {}.\".format(count, constraint_name.value))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "    \n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions where a column is causing drop in data consistency\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_query = get_column_query(constraints_set, schema, column=\"<column_name>\")\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .where(filter_query)\\\n",
    "    .select([\"scoring_id\"])\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are satisfying the given query.\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query all the learnt constraints based on a column name\n",
    "\n",
    "Use the `show_constraints_by_column` method to find all the constraints learnt for a particular column at training time. The constraint ids shown in the cell output can be used to explain the corresponding constraint in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_constraints_by_column(constraints_set, \"<column_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain categorical distribution constraint violations\n",
    "\n",
    "Explains categorical distribution constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_categorical_distribution_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain numeric range constraint violations\n",
    "\n",
    "Explains numeric range constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_numeric_range_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain cat-numeric range constraint violations\n",
    "\n",
    "Explains cat-numeric range constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_catnum_range_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain cat-cat distribution constraint violations\n",
    "\n",
    "Explains cat-cat distribution constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_catcat_distribution_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "Developed by [Prem Piyush Goyal](mailto:prempiyush@in.ibm.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
