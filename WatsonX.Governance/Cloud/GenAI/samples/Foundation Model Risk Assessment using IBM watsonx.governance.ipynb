{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc855a34-bbed-4a93-adbd-b59c0bab8b12",
   "metadata": {},
   "source": [
    "# Using IBM watsonx.governance metrics toolkit to assess the risk of a foundation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dc2e579-4258-492c-be1f-df8157ad8347",
   "metadata": {},
   "source": [
    "\n",
    "This notebook should be run using Python 3.10.\n",
    "\n",
    "The notebook evaluates the risk associated with the given foundation model. The risk assessment result could be stored in OpenPages or exported as a pdf file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b2e99-1ca1-4e13-8140-186b48d5ef7f",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- Setup\n",
    "- Configure credentials \n",
    "- Evaluate the risks for a FM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4390693-5a86-4971-962c-965b295555c7",
   "metadata": {},
   "source": [
    "## Setup <a name=\"settingup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc3415",
   "metadata": {},
   "source": [
    "### Replace your username and artifactory token below, and use the correct version for ibm-metrics-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dac475-4ed8-4246-81bd-642482817710",
   "metadata": {
    "msg_id": "9d3ec2f3-cac0-4874-bfe3-886309d62a44"
   },
   "outputs": [],
   "source": [
    "!pip install ibm-metrics-plugin[mra]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b2260-76ff-4a5c-9dda-3663ad8133a5",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80606d",
   "metadata": {},
   "source": [
    "If you want to evaluate the risks against watsonx.ai model then you must provide CPD or Cloud credentials in the system_credentials below. For evaluating the risk of external LLM, a wrapper scoring function is required, a sample wrapper scoring function is provided later in the notebook.\n",
    "\n",
    "\n",
    "The computed metrics can be displayed in cell output as json or table format. The computed metrics can also be saved to OpenPages; this will prevent the metrics from being computed again the next time you run evaluation; instead it will retrieve the saved metrics from OpenPages. \n",
    "\n",
    "If you want to store metrics to OpenPages then you must provide CPD or Cloud credentials in the system_credentials below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210933df-afce-44c0-a445-e3d2a59fcd42",
   "metadata": {},
   "source": [
    "## Credentials Details:\n",
    "\n",
    "\n",
    "#### Below are the inputs for the API:\n",
    "\n",
    "| API input       | Expected value | optional/required| \n",
    "|------------|-----|-----|\n",
    "| system_credentials      | Contains necessary details to connect to OpenPages, and IBM watsonx.ai. |Required | \n",
    "| risk_dimensions      |  List of risks to be evaluated. If set to None; all the available risk will be evaluated  |Optional | \n",
    "| scoring_function      |  Function that encapsulates all logic to infer external LLM. |Optional | \n",
    "| foundation_model_name      | Specifies the name of the foundation model under evaluation. Needed if user want to export metrics to a pdf|Optional | \n",
    "| max_sample_size      | Set maximum data instances for evaluation. Defaults to None, which will load all data and increase evaluation time|Optional | \n",
    "\n",
    "#### Below are system_credentials dictionary keys definitions:\n",
    "| Dictionary key       | Expected dictionary value |  optional/required| \n",
    "|------------|-----|-----|\n",
    "| op_url      |  Wx.gov Software URL>/openpages-openpagesinstance-cr-grc  | Optional | \n",
    "| op_username      | OpenPages cloud instance username  |Optional  | \n",
    "| op_password      | OpenPages cloud instance password  |Optional | \n",
    "| op_model_name      | OpenPages FM Model ID to which metrics needs to be published  |Optional | \n",
    "| op_cpd_host      | Your CPD or wx.gov Software URL - without https:// |Optional | \n",
    "| op_cpd_apikey      | CPD apikey for OpenPages  |Optional | \n",
    "| watsonx_ai_cloud_apikey      | Your cloud apikey for Wastonx.ai |Required (if you do not provide scoring function for external LLM )| \n",
    "| watsonx_ai_project_id      | Project id for inference against the watsonx.ai model  |Required (if you do not provide scoring function for external LLM ) | \n",
    "| watsonx_ai_endpoint_url      | Cloud Software URL for Wastonx.ai  |Required (if you do not provide scoring function external for LLM ) | \n",
    "| watsonx_ai_model_id      |Wastonx.ai model ID ex:google/flan-t5-xxl   |Required (if you do not provide scoring function external for LLM ) | \n",
    "| watsonx_ai_cpd_username      | Your CPD username for Wastonx.ai |Required (if you do not provide scoring function external for LLM ) | \n",
    "| watsonx_ai_cpd_apikey      | Your CPD api key for Wastonx.ai  |Required (if you do not provide scoring function external for LLM ) | \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd538972",
   "metadata": {
    "msg_id": "c48c0825-a40e-44bd-bc8f-447f3862a072"
   },
   "outputs": [],
   "source": [
    "OP_URL = \"<EDIT THIS>\"\n",
    "OP_USERNAME = \"<EDIT THIS>\"\n",
    "OP_PASSWORD = \"<EDIT THIS>\"\n",
    "OP_MODEL_NAME = \"<EDIT THIS>\"\n",
    "OP_HOST = \"<EDIT THIS>\"\n",
    "OP_APIKEY = \"<EDIT THIS>\"\n",
    "\n",
    "CLOUD_APIKEY = \"<EDIT THIS>\"\n",
    "PROJECT_ID = \"<EDIT THIS>\"\n",
    "ENDPOINT_URL = \"<EDIT THIS>\"\n",
    "CPD_USERNAME= \"<EDIT THIS>\"\n",
    "CPD_APIKEY = \"<EDIT THIS>\"\n",
    "WATSONX_AI_MODEL_ID = \"<EDIT THIS>\"\n",
    "\n",
    "\n",
    "#Set this variable to the Wastonx.ai model name ex:google/flan-t5-xxl\n",
    "# if evaluating the risk of external LLM set variable to the FM under evaluation.\n",
    "foundation_model_name = \"<EDIT THIS>\"\n",
    "\n",
    "\n",
    "risk_dimensions =  \"[<EDIT THIS>]\"\n",
    "\n",
    "\n",
    "#This setup will access wastonx.ai using Cloud\n",
    "system_credentials = {\n",
    "    \"watsonx_ai_cloud_apikey\": CLOUD_APIKEY,\n",
    "    \"watsonx_ai_endpoint_url\": ENDPOINT_URL,\n",
    "    \"watsonx_ai_project_id\": PROJECT_ID,\n",
    "    \"watsonx_ai_model_id\": WATSONX_AI_MODEL_ID,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# uncomment to access wastonx.ai using CPD\n",
    "# system_credentials = {\n",
    "#     \"watsonx_ai_endpoint_url\": ENDPOINT_URL,\n",
    "#     \"watsonx_ai_project_id\": PROJECT_ID,\n",
    "#     \"watsonx_ai_cpd_username\": CPD_USERNAME,\n",
    "#     \"watsonx_ai_cpd_apikey\": CPD_APIKEY,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# uncomment below step if the result should be pushed to openpages using CPD\n",
    "# system_credentials.update({\n",
    "#     \"op_cpd_host\": OP_HOST,\n",
    "#     \"op_username\": OP_USERNAME,\n",
    "#     \"op_password\": OP_PASSWORD, \n",
    "#     \"op_cpd_apikey\": OP_APIKEY,  \n",
    "#     \"op_model_name\": OP_MODEL_NAME,\n",
    "#     \"op_url\": OP_URL\n",
    "# })\n",
    "\n",
    "\n",
    "# uncomment below step if the result should be pushed to openpages using Cloud\n",
    "# system_credentials.update({\n",
    "#     \"op_username\": OP_USERNAME,\n",
    "#     \"op_password\": OP_PASSWORD, \n",
    "#     \"op_model_name\": OP_MODEL_NAME,\n",
    "#     \"op_url\": OP_URL\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbe8ca-baa6-4c6d-8724-d0ea5f05654a",
   "metadata": {},
   "source": [
    "### Scoring function\n",
    "For evaluating the risk of external LLM, a wrapper scoring function is required. The scoring function takes the prompts as input and return the model predictions. a sample scoring function is provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33a6329-c012-4e8a-8924-41507ce75479",
   "metadata": {
    "msg_id": "1068bcf7-fd9b-4b5f-884f-25ac7b24f67f"
   },
   "outputs": [],
   "source": [
    "scoring_function = None\n",
    "##sample_scoring_function: For evaluating the risk of external LLM, uncomment the below code to use the sample_scoring_function instead of watsonx_ai\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install SentencePiece\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# import pandas as pd\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "\n",
    "# def sample_scoring_function(data):\n",
    "#     predictions_list = []\n",
    "#     print(\"working\")\n",
    "#     for prompt_text in data.iloc[:, 0].values.tolist():\n",
    "#         input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "#         output = model.generate(input_ids)\n",
    "#         output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#         predictions_list.append(output)\n",
    "#     return pd.DataFrame({\"generated_text\": predictions_list})\n",
    "\n",
    "\n",
    "# scoring_function = sample_scoring_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe49c3-1b97-44e6-bb87-03a32e1da5c4",
   "metadata": {
    "msg_id": "b2858767-a90a-4a4d-9db7-e624f317b3fd"
   },
   "outputs": [],
   "source": [
    "from ibm_metrics_plugin.mra.evaluate_fm_risk import EvaluateFMRisk\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import  create_download_link_for_file\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!python -m nltk.downloader stopwords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d706bf",
   "metadata": {},
   "source": [
    "### Evaluate the risks for a FM and show the computed metrics as JSON \n",
    "\n",
    "In the cell below, max_sample_size = 5 to demonstrate the use case and save time. However, for a proper evaluation of the LLM, you should use the full dataset to obtain meaningful results. \n",
    "\n",
    "Note: If the full dataset is used for evaluation; expect the evaluation to run for hours depending on the risk, and the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "077f8ae2-aef7-4264-9c3e-49fc4101fd68",
   "metadata": {
    "msg_id": "787640f4-ba7a-43c0-81d2-3e65a39e8c7b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3ec49193a445bda541c9e1532e0f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3aac4aa8cf941ebaa860cd6784b8665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "{'hallucination': {'cards.value_alignment.hallucinations.truthfulqa': {'score_name': 'rougeL', 'score': 0.144, 'num_of_instances': 5, 'counts': 1.0, 'totals': 8.0, 'precisions': 0.106, 'bp': 0.02, 'sys_len': 14, 'ref_len': 69, 'sacrebleu': 0.001, 'score_ci_low': 0.054, 'score_ci_high': 0.209, 'sacrebleu_ci_low': 0.0, 'sacrebleu_ci_high': 0.003, 'rougeL': 0.144, 'rouge1': 0.144, 'rouge2': 0.019, 'rougeLsum': 0.144, 'rougeL_ci_low': 0.054, 'rougeL_ci_high': 0.209, 'rouge1_ci_low': 0.054, 'rouge1_ci_high': 0.209, 'rouge2_ci_low': 0.0, 'rouge2_ci_high': 0.076, 'rougeLsum_ci_low': 0.054, 'rougeLsum_ci_high': 0.209}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fm_evaluate = EvaluateFMRisk(\n",
    "    system_credentials=system_credentials,\n",
    "    foundation_model_name=foundation_model_name,\n",
    "    risk_dimensions=risk_dimensions,\n",
    "    scoring_function=scoring_function,\n",
    "    max_sample_size = 5\n",
    ")\n",
    "risks_metrics_results = fm_evaluate.evaluate_fm_risks()\n",
    "print(risks_metrics_results)\n",
    "\n",
    "\n",
    "\n",
    "## uncomment  below to use the full dataset and obtain meaningful results. \n",
    "#max_sample_size = None\n",
    "\n",
    "# fm_evaluate = EvaluateFMRisk(\n",
    "#     system_credentials=system_credentials,\n",
    "#     foundation_model_name=foundation_model_name,\n",
    "#     risk_dimensions=risk_dimensions,\n",
    "#     scoring_function=scoring_function,\n",
    "#     max_sample_size = max_sample_size\n",
    "# )\n",
    "\n",
    "# risks_metrics_results = fm_evaluate.evaluate_fm_risks()\n",
    "# print(risks_metrics_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4ff56",
   "metadata": {},
   "source": [
    "### Show the computed metrics in a table format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d6170f",
   "metadata": {
    "msg_id": "cacce334-b89f-4943-b3fd-b43a11f68461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Name</th>\n",
       "      <th>cards.value_alignment.hallucinations.truthfulqa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bp</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>counts</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_of_instances</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precisions</td>\n",
       "      <td>0.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ref_len</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rouge1_ci_high</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rouge1_ci_low</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rouge2_ci_high</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rouge2_ci_low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rougeL_ci_high</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rougeL_ci_low</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rougeLsum_ci_high</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rougeLsum_ci_low</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sacrebleu</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sacrebleu_ci_high</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sacrebleu_ci_low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>score</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>score_ci_high</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>score_ci_low</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>score_name</td>\n",
       "      <td>rougeL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sys_len</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>totals</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metric Name cards.value_alignment.hallucinations.truthfulqa\n",
       "0                  bp                                            0.02\n",
       "1              counts                                             1.0\n",
       "2    num_of_instances                                               5\n",
       "3          precisions                                           0.106\n",
       "4             ref_len                                              69\n",
       "5              rouge1                                           0.144\n",
       "6      rouge1_ci_high                                           0.209\n",
       "7       rouge1_ci_low                                           0.054\n",
       "8              rouge2                                           0.019\n",
       "9      rouge2_ci_high                                           0.076\n",
       "10      rouge2_ci_low                                             0.0\n",
       "11             rougeL                                           0.144\n",
       "12     rougeL_ci_high                                           0.209\n",
       "13      rougeL_ci_low                                           0.054\n",
       "14          rougeLsum                                           0.144\n",
       "15  rougeLsum_ci_high                                           0.209\n",
       "16   rougeLsum_ci_low                                           0.054\n",
       "17          sacrebleu                                           0.001\n",
       "18  sacrebleu_ci_high                                           0.003\n",
       "19   sacrebleu_ci_low                                             0.0\n",
       "20              score                                           0.144\n",
       "21      score_ci_high                                           0.209\n",
       "22       score_ci_low                                           0.054\n",
       "23         score_name                                          rougeL\n",
       "24            sys_len                                              14\n",
       "25             totals                                             8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for risk_name, risks_metrics in risks_metrics_results.items():\n",
    "    df = pd.DataFrame.from_dict(risks_metrics)\n",
    "    df = df.reset_index().rename(columns={\"index\":\"Metric Name\"})\n",
    "    print(risk_name)\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e62f8f",
   "metadata": {},
   "source": [
    "### Export the computed metrics to PDF report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d40cb",
   "metadata": {
    "msg_id": "0894bc8a-99d8-48e9-8ce5-5b52bd7322b7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "output_path = current_directory\n",
    "\n",
    "\n",
    "\n",
    "pdf_path = fm_evaluate.get_pdf_report(risks_metrics_results, output_path)\n",
    "pdf_file = create_download_link_for_file(pdf_path)\n",
    "display((pdf_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be8bf7",
   "metadata": {},
   "source": [
    "### (Optional step) HuggingFace\n",
    "Some models used by risks ex: [social-bias] may require access to IBM gated models. In this case it is necessary to set an environment variable with a Hugging face token.\n",
    "\n",
    "For example to get access to\n",
    "\n",
    "https://huggingface.co/ibm/social-bias-detector-v0\n",
    "\n",
    "join the IBM HF interest group: https://huggingface.co/ibm\n",
    "\n",
    "\n",
    "Note: If the token is not found the risk require access to IBM gated models; the risk will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc050898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN = \"<EDIT THIS>\"\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = HF_TOKEN\n",
    "os.system(f\"export HF_ACCESS_TOKEN={HF_TOKEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea05f8d",
   "metadata": {},
   "source": [
    "### (Optional step)  Wastonx.ai\n",
    "Some risks use the LLM as a judge approach, where a Judge LLM is used to evaluate the output of the LLM under investigation.\n",
    "If the judge LLM is too large to run on the client's machine then it must be run on Wastonx.ai. it is necessary to have access to a Wastonx.ai account and to set the aprropriate credentuals in oreder to be able to use these risks.\n",
    "Note: If the token is not found the risk simply will be skiiped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e60011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WML_URL\"] = ENDPOINT_URL\n",
    "os.system(f\"export WML_URL={ENDPOINT_URL}\")\n",
    "\n",
    "os.environ[\"WML_PROJECT_ID\"] = PROJECT_ID\n",
    "os.system(f\"export WML_PROJECT_ID={PROJECT_ID}\")\n",
    "\n",
    "os.environ[\"WML_APIKEY\"] = CLOUD_APIKEY\n",
    "os.system(f\"export WML_APIKEY={CLOUD_APIKEY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287be06",
   "metadata": {},
   "source": [
    "### (Optional step) Evaluate the risks for a FM again to include any risks that was not computed due to the missing HF token and Wastonx.ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_evaluate = EvaluateFMRisk(\n",
    "    system_credentials=system_credentials,\n",
    "    foundation_model_name=foundation_model_name,\n",
    "    risk_dimensions=risk_dimensions,\n",
    "    scoring_function=scoring_function,\n",
    "    max_sample_size = 5\n",
    ")\n",
    "\n",
    "risks_metrics_results = fm_evaluate.evaluate_fm_risks()\n",
    "print(risks_metrics_results)\n",
    "\n",
    "\n",
    "\n",
    "## uncomment  below to use the full dataset and obtain meaningful results. \n",
    "#max_sample_size = None\n",
    "\n",
    "# fm_evaluate = EvaluateFMRisk(\n",
    "#     system_credentials=system_credentials,\n",
    "#     foundation_model_name=foundation_model_name,\n",
    "#     risk_dimensions=risk_dimensions,\n",
    "#     scoring_function=scoring_function,\n",
    "#     max_sample_size = max_sample_size\n",
    "# )\n",
    "\n",
    "# risks_metrics_results = fm_evaluate.evaluate_fm_risks()\n",
    "# print(risks_metrics_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mra008)",
   "language": "python",
   "name": "mra008"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
