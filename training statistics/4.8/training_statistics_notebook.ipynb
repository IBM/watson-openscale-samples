{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345cc5d9",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867c1b4",
   "metadata": {},
   "source": [
    "# IBM Watson OpenScale - Generate Configuration Archive\n",
    "\n",
    "This notebook demonstrates how to generate a configuration archive for monitoring deployments in IBM Watson OpenScale. This configuration is targetted for `System-Managed` monitored deployments.\n",
    "\n",
    "***Target audience for this notebook:***\n",
    "This notebook is targetted for users who fall in either of the below categories:\n",
    "- Users who cannot provide training data (*as CSV or in DB2 or COS*) while configuring a deployment for monitoring in IBM Watson OpenScale\n",
    "- Users who have large amount of training data (> 500MB) and as such can not be used for creating artifacts in IBM Watson OpenScale\n",
    "- Users who are looking for automation and/or more granular control using Python SDK.\n",
    "\n",
    "User must provide the necessary inputs where marked. Generated configuration package can be used in IBM Watson OpenScale UI while configuring monitoring of a model deployment in IBM Watson OpenScale.\n",
    "\n",
    "**Contents:**\n",
    "1. [Setting up the environment](#setting-up-the-environment) - Pre-requisites: Install Libraries and required dependencies\n",
    "2. [Training Data](#training-data) - Read the training data as a pandas DataFrame\n",
    "3. [User Inputs Section](#user-inputs-section) - Provide Model Details, IBM Watson OpenScale Services and their configuration\n",
    "4. [Generate Configuration Archive](#generate-configuration-archive)\n",
    "5. [Helper Methods](#helper-methods)\n",
    "6. [Definitions](#definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd79c5",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==1.1.1 using conda before installing ibm-metrics-plugin\n",
    "# !conda install scikit-learn=1.1.1\n",
    "# !conda install -y cmake cython==0.29.33 openblas==0.3.21 qdldl-python==0.1.7 pandas==1.4.4 pyparsing==2.4.7 statsmodels==0.13.2\n",
    "# !git clone https://github.com/tommyod/KDEpy && cd KDEpy && git checkout d52233099978dec38fa622fc86ce5e10368db1bd && rm -f KDEpy/cutils.c && CC=gcc python -m pip install . && cd .. && rm -rf KDEpy\n",
    "# !CC=gcc python -m pip install jenkspy==0.2.0 retrying==1.3.4 marshmallow==3.10.0 more-itertools==8.12.0 numba==0.57.1\n",
    "# !python -m pip install --no-build-isolation osqp==0.6.2 cvxpy==1.3.2\n",
    "# !python -m pip install shap==0.41.0\n",
    "# !CC=gcc GXX=g++ python -m pip install ibm-metrics-plugin==5.0.1.17\n",
    "\n",
    "# If running the notebooks against Default Spark runtimes on CP4D clusters, please replace the following command\n",
    "%pip install --upgrade \"ibm-metrics-plugin==5.0.1.17\" \"ibm-watson-openscale>=3.0.31\" | tail -n 1\n",
    "\n",
    "# with\n",
    "# %pip install --upgrade \"ibm-metrics-plugin==5.0.1.17\" \"ibm-watson-openscale>=3.0.32\" -t /home/spark/shared/user-libs/python | tail -n 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57774483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5900-A3Q, 5737-H76\n",
    "# Copyright IBM Corp. 2018, 2023\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade \n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "VERSION = \"7.0.0\"\n",
    "# Version history\n",
    "# 7.0.0 : Official notebook for IBM CP4D 4.8.x\n",
    "#         Upgrade ibm-metrics-plugin to >= 4.8.0;\n",
    "# 6.0.0 : Complete Re-design of the notebook; Official notebook for IBM CP4D 4.7.x\n",
    "#         Upgrade ibm-metrics-plugin to >= 4.7.0; Includes Support for Drift v2 Archive\n",
    "#         Refactored for Configuration Package; Deprecate the Drift Archive\n",
    "# 5.4.7 : Official notebook for IBM CP4D 4.6.4\n",
    "#         Upgrade ibm-metrics-plugin to >= 4.6.4.0\n",
    "# 5.4.6 : Take optional class labels input for global explainability\n",
    "# 5.4.5 : Remove numpy and scipy versions to be installed.\n",
    "# 5.4.4 : Add support for lime global explanation\n",
    "# 5.4.3 : Add numpy and scipy versions to be installed.\n",
    "# 5.4.2 : Remove explainability configuration while saving training_distribution\n",
    "# 5.4.1 : Add sample size for generating global explanation\n",
    "# 5.4.0 : Add support for SHAP Global explanation\n",
    "# 5.3.6 : Fix issue with explainability archive generation for regression model\n",
    "# 5.3.5 : Official notebook for IBM CPD 4.5.0. \n",
    "#         Upgrade ibm-wos-utils to 4.5.0. \n",
    "#         Added code to generate explainability perturbations archive.\n",
    "# 5.3.4 : Upgrade ibm-wos-utils to 4.1.1 (scikit-learn has been upgraded to 1.0.2)\n",
    "# 5.3.3 : Upgrade ibm-wos-utils to 4.0.34\n",
    "# 5.3.2 : Upgrade ibm-wos-utils to 4.0.31\n",
    "# 5.3.1 : Official notebook for IBM CPD 4.0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a737d3",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "*Note: Pandas' read\\_csv method converts the columns to its data types. If you want the column type to not be interpreted, specify the dtype param to read_csv method in this cell. More on this method [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)*\n",
    "\n",
    "*Note: By default NA values will be dropped while computing training data distribution. Please ensure to handle the NA values during Pandas' read\\_csv method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data_df = pd.read_csv(\"TO_BE_EDITED\")\n",
    "\n",
    "print(training_data_df.head())\n",
    "print(\"Columns:{}\".format(list(training_data_df.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c4767",
   "metadata": {},
   "source": [
    "## User Inputs Section\n",
    "\n",
    "##### _1. Provide Common Parameters_:\n",
    "\n",
    "Provide the common parameters like the basic model details like type, feature columns, etc. Also, enable/disable the different monitors you would like th artifacts for. Read more about these [here](#common-parameters). \n",
    "\n",
    "##### _2. Provide Fairness Parameters_\n",
    "The fairness parameters are required if `enable_fairness` is set to `True`. Read more about these parameters [here](#fairness-parameters)\n",
    "\n",
    "##### _3. Provide Explainability Parameters_\n",
    "The explainability parameters are required if `enable_explainability` is set to `True`. Read more about these parameters [here](#explainability-parameters)\n",
    "\n",
    "*When LIME global explanation is enabled, the explainability archive upload and explainability monitor enablement should be done using python sdk/api.*\n",
    "*LIME global explanation configuration is not supported from IBM Watson OpenScale GUI.*\n",
    "\n",
    "##### _4. Provide Drift v2 Parameters_\n",
    "Read more about these parameters [here](#drift-v2-parameters)\n",
    "\n",
    "##### _5. *DEPRECATED* Provide Drift Parameters_\n",
    "Read more about these parameters [here](#deprecated-drift-parameters)\n",
    "\n",
    "##### _6. Provide a scoring function_\n",
    "The scoring function is required if any of  `enable_explainability`, `enable_drift_v2` or `enable_drift` is set to `True`. The scoring function should adhere to the following guidelines.\n",
    "\n",
    "- The input of the scoring function should accept a `pandas.DataFrame` containing all the `feature_columns` used to build the model.\n",
    "- The output of the scoring function should return:\n",
    "    - a `tuple` of `(probabilities, predictions)` for classification problems. Both `probabilities` and `predictions` are of type `numpy.ndarray`\n",
    "    - a `numpy.ndarray` of `predictions` for regression problems.\n",
    "- The data type of the label column and prediction column should be same. Moreover, the label column and the prediction column array should have the same unique class labels\n",
    "- A host of different scoring function templates are provided [here](https://github.com/IBM/watson-openscale-samples/wiki/Score-function-templates-for-IBM-Watson-OpenScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97033396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_parameters = {\n",
    "    \"problem_type\" : \"TO_BE_EDITED\", \n",
    "    \"label_column\": \"TO_BE_EDITED\",\n",
    "    \"prediction_column\": \"TO_BE_EDITED\",\n",
    "    \"probability_column\": \"TO_BE_EDITED\", # <- Not required for Regression problems.\n",
    "    \"class_probabilities\": [\"TO_BE_EDITED\"], # <- Optional. Not required for Regression problems.\n",
    "    \"feature_columns\": [\"TO_BE_EDITED\"], # <- If not provided, all columns in data frame except \n",
    "                                       # (label, prediction and probability) columns, will be\n",
    "                                       # marked as feature_columns\n",
    "    \"categorical_columns\": [\"TO_BE_EDITED\"], # <- If not provided, all columns of dtype object \n",
    "                                           # and bool in feature columns, will be marked as\n",
    "                                           # categorical_columns.\n",
    "    \"enable_quality\": True,\n",
    "    \"enable_fairness\": True,\n",
    "    \"enable_explainability\": True,\n",
    "    \"enable_drift_v2\": True,\n",
    "    # \"enable_drift\": False, # <- set this to True for creating the archive for the DEPRECATED Drift monitor\n",
    "    \"notebook_version\": VERSION\n",
    "}\n",
    "\n",
    "fairness_parameters = {\n",
    "    \"fairness_attributes\": [\n",
    "        {\n",
    "            \"type\": \"TO_BE_EDITED\",\n",
    "            \"feature\": \"TO_BE_EDITED\",\n",
    "            \"majority\": [\n",
    "                \"TO_BE_EDITED\"\n",
    "            ],\n",
    "            \"minority\": [\n",
    "                \"TO_BE_EDITED\"\n",
    "            ],\n",
    "            \"threshold\": 0.8\n",
    "        }\n",
    "    ],\n",
    "    \"min_records\" : \"TO_BE_EDITED\",\n",
    "    \"max_records\": None,\n",
    "    \"favourable_class\" : [\"TO_BE_EDITED\"],\n",
    "    \"unfavourable_class\": [\"TO_BE_EDITED\"]\n",
    "}\n",
    "\n",
    "explainability_parameters = {\n",
    "    \"global_explanation\": {\n",
    "        \"enabled\": True,\n",
    "        \"sample_size\": 50,\n",
    "        \"training_data_sample_size\": 100,\n",
    "        \"explanation_method\": \"lime\"\n",
    "    },\n",
    "    \"shap\": {\n",
    "        \"enabled\": False,\n",
    "#         \"perturbations_count\": 100,\n",
    "#         \"background_data_set\": None,\n",
    "#         \"background_data_sets\": []\n",
    "    },\n",
    "    \"lime\": {\n",
    "        \"enabled\": True,\n",
    "#         \"perturbations_count\": 5000\n",
    "    },\n",
    "    \"local_explanation_method\": \"lime\",\n",
    "}\n",
    "\n",
    "\n",
    "drift_v2_parameters = {\n",
    "    # \"max_samples\": 10000\n",
    "}\n",
    "\n",
    "# Drift is being deprecated in favour of Drift v2.\n",
    "# drift_parameters = {\n",
    "#     \"data_drift\": {\n",
    "#         \"two_column_learner_limit\": 200,\n",
    "#         \"categorical_unique_threshold\": 0.8,\n",
    "# #         \"user_overrides\": [\n",
    "# #             {\n",
    "# #                 \"constraint_type\": \"TO_BE_EDITED\",\n",
    "# #                 \"learn_distribution_constraint\": \"TO_BE_EDITED\",\n",
    "# #                 \"learn_range_constraint\": \"TO_BE_EDITED\",\n",
    "# #                 \"features\": [\"TO_BE_EDITED\"]\n",
    "# #             }\n",
    "# #         ]\n",
    "#     },\n",
    "#     \"model_drift\": {\n",
    "#         \"optimise\": True,\n",
    "#         \"check_for_ddm_quality\": False,\n",
    "#         \"ddm_quality_check_threshold\": 0.3\n",
    "#     }\n",
    "# }\n",
    "\n",
    "scoring_fn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb2879",
   "metadata": {},
   "source": [
    "## Generate Configuration Archive\n",
    "\n",
    "Run the following code to generate the configuration archive for the IBM Watson OpenScale monitors. This archive is used as is by IBM Watson OpenScale UI/SDK to onboard model for monitoring. UI/SDK will identify the different artifacts and appropriately upload to respective monitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc16aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_openscale.utils.configuration_utility import ConfigurationUtility\n",
    "\n",
    "config_util = ConfigurationUtility(\n",
    "    training_data=training_data_df,\n",
    "    common_parameters=common_parameters,\n",
    "    scoring_fn=scoring_fn if \"scoring_fn\" in locals() else None)\n",
    "\n",
    "config_util.create_configuration_package(\n",
    "    explainability_parameters=explainability_parameters if \"explainability_parameters\" in locals() else None,\n",
    "    drift_v2_parameters=drift_v2_parameters if \"drift_v2_parameters\" in locals() else {},\n",
    "    fairness_parameters=fairness_parameters if \"fairness_parameters\" in locals() else None,\n",
    "    display_link=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b3a04",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f3ac3",
   "metadata": {},
   "source": [
    "### Read file in COS to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ibm-cos-sdk\n",
    "\n",
    "import ibm_boto3\n",
    "import pandas as pd\n",
    "import sys\n",
    "import types\n",
    "\n",
    "from ibm_botocore.client import Config\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "api_key = \"TO_BE_EDITED\" # cos api key\n",
    "resource_instance_id = \"TO_BE_EDITED\" # cos resource instance id\n",
    "service_endpoint =  \"TO_BE_EDITED\" # cos service region endpoint\n",
    "bucket =  \"TO_BE_EDITED\" # cos bucket name\n",
    "file_name= \"TO_BE_EDITED\" # cos file name\n",
    "auth_endpoint = \"https://iam.ng.bluemix.net/oidc/token\"\n",
    "\n",
    "cos_client = ibm_boto3.client(service_name=\"s3\",\n",
    "    ibm_api_key_id=api_key,\n",
    "    ibm_auth_endpoint=auth_endpoint,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=service_endpoint)\n",
    "\n",
    "body = cos_client.get_object(Bucket=bucket,Key=file_name)[\"Body\"]\n",
    "\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "training_data_df = pd.read_csv(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58012329",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681fcdd",
   "metadata": {},
   "source": [
    "### Common Parameters\n",
    "\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "|:-|:-|:-|:-|\n",
    "| model_type | Enumeration classifying if your model is a binary or a multi-class classifier or a regressor. |  | `binary`, `multiclass`, `regression` |\n",
    "| label_column | The column which contains the target field (also known as label column or the class label). |  | A string value referring column name |\n",
    "| feature_columns | Columns identified as features by model. The order of the feature columns should be same as that of the subscription. Use helper methods to compute these if required. |  | A list of column names |\n",
    "| categorical_columns | Feature columns identified as categorical by model. Use helper methods to compute these if required. |  | A list of column names |\n",
    "| prediction_column | The column containing the model output. This should be of the same data type as the label column. |  | A string value referring column name |\n",
    "| probability_column | The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models. One of `probability_column` or `class_probabilities` must be specified for classification models. If both are specified, `class_probabilities` is preferred.|  | A string value referring column name |\n",
    "| class_probabilities | The columns (of type double) containing the model probabilities of class labels. This is not required for regression models. For example, for Go Sales model deployed in MS Azure ML Studio, value of this property would be `[\"Scored Probabilities for Class \\\"Camping Equipment\\\"\", \"Scored Probabilities for Class \\\"Mountaineering Equipment\\\"\", \"Scored Probabilities for Class \\\"Personal Accessories\\\"\"]`. Please note escaping double quotes is a must-have requirement for above example. One of `probability_column` or `class_probabilities` must be specified for classification models. If both are specified, `class_probabilities` is preferred. |  | A list of column names |\n",
    "| enable_quality | Boolean value to enable the Quality monitor | `True` | `True` or `False` |\n",
    "| enable_fairness | Boolean value to allow generation of fairness specific data distribution needed for configuration | `True` | `True` or `False` |\n",
    "| enable_explainability | Boolean value to allow generation of explainability configuration | `True` | `True` or `False` |\n",
    "| enable_drift_v2 | Boolean value to allow generation of Drift v2 Archive. | `True` | `True` or `False` |\n",
    "| enable_drift | Boolean value to allow generation of *DEPRECATED* Drift Archive containing relevant information for Model and Data Drift. | `False` | `True` or `False` |\n",
    "\n",
    "\n",
    "### Fairness Parameters\n",
    "Provide the fairness parameters. Leave the variable `fairness_parameters` to `None` or `{}` if fairness is not to be enabled.\n",
    "\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "| :- | :- | :- | :- |\n",
    "| fairness_attributes.type | Data type of the fairness attribute. | | `float`, `int`, `double`, `string`, etc.|\n",
    "| fairness_attributes.feature | Defines feature to monitor for fairness. | | |\n",
    "| fairness_attributes.majority | Defines majority group towards which the model might be biased. | | |\n",
    "| fairness_attributes.minority | Defines minority group for which we want to ensure that the model is not biased. | | |\n",
    "| fairness_attributes.threshold | Defines value beyond which the model is considered to be biased. | | |\n",
    "| min_records | Number of (latest) records to use for computing fairness. If we set the value of \"min_records\" to a small number, then fairness computation will get influenced by the scoring requests sent to the model in the recent past. In other words, the model might be flagged as being biased if it is acting in a biased manner on the last few records, but overall it might not be acting in a biased manner. On the other hand, if the \"min_records\" is set to a very large number, then we will not be able to catch model bias quickly. Hence the value of min_records should be set such that it is neither too small or too large. | | |\n",
    "| max_records | Optional parameter. | | |\n",
    "| favourable_class | Class labels considered to be expected outcome. For regression models, this is defined as range of values. | | |\n",
    "| unfavourable_class | Class labels considered to be un-expected outcome. For regression models, this is defined as range of values. | | |\n",
    "\n",
    "Example:\n",
    "For a Loan Processing Model:\n",
    "- We want to ensure that the model is not biased against people of specific age group and people belonging to a specific gender. Hence `Applicant_Age` and `Gender` will be the fairness attributes for this model.\n",
    "- Majority group for the fairness attribute `Applicant_Age` is defined as `[31,60]`, i.e., all the ages except the minority group. For the fairness attribute `Gender`, the majority group is defined as `Male`. \n",
    "- Further, we want to ensure that model is not biased against people in the age group 15 to 30 years & 61 to 120 years as well as people with Gender = Female or Gender = Transgender. Hence, minority group for the fairness attribute `Applicant_Age` is defined as `[15,30]` and `[61,120]` and minority group for fairness attribute `Gender` is defined as `Female`, `Transgender`.\n",
    "- Let us say that the Bank is willing to tolerate the fact that Female and Transgender applicants will get up to 20% lesser approved loans than Males. However, if the percentage is more than 20% then the Loan Processing Model will be considered biased. E.g., if the percentage of approved loans for Female or Transgender applicants is say 25% lesser than those approved for Male applicants then the Model is to be considered as acting in a biased manner. Thus for this scenario, the Fairness threshold will be `80` (100-20) (this is represented as a value normalized to 1, i.e., 0.8).\n",
    "- In case of Loan Processing Model, the target field (label column or class label) can have the following values: `Loan Granted`, `Loan Denied` and `Loan Partially Granted`. Out of these values `Loan Granted` and `Loan Partially Granted` can be considered as being favorable and `Loan Denied` is unfavorable. In other words in order to measure fairness, we need to know the target field values which can be considered as being favourable and those values which can be considered as unfavourable.\n",
    "- In case of a regression models, the favourable and unfavourable classes will be ranges. For example, for a model which predicts medicine dosage, the favorable outcome could be between 80 ml to 120 ml or between 5 ml to 20 ml whereas unfavorable outcome will be values between 21 ml to 79ml.\n",
    "- Fairness checks runs hourly. If `min_records` is set to `5000`, then every hour fairness checking will pick up the last 5000 records which were sent to the model for scoring and compute fairness using these. Please note that fairness computation will not start till the time that 5000 records are sent to the model for scoring.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"fairness_attributes\": [\n",
    "        {\n",
    "            \"feature\": \"Applicant_Age\",\n",
    "            \"type\": \"int\",\n",
    "            \"majority\": [\n",
    "                [31, 60]\n",
    "            ],\n",
    "            \"minority\": [\n",
    "                [15, 30],\n",
    "                [61, 120]\n",
    "            ],\n",
    "            \"threshold\": 0.8\n",
    "        },\n",
    "        {\n",
    "            \"feature\": \"Gender\",\n",
    "            \"type\": \"string\",\n",
    "            \"majority\": [\"Male\"],\n",
    "            \"minority\": [\"Female\", \"Transgender\"],\n",
    "            \"threshold\": 0.8\n",
    "        }\n",
    "    ],\n",
    "    \"min_records\": 5000,\n",
    "    \"max_records\": null,\n",
    "    \"favourable_class\": [\"Loan Granted\", \"Loan Partially Granted\"],\n",
    "    \"unfavourable_class\": [\"Loan Denied\"]\n",
    "}\n",
    "```\n",
    "\n",
    "- For regression problems, set `favourable_class` and `unfavourable_class` as shown below:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"favourable_class\" : [[5, 20], [80, 120]],\n",
    "    \"unfavourable_class\": [[21, 79]]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Explainability Parameters\n",
    "Provide the explainability parameters. Leave the variable `explainability_parameters` to `None` or `{}` if explainability is not to be enabled.\n",
    "\n",
    "**Note: LIME global explanation feature is supported from Cloud Pak for Data version 4.6.4 onwards**\n",
    "\n",
    "*When LIME global explanation is enabled, the explainability archive upload and explainability monitor enablement should be done using python sdk/api.*\n",
    "*LIME global explanation configuration is not supported from IBM Watson OpenScale GUI.*\n",
    "\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "| :- | :- | :- | :- |\n",
    "| global_explanation | The global explanation parameters. | | |\n",
    "| global_explanation.enabled | Boolean value to enable of disable global explanation. | | `True` or `False` |\n",
    "| global_explanation.sample_size | The sample size of the records to be considered for computing global explanation in the payload window. | | |\n",
    "| global_explanation.training_data_sample_size | The sample size of the records to be considered for computing global explanation on the training data. | 1000| |\n",
    "| global_explanation.explanation_method | Type of technique to use for generating global explanation. SHAP parameters should be provided when shap explanation method is selected. | `lime` | `shap` or `lime` |\n",
    "| shap | The shap explanation parameters are **mandatory** when SHAP explanation method is selected for local or global explanation. | | |\n",
    "| shap.enabled | Boolean value to enable or disable SHAP based explanations. | | `True` or `False` |\n",
    "| shap.perturbations_count | Number of perturbations to generate during explanation generation. | 100 | |\n",
    "| shap.background_data_set | The background data set to be used when generating the SHAP explanation of a transaction. The background data is used to determine the average predicted value for regression models and the average confidence value for classification models. When generating a local explanation, SHAP computes the shapley values which signify how much each feature contributed to moving the model output or model confidence from the computed average value. If not defined, it is auto-generated using training data. | | |\n",
    "| shap.background_data_sets | A list of available background_data_set which can be used for configuration. | | |\n",
    "| lime.enabled | Boolean value to enable or disable LIME based explanations. | | `True` or `False` |\n",
    "| lime.perturbations_count | Number of perturbations to generate during explanation generation. | 5000 | |\n",
    "| local_explanation_method | Set explanation method to use for generating local explanations. | | `shap` or `lime` |\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"global_explanation\": {\n",
    "        \"enabled\": true,\n",
    "        \"sample_size\": 50,\n",
    "        \"training_data_sample_size\": 1000,\n",
    "        \"explanation_method\": \"shap\"\n",
    "    },\n",
    "    \"shap\": {\n",
    "        \"enabled\": true,\n",
    "        \"perturbations_count\": 100,\n",
    "        \"background_data_set\": \"data_set_1\",\n",
    "        \"background_data_sets\": [\n",
    "            {\n",
    "                \"name\": \"data_set_1\",\n",
    "                \"file_name\": \"data_set_1.csv\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"lime\": {\n",
    "        \"enabled\": true,\n",
    "        \"perturbations_count\": 5000\n",
    "    },\n",
    "    \"local_explanation_method\": \"shap\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Drift v2 Parameters\n",
    "\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "| :- | :- | :- | :- |\n",
    "| max_samples | Defines maximum sample size on which the drift v2 archive is created. | None | |\n",
    "\n",
    "### *DEPRECATED* Drift Parameters\n",
    "\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "| :- | :- | :- | :- |\n",
    "| two_column_learner_limit | Defines upper limit on number of two-column constraints to learn. | 200 | |\n",
    "| categorical_unique_threshold | Defines the threshold on percentage of unique categories. Ignore categorical column from constraint learning process if breached. | 0.8 | |\n",
    "| user_overrides | Define custom rules for including or excluding columns during constraining learning. | | |\n",
    "| constraint_type | Identifies type of constraint to apply rule to. | | `single` or `double` |\n",
    "| learn_distribution_constraint | Boolean value enabling or disabling distribution constraint learning. | | `True` or `False` |\n",
    "| learn_range_constraint | Boolean value enabling or disabling range constraint learning. | | `True` or `False` |\n",
    "| features | Define features or a combination of features to apply rule on. | | |\n",
    "\n",
    "In the example below:\n",
    "- first config block says do not learn distribution and range single column constraints for features `MARITAL_STATUS`, `PROFESSION`, `IS_TENT` and `age`.\n",
    "- Second config block says do not learn distribution and range two column constraints where `IS_TENT`, `PROFESSION`, and `AGE` are one of the two columns. Whereas, specifically, do not learn two column distribution and range constraint on combination of `MARITAL_STATUS` and `PURCHASE_AMOUNT`.\n",
    "\n",
    "```json\n",
    "\"user_overrides\"= [\n",
    "    {\n",
    "        \"constraint_type\": \"single\",\n",
    "        \"learn_distribution_constraint\": False,\n",
    "        \"learn_range_constraint\": False,\n",
    "        \"features\": [\n",
    "          \"MARITAL_STATUS\",\n",
    "          \"PROFESSION\",\n",
    "          \"IS_TENT\",\n",
    "          \"age\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"constraint_type\": \"double\",\n",
    "        \"learn_distribution_constraint\": False,\n",
    "        \"learn_range_constraint\": False,\n",
    "        \"features\": [\n",
    "          [\n",
    "            \"IS_TENT\"\n",
    "          ],\n",
    "          [\n",
    "            \"MARITAL_STATUS\"\n",
    "            \"PURCHASE_AMOUNT\"\n",
    "          ],\n",
    "          [\n",
    "            \"PROFESSION\"\n",
    "          ],\n",
    "          [\n",
    "            \"AGE\"\n",
    "          ]\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
