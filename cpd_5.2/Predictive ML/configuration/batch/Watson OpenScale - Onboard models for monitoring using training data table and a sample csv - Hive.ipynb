{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9cf9b7-d191-4afe-b437-0b0532f51e08",
   "metadata": {
    "id": "4a1eb793-4342-4ada-8765-f469887769e8"
   },
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544829a-7251-40bc-bff1-ead8bf5af668",
   "metadata": {
    "id": "c71a2cd1-3e3f-49d7-8ec2-44ecbb2c382b"
   },
   "source": [
    "# IBM Watson OpenScale - Onboard models for monitoring using scored training data table and a sample csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7207a-42da-48ad-bdf6-f655d4fd9dea",
   "metadata": {
    "id": "21502130-860b-4d9e-b2f4-34fd0741ac8a"
   },
   "source": [
    "This notebook must be run in the Python 3.10 runtime environment. It requires Watson OpenScale service credentials.\n",
    "\n",
    "The notebook demonstrates how to onboard a model (which stores its runtime data in a remote Hive database) for monitoring in IBM Watson OpenScale. Use the notebook to enable quality, drift, drift v2, fairness and explainability monitoring. Before you can run the notebook, you must have the following resources:\n",
    "\n",
    "1. Sample CSV file\n",
    "2. Scored training data table (existing) in Hive\n",
    "3. Feedback, Payload, Drifted transactions, Explanations Queue and Result tables details (either existing or to be created) in an Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fb3f4-feb6-415f-8fc8-49af129fa066",
   "metadata": {
    "id": "99eaf321-4a1d-4f12-ac16-b43c4082b69e"
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Provide path to sample csv file containing training data](#path-to-csv)\n",
    "3. [Provide Storage Details](#backend-storage)\n",
    "4. [Provide Table Details](#table-details)\n",
    "5. [Provide model details](#model_details)\n",
    "2. [Provide Spark Compute Engine Details](#spark)\n",
    "5. [Connect to IBM Watson OpenScale Instance](#connect-openscale)\n",
    "6. [Connect service provider in IBM Watson OpenScale Instance](#create-service-provider)\n",
    "7. [Onboard model for monitoring in IBM Watson OpenScale Instance](#create-subscription)\n",
    "9. [Enable services to monitor model](#enable-monitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467a5e4-fbe5-4ab6-86c9-c7490fdcb4dc",
   "metadata": {
    "id": "c803196d-297f-43b3-8e7f-4045f30650e1"
   },
   "source": [
    "## Setup <a name=\"setup\"></a>\n",
    "\n",
    "### Installing Required Libraries\n",
    "\n",
    "First import some of the packages you need to use. After you finish installing the following software packages, restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f0b22-fd9a-47af-a113-9bfff2fb37af",
   "metadata": {
    "id": "75b16cec-3036-4ffa-8b20-175299d45fdb"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Note: Restart kernel after the dependencies are installed\n",
    "!pip install --upgrade ibm-watson-openscale\n",
    "!pip install \"ibm_wos_utils~=5.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da9632-6ed8-4fb7-b07f-e01f7f8dad2c",
   "metadata": {
    "id": "035c3a02-f76f-410c-b6ae-545355dea9e3"
   },
   "source": [
    "## Provide path to sample csv file containing model input and output including label column <a name=\"path-to-csv\">\n",
    "This csv file is required to understand model input and output columns and their data-types. Provide path location of csv file here.\n",
    "\n",
    "Please note if you are executing this notebook in IBM Watson Studio, first upload the csv file to project and use provided code snippet to download it to local directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b242aa-2f05-42fc-8652-5ab4eb922bab",
   "metadata": {
    "id": "0250c2fb-5ec6-4d26-9db5-14503cbd7bf4"
   },
   "outputs": [],
   "source": [
    "# # Download \"sample_csv\" from project to local directory\n",
    "# from ibm_watson_studio_lib import access_project_or_space\n",
    "# wslib = access_project_or_space()\n",
    "# wslib.download_file(\"sample_csv\")\n",
    "sample_csv = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689af9a-4736-4804-a950-add1f09f80ac",
   "metadata": {
    "id": "4252c22f-d3a0-418e-b493-26fc93e648e8",
    "tags": []
   },
   "source": [
    "## Provide Backend Storage Details <a name=\"backend-storage\"></a>\n",
    "\n",
    "IBM Watson OpenScale services monitors models by analyzing runtime data, i.e., the data model is making predictions on. To do this analysis, most of the services require access to this runtime data (also called payload data). In addition, some of the services may require access to manually labelled runtime data (also called feedback data). Hence, user needs to store such data in some backend storage and connect this storage to IBM Watson OpenScale.\n",
    "\n",
    "### Provide Hive database connection details\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| type | Describes the type of storage being used. For hive, this must be set to `hive`. | `hive` |\n",
    "| metastore_url | An optional string value specifying hive metastore url. Example: `thrift://localhost:9083` | |\n",
    "| location_type | Identifies the type of location for connection to use. For hive, this must be set to `metastore`. | `metastore` |\n",
    "\n",
    "#### Provide additional details related Hadoop delegation token if the Hive is Kerberos secured and Spark in IBM Analytics Engine is used [Optional]\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| kerberos_principal | The kerberos principal used to generate the delegation token. | |\n",
    "| delegation_token_urn | The secret_urn of the CP4D vault where the delegation token is stored. | |\n",
    "| delegation_token_endpoint | The REST endpoint which generates and returns the delegation token. | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07dd9020-9fd6-44bd-abf5-48ae013f1363",
   "metadata": {
    "id": "853f91ae-e660-4f70-92af-84a9cbe00532"
   },
   "outputs": [],
   "source": [
    "datawarehouse_details = {\n",
    "    \"type\": \"hive\",\n",
    "    \"connection\": {\n",
    "        \"location_type\": \"metastore\",\n",
    "        \"metastore_url\": \"\"\n",
    "    },\n",
    "    \"credentials\": {}\n",
    "}\n",
    "\n",
    "# Flag to indicate if the Hive is secured with Kerberos and Spark in IAE is used\n",
    "kerberos_enabled = False\n",
    "\n",
    "# Provide Hadoop delegation token details if kerberos_enabled is True\n",
    "# Provide either secret_urn of the CP4D vault OR the delegation token endpoint. One of the two fields is mandatory to fetch the delegation token.\n",
    "kerberos_principal = \"\"\n",
    "delegation_token_urn = \"\"\n",
    "delegation_token_endpoint = \"\"\n",
    "\n",
    "if kerberos_enabled is True:\n",
    "    datawarehouse_details[\"connection\"][\"kerberos_enabled\"] = True\n",
    "    datawarehouse_details[\"credentials\"][\"kerberos_principal\"] = kerberos_principal\n",
    "    if delegation_token_urn:\n",
    "        datawarehouse_details[\"credentials\"][\"delegation_token_urn\"] = delegation_token_urn\n",
    "    if delegation_token_endpoint:\n",
    "        datawarehouse_details[\"credentials\"][\"delegation_token_endpoint\"] = delegation_token_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1d363-94e9-4588-b4d9-bdacb14c331c",
   "metadata": {
    "id": "25fb1fc0-1c4c-4661-8cd0-5b68a20172fd",
    "tags": []
   },
   "source": [
    "## Provide details of different tables<a name=\"table-details\"></a>\n",
    "\n",
    "IBM Watson OpenScale services require different tables to perform their analysis. Depending on which services you have enabled, provide details of the corresponding tables.\n",
    "Tables are:\n",
    "\n",
    "| Table | Description |\n",
    "| :- | :- |\n",
    "| Payload Table | Hosts the runtime data predicted by model. Required for detecting fairness and drift in runtime data. |\n",
    "| Feedback Table | Hosts the manually labelled runtime data (also called feedback data) predicted by model. Required for tracking quality of monitor by analyzing feedback data. |\n",
    "| Drifted Transactions Table | Hosts the data identified to be drifted.|\n",
    "| Explain Queue Table | Hosts the data for which explanations are required to be generated. This can be same as payload table.|\n",
    "| Explain Results Table | Hosts the explanations generated for records in explain queue table. |\n",
    "| Scored Training Data Table | Contains the details of table containing scored training data. If you dont have this table available, Please refer to [this notebook](https://github.ibm.com/aiopenscale/api-client-utils/blob/master/notebooks/batch/4.6/jdbc/common_configuration_notebook_simplified_jdbc.ipynb) for creating Scored Training Data Table. Scored training data table should be available in the DATABASE. |\n",
    "\n",
    "For each of the table, following information is required:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| database | Name of the database hosting the schema. | |\n",
    "| table | Name of the table. | |\n",
    "| auto_create | Boolean value identifying if the table already exists or has to be created via IBM Watson OpenScale. | `True` or `False`|\n",
    "| hive_storage_format | Storage format to use for data in tables. Used only when tables are created using IBM Watson OpenScale. | `csv`, `parquet`, `orc` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7de8924-597c-4a42-b14c-4447a558c9d7",
   "metadata": {
    "id": "ac52fb68-6b64-4279-87da-edea05f7cecc"
   },
   "outputs": [],
   "source": [
    "DATABASE_NAME= \"\"\n",
    "# Scored training data table information \n",
    "scored_training_data_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": False, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"table\": \"\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8cc192-5e8d-4f74-9e7b-aff4e24509df",
   "metadata": {
    "id": "6cabbe4a-6662-4cd3-8203-cf60a620c5ba"
   },
   "outputs": [],
   "source": [
    "DATABASE_NAME=\"\"\n",
    "\n",
    "# Payload table information\n",
    "payload_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"table\": \"\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Feedback table information\n",
    "feedback_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"table\": \"\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# The below tables are required by monitors\n",
    "\n",
    "#Drifted Transaction table. \n",
    "#Set this table information if drift is enabled\n",
    "drifted_transaction_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"table\": \"\"\n",
    "    },\n",
    "    \"parameters\":{}\n",
    "}\n",
    "\n",
    "#Explanation Result table\n",
    "#Set this table information if Explain is enabled\n",
    "explain_result_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"table\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Explanation Queue table\n",
    "#Set this table information if Explain is enabled\n",
    "explain_queue_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"table\": \"\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de2f07-945c-42c1-a58c-800dd15a85a9",
   "metadata": {
    "id": "b8cde18f-37ec-4ecc-ad60-2e9e5b6da3ff",
    "tags": []
   },
   "source": [
    "## Provide Model Details <a name=\"model-details\"></a>\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| label_column | The column which contains the target field (also known as label column or the class label). | |\n",
    "| model_type | Enumeration classifying if your model is a binary or a multi-class classifier or a regressor. | `binary`, `multiclass`, `regression` |\n",
    "| prediction | The column containing the model output. This should be of the same data type as the label column. | |\n",
    "| probability | The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models. | |\n",
    "| url | scoring url for the deployed model. | |\n",
    "| token | scoring token for the deployed model. This is required only for Azure ML studio model | |\n",
    "| feature_columns | Columns identified as features by model. If user is not providing this, it will be inferred from the input csv file. | A list of column names, `None` |\n",
    "| categorical_columns | Feature columns identified as categorical by model. If user is not providing this, it will be inferred from the input csv file. | A list of column names,  `None` |\n",
    "\n",
    "## Select IBM Watson OpenScale services\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| enable_quality | Boolean value to allow generation of common configuration details needed if quality alone is selected | `True` or `False` |\n",
    "| enable_fairness | Boolean value to allow generation of fairness specific data distribution needed for configuration | `True` or `False` |\n",
    "| enable_drift | Boolean value to allow generation of Drift Archive containing relevant information for Model and Data Drift. | `True` or `False` |\n",
    "| enable_drift_v2 | Boolean value to allow generation of Drift v2 Archive. | `True` or `False` |\n",
    "| enable_explainability | Boolean value to allow generation of explainability configuration and perturbations | `True` or `False` |\n",
    "| parameters | Provide the parameters for a monitor that needs to get enabled, | |\n",
    "| thresholds | Provide the thresholds for faireness and quality monitor if that monitor needs to get enabled | |\n",
    "| train_drift_model | It is set to `True` to train drift model and learn stats online. |`True` or `False` |\n",
    "| enable_online_learning | It is set to `True` to generate the stats and scored perturbations online. |`True` or `False` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2f4a5a-b447-4469-aafd-84c9a7da6a68",
   "metadata": {
    "id": "beb8c170-5ac3-4ea8-af1e-337862984fc1"
   },
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"model_type\": \"\",\n",
    "    \"label_column\": \"\",\n",
    "    \"prediction\": \"\",\n",
    "    \"probability\": \"\",\n",
    "    \"feature_columns\": [\"\"],\n",
    "    \"categorical_columns\": [\"\"],\n",
    "    \"scoring\":{\n",
    "        \"url\":\"\",\n",
    "        \"token\":\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "monitors_config = {\n",
    "    \"fairness_configuration\": {\n",
    "        \"enabled\": True,\n",
    "        \"parameters\":{\n",
    "        },\n",
    "        \"thresholds\": [\n",
    "        ]\n",
    "    },\n",
    "    \"quality_configuration\": {\n",
    "        \"enabled\": True,\n",
    "        \"parameters\" : {\n",
    "        },\n",
    "        \"thresholds\" : [\n",
    "        ]\n",
    "    },\n",
    "    \"drift_configuration\": {\n",
    "        \"enabled\": True,\n",
    "        \"parameters\":{\n",
    "            \"train_drift_model\": True\n",
    "        }\n",
    "    },\n",
    "    \"explainability_configuration\":{\n",
    "        \"enabled\": True,\n",
    "        \"parameters\":{\n",
    "            \"enable_online_learning\": True,\n",
    "            # Set below params to enable global explanation. Available from Cloud Pak for Data 4.6.4 onwards.\n",
    "            #\"global_explanation\": {\n",
    "            #    \"enabled\": True,\n",
    "            #    \"explanation_method\": \"lime\", # The explanation method\n",
    "            #    \"training_data_sample_size\": 1000, # [Optional] The sample size of records to be used for generating training data global explanation. If not specified entire training data is used.\n",
    "            #    \"sample_size\": 1000, # [Optional] The sample size of records to be used for generating payload data global explanation. If not specified entire data in the payload window is used.\n",
    "            #}\n",
    "        }\n",
    "    },\n",
    "    \"drift_v2_configuration\":{\n",
    "        \"enabled\": True,\n",
    "        \"parameters\": {\n",
    "            \"train_archive\": True,\n",
    "            \"feature_importance\": [], # required field\n",
    "            \"most_important_features\":[],\n",
    "            \"important_input_metadata_columns\": [] # <- Add this if input metadata drift to be calculated and meta columns are available\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f5d9cf-3f36-4c25-bd8c-8fda1d986e0b",
   "metadata": {
    "id": "02a09409-02be-49dc-aa82-69a64b49fa75"
   },
   "source": [
    "## Provide Spark Connection Details <a name=\"spark\"></a>\n",
    "\n",
    "To generate configuration for monitoring models in IBM Watson OpenScale, a spark compute engine is required. It can be either IBM Analytics Engine or your own Spark Cluster. Provide details of any one of them in this section.\n",
    "\n",
    "Please note, if you are using your own Spark cluster, checkout IBM Watson OpenScale documentation on how to setup spark manager API to enable interface for use with IBM Watson OpenScale services.\n",
    "\n",
    "### Parameters for IBM Analytics Engine\n",
    "If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| display_name | Display Name of the Spark instance in IBM Analytics Engine | |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For IBM IAE, this must be set to `cpd_iae`. | `cpd_iae` |\n",
    "| endpoint | Spark Jobs Endpoint for IBM Analytics Engine | |\n",
    "| volume | IBM Cloud Pak for Data storage volume name | |\n",
    "| username | IBM Cloud Pak for Data username | |\n",
    "| apikey | IBM Cloud Pak for Data API key | |\n",
    "\n",
    "### Parameters for Remote Spark Cluster\n",
    "If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For Remote Spark, this must be set to `custom`. | `custom` |\n",
    "| endpoint | Endpoint URL where the Spark Manager Application is running | |\n",
    "| username | Username to connect to Spark Manager Application | |\n",
    "| password | Password to connect to Spark Manager Application | |\n",
    "\n",
    "\n",
    "### Provide Spark Resource Settings [Optional]\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `{}` if no customisation is required.\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| max_num_executors | Maximum Number of executors to launch for this session |\n",
    "| min_executors | Minimum Number of executors to launch for this session |\n",
    "| executor_cores | Number of cores to use for each executor |\n",
    "| executor_memory | Amount of memory (in GBs) to use per executor process |\n",
    "| driver_cores | Number of cores to use for the driver process |\n",
    "| driver_memory | Amount of memory (in GBs) to use for the driver process |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a59e4db5-0d2e-45a2-91cf-2a2afd9810db",
   "metadata": {
    "id": "f8b98056-4b9c-4977-8ce6-3c674e596caa"
   },
   "outputs": [],
   "source": [
    "spark_connection_info = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": \"\",\n",
    "        \"location_type\": \"\",\n",
    "        \"display_name\": \"\",\n",
    "        \"volume\": \"\",\n",
    "        \"instance_id\":\"\"\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": \"\",\n",
    "        \"password\": \"\",\n",
    "        \"apikey\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": \"2\",\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": \"1\",\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": \"2\",\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": \"2\",\n",
    "    \n",
    "    # driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": \"2\",\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": \"1\"\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = {}\n",
    "\n",
    "spark_connection_info[\"spark_settings\"] = spark_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fc3ef-30e8-4ca9-b785-4f2f32625253",
   "metadata": {
    "id": "db55d582-9157-4520-9901-0b58f6765f30"
   },
   "source": [
    "## Connect to IBM Watson OpenScale instance <a name=\"connect-openscale\"></a>\n",
    "\n",
    "Following information is required to connect to IBM Watson OpenScale instance:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| url | Base url of your Cloud Pak for Data cluster hosting IBM Watson OpenScale instance. |\n",
    "| username | Username to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. |\n",
    "| password | Password to connect to your IBM Watson OpenScale instance in  Cloud Pak for Data cluster. One of `password` or `api_key` must be provided. |\n",
    "| api_key | API Key to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. One of `password` or `api_key` must be provided. |\n",
    "| service_instance_id | Id of your IBM Watson OpenScale Instance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "183e1085-91db-4a0b-ae63-2fa71bbb1597",
   "metadata": {
    "id": "19245380-d556-430a-a6ad-38f3e0be4359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.45.2\n"
     ]
    }
   ],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import APIClient\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "service_instance_id = \"\" #Default is 00000000-0000-0000-0000-000000000000\n",
    "service_credentials = {\n",
    "    \"url\": \"\",\n",
    "    \"username\": \"\",\n",
    "    \"password\": \"\"\n",
    "    #     \"apikey\":\"\"\n",
    "}\n",
    "\n",
    "authenticator = CloudPakForDataAuthenticator(\n",
    "    url=service_credentials['url'],\n",
    "    username=service_credentials['username'],\n",
    "    password=service_credentials['password'],\n",
    "    #     apikey=service_credentials['apikey'],\n",
    "    disable_ssl_verification=True\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    service_url=service_credentials['url'],\n",
    "    service_instance_id=service_instance_id,\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af9880-8d90-4726-a768-511548255ef2",
   "metadata": {
    "id": "199add6f-dc8a-48e8-82da-f4020ce74652"
   },
   "source": [
    "## Configure Machine Learning Provider in IBM Watson OpenScale instance <a name=\"create-service-provider\"></a>\n",
    "\n",
    "Before configuring model for monitoring in IBM Watson OpenScale, you need to connect your machine learning provider with IBM Watson OpenScale instance. Since, we are configuring a model for monitoring which has its runtime data located remotely to IBM Watson OpenScale, we'll create a custom machine learning provider in given instance.\n",
    "\n",
    "Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| name | Name of the machine learning provider being configured. This can be any string value. |\n",
    "| description | Description for the machine learning provider being configured. |\n",
    "| service_type | Identifies type of the machine learning provider. In this case, this value must be `ServiceTypes.CUSTOM_MACHINE_LEARNING` |\n",
    "| credentials | Stores username and password to connect to machine learning provider. |\n",
    "| deployment_space_id | Identifies the space where the model is deployed. |\n",
    "| operational_space_id | Defines the classification of machine learning provider. Possible values are `pre-production` and `production`. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d4818-60ba-4740-8b2e-56ac847b92e8",
   "metadata": {
    "id": "4c7cd748-9c0a-42fc-b624-9e03b9cc9b28"
   },
   "source": [
    "## Onboard model for monitoring in IBM Watson OpenScale instance <a name=\"create-subscription\"></a>\n",
    "\n",
    "When you configure a model for monitoring in IBM Watson OpenScale instance, a corresponding subscription is created for this model. Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| subscription_name | Name of the subscription to use. This can be any string value typically identifying model being monitored. |\n",
    "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
    "| service_provider_id | Id of the machine learning provider instance created in IBM Watson OpenScale. |\n",
    "| model_info | Details of the model to be monitored |\n",
    "| sample_csv | Path to the csv file containing scored training data |\n",
    "| spark_credentials | Connection details of Spark compute engine to use for analysis by different IBM Watson OpenScale services. |\n",
    "| payload_table | Details of the payload table to be used with this subscription. |\n",
    "| feedback_table | Details of the feedback table to be used with this subscription. |\n",
    "| scored_training_data_table | Details of the scored training data table to be used with this subscription. |\n",
    "| managed_by | To identify whether the subscription is `system` managed (Model transactions are stored in the OpenScale database and evaluated using OpenScale computing resources) or `self` managed (Model transactions are stored in a your own data warehouse and evaluated by your Spark analytics engine.) . This function is not supporting system managed subscriptions as of now. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75fdbbea-74fe-44b3-9463-7d9a721b1a6b",
   "metadata": {
    "id": "75fdbbea-74fe-44b3-9463-7d9a721b1a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=================================================================================\n",
      "\n",
      " Waiting for end of adding service provider 01963d9c-8b77-76d3-b647-aa75d5b73d16 \n",
      "\n",
      "=================================================================================\n",
      "\n",
      "\n",
      "\n",
      "active\n",
      "\n",
      "-----------------------------------------------\n",
      " Successfully finished adding service provider \n",
      "-----------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "        <body>\n",
       "            <h3>Service Providers</h3>\n",
       "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
       "                <th style='border: 1px solid #dddddd'>instance_id</th><th style='border: 1px solid #dddddd'>status</th><th style='border: 1px solid #dddddd'>name</th><th style='border: 1px solid #dddddd'>service_type</th><th style='border: 1px solid #dddddd'>created_at</th><th style='border: 1px solid #dddddd'>id</th>\n",
       "                <tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>Final-Batch-Sample-CSV-Hive</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-16 07:59:51.679000+00:00</td><td style='border: 1px solid #dddddd'>01963d9c-8b77-76d3-b647-aa75d5b73d16</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>GCR-Driftv2-Batch-Sample-CSV-DB2</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 09:58:01.562000+00:00</td><td style='border: 1px solid #dddddd'>019638e2-5e3a-703e-a39f-fce0404f5fea</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>GCR-Driftv2-Batch-Sample-CSV-JDBC</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 08:16:41.875000+00:00</td><td style='border: 1px solid #dddddd'>01963885-9988-7328-9e2d-8c2f1aca6025</td></tr><tr><td style='border: 1px solid #dddddd'>99999999-9999-9999-9999-999999999999</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>service-provider-space-f18c065b-6096-448c-b1aa-80bd23f1cecb</td><td style='border: 1px solid #dddddd'>watson_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-15 07:58:34.729000+00:00</td><td style='border: 1px solid #dddddd'>01963875-0294-7954-90f5-2606051356c8</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_APIKEY_CLOUD_WITHOUTAPI_PREPROD</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:07.777000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-6096-7a89-8be6-628d39f2bef2</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_APIKEY_CLOUD_WITHOUTAPI</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:07.574000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-5fb8-7c3c-abc4-1a84f67a91c6</td></tr><tr><td style='border: 1px solid #dddddd'>4ca7ec1c-6b35-48ef-a45c-0434c6985058</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>MRM_WMLV4_CLOUD_PREPROD</td><td style='border: 1px solid #dddddd'>watson_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:07.415000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-5e59-7b4d-b912-3a27e1e18391</td></tr><tr><td style='border: 1px solid #dddddd'>4ca7ec1c-6b35-48ef-a45c-0434c6985058</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>MRM_WMLV4_CLOUD_PROD</td><td style='border: 1px solid #dddddd'>watson_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:58:04.460000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-50f0-7b96-b640-cddd7685e822</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_HLS_PREPROD</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:57:59.876000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-41ba-7b9c-8ee2-2ce5df750412</td></tr><tr><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>CUSTOM_BATCH_PROD</td><td style='border: 1px solid #dddddd'>custom_machine_learning</td><td style='border: 1px solid #dddddd'>2025-04-14 05:57:59.780000+00:00</td><td style='border: 1px solid #dddddd'>019632e0-4159-791d-b775-1035fe606be9</td></tr>\n",
       "            </table>\n",
       "        </body>\n",
       "        </HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: First 10 records were displayed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: First 10 records were displayed.\n"
     ]
    }
   ],
   "source": [
    "# [OPTIONAL] Delete existing service provider with the same name as provided\n",
    "\n",
    "# SERVICE_PROVIDER_NAME = \"\"\n",
    "# service_providers = client.service_providers.list().result.service_providers\n",
    "# for provider in service_providers:\n",
    "#     if provider.entity.name == SERVICE_PROVIDER_NAME:\n",
    "#         client.service_providers.delete(service_provider_id=provider.metadata.id)\n",
    "#         break\n",
    "\n",
    "# Add Service Provider\n",
    "from ibm_watson_openscale.supporting_classes.enums import ServiceTypes\n",
    "# from ibm_watson_openscale.base_classes.watson_open_scale_v2 import CustomCredentials\n",
    "\n",
    "added_service_provider_result = client.service_providers.add(\n",
    "        name=\"\",\n",
    "        description=\"\",\n",
    "        service_type=ServiceTypes.CUSTOM_MACHINE_LEARNING,\n",
    "        credentials={},\n",
    "        deployment_space_id = \"\",\n",
    "        operational_space_id=\"\",\n",
    "        background_mode=False\n",
    "    ).result\n",
    "\n",
    "service_provider_id = added_service_provider_result.metadata.id\n",
    "\n",
    "client.service_providers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b829a86f-69c6-443b-9af2-69275b9c9d70",
   "metadata": {
    "id": "2ab3c8fc-18e9-4ba6-90b0-fa2a2a603a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Missing scoring url : scoring url is mandatory if explain needs to be enabled\n",
      "Creating integrated system for Spark\n",
      "Integrated system 01963d9c-a87f-7588-911a-3e70d994d77f created \n",
      "Creating integrated system for Hive/DB2\n",
      "Hive/Db2 Integrated system 01963d9c-a925-7878-ad2f-4ce47e94a3e8 created\n",
      "Updating schemas ...\n",
      "08:00:00 preparing\n",
      "Schemas update completed.\n",
      "Updating data-sources ...\n",
      "Data-sources update complete.\n",
      "Subscription is created. Id is : 01963d9c-a9d1-7f85-bf0f-5f7193460ee8\n",
      "Subscription is being activated, please wait for state to be active before using it further.\n",
      "Subscription id is 01963d9c-a9d1-7f85-bf0f-5f7193460ee8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating integrated system for Spark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated system 01963a38-fdd5-7c34-a651-13190d3aea8e created \n",
      "Creating integrated system for Hive/DB2\n",
      "Hive/Db2 Integrated system 01963a38-ff38-782d-b4f2-1f412be9c31c created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating schemas ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:12:18 preparing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas update completed.\n",
      "Updating data-sources ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-sources update complete.\n",
      "Subscription is created. Id is : 01963a38-ffa1-784e-8811-e044ff914431\n",
      "Subscription is being activated, please wait for state to be active before using it further.\n",
      "Subscription id is 01963a38-ffa1-784e-8811-e044ff914431\n"
     ]
    }
   ],
   "source": [
    "subscription_id = client.subscriptions.create_subscription_using_training_data(\n",
    "    subscription_name=\"My SDK Batch Subscription-hive\",\n",
    "    datamart_id=service_instance_id,\n",
    "    service_provider_id=service_provider_id,\n",
    "    model_info=model_info,\n",
    "    sample_csv = sample_csv,\n",
    "    spark_credentials=spark_connection_info,\n",
    "    data_warehouse_connection = datawarehouse_details,\n",
    "    payload_table=payload_table,\n",
    "    feedback_table=feedback_table,\n",
    "    scored_training_data_table = scored_training_data_table,\n",
    "    managed_by=\"self\"\n",
    ")\n",
    "\n",
    "print(\"Subscription id is {}\".format(subscription_id))\n",
    "\n",
    "# Wait for the subscription to get in active state and to create the \n",
    "# required tables in the background before moving onto enabling monitors\n",
    "\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "\n",
    "# subscription_status = None\n",
    "# while subscription_status not in (\"active\", \"error\"):\n",
    "#     subscription_status = client.subscriptions.get(subscription_id).result.entity.status.state\n",
    "#     if subscription_status not in (\"active\", \"error\"):\n",
    "#         print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)\n",
    "#         time.sleep(15)\n",
    "        \n",
    "# print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475546b-1db0-4066-bfc8-e684c0977a92",
   "metadata": {
    "id": "77f61449-3a4c-4ce1-9724-c7fb0cf0261e"
   },
   "source": [
    "## Enable different services to monitor model <a name=\"enable-monitors\"></a>\n",
    "\n",
    "Depending on the services enabled in `monitors_config`, different services are enabled in given subscription. There services are called monitors.\n",
    "\n",
    "Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
    "| subscription_id | Id of the subscription created for given model in IBM Watson OpenScale instance. |\n",
    "| monitors_config | Details of the monitores that needs to get configured. |\n",
    "| drifted_transaction_table | Details of the drifted transactions table to be used with this subscription. |\n",
    "| explain_queue_table | Details of the explain queue table to be used with this subscription. |\n",
    "| explain_results_table | Details of the explain results table to be used with this subscription. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9cb5d36-ee6a-4cd1-9968-bb7d4513293b",
   "metadata": {
    "id": "f93779fb-901e-4aab-80d0-bada39a7678d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Drift V2....\n",
      "{'drift_v2': '01963d9d-71df-7349-bae3-cae49d2d5382'}\n"
     ]
    }
   ],
   "source": [
    "instance_ids = client.monitor_instances.enable_monitor_using_training_data(\n",
    "                  datamart_id = service_instance_id,\n",
    "                  subscription_id = subscription_id,\n",
    "                  monitors_config = monitors_config,\n",
    "                  drifted_transaction_table = drifted_transaction_table,\n",
    "                  explain_queue_table = explain_queue_table,\n",
    "                  explain_results_table = explain_result_table\n",
    ")\n",
    "\n",
    "print(instance_ids)\n",
    "\n",
    "## Track each monitor instance status\n",
    "# for key, value in instance_ids.items():\n",
    "#     monitor_instance_status = None\n",
    "\n",
    "#     while monitor_instance_status not in (\"active\", \"error\"):\n",
    "#         monitor_instance_details = client.monitor_instances.get(monitor_instance_id=value).result\n",
    "#         monitor_instance_status = monitor_instance_details.entity.status.state\n",
    "#         if monitor_instance_status not in (\"active\", \"error\"):\n",
    "#             print(datetime.now().strftime(\"%H:%M:%S\"), monitor_instance_status)\n",
    "#             time.sleep(30)\n",
    "\n",
    "#     print(key, monitor_instance_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca1292-9c58-4160-95c1-a3087f79f73d",
   "metadata": {
    "id": "273c677a-5ada-4223-8ac8-f71580eb776f"
   },
   "source": [
    "## Congratulations!\n",
    "\n",
    "All the monitors have been enabled. It will take some time for monitors to get into active state. You can track the status of each monitor separately by using above code snippet.\n",
    "\n",
    "Once, all monitors are active, load data into payload or feedback table and either run on-demand evaluations or wait for scheduled evaluations to complete for each monitor. You can check more details in [Watson OpenScale Dashboard](https://url-to-your-cp4d-cluster/aiopenscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121e9a9",
   "metadata": {
    "id": "c9183b3a-4558-417b-a478-16c964313b36"
   },
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbf2dd",
   "metadata": {
    "id": "02cd8be8-0a92-4af4-93c3-7c19ff48d3a1"
   },
   "source": [
    "### Cleanup subscription and its related artefacts\n",
    "Crawls through subscription json and identifies entities to be deleted. Currently, following entities are identified and deleted:\n",
    "- Analytics Engine integrated system\n",
    "- Data Warehouse Connection integrated system(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40011e",
   "metadata": {
    "id": "29297bc3-c268-4b35-b028-76db5e5c3c91"
   },
   "outputs": [],
   "source": [
    "# # Uncomment and update following if you are running this at a later point of time or \n",
    "# # separate from this notebook with no subscription id and wos client session\n",
    "\n",
    "# from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "# from ibm_watson_openscale import APIClient\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# service_instance_id = \"<SERVICE_INSTANCE_ID>\" #Default is 00000000-0000-0000-0000-000000000000\n",
    "# service_credentials = {\n",
    "#     \"url\": \"<to_be_edited>\",\n",
    "#     \"username\": \"<to_be_edited>\",\n",
    "#     \"password\": \"<to_be_edited>\",\n",
    "# #     \"apikey\":\"<to_be_edited>\"\n",
    "# }\n",
    "\n",
    "# authenticator = CloudPakForDataAuthenticator(\n",
    "#     url=service_credentials['url'],\n",
    "#     username=service_credentials['username'],\n",
    "#     password=service_credentials['password'],\n",
    "# #     apikey=service_credentials['apikey'],\n",
    "#     disable_ssl_verification=True\n",
    "# )\n",
    "\n",
    "# client = APIClient(\n",
    "#     service_url=service_credentials['url'],\n",
    "#     service_instance_id=service_instance_id,\n",
    "#     authenticator=authenticator\n",
    "# )\n",
    "\n",
    "# print(client.version)\n",
    "\n",
    "# subscription_id = \"<to_be_edited>\"\n",
    "\n",
    "subscription_details = client.subscriptions.get(\n",
    "    subscription_id=subscription_id).result.to_dict()\n",
    "subscription_entity = subscription_details.get(\"entity\", {})\n",
    "\n",
    "integrated_systems_id = []\n",
    "\n",
    "# add analytics engine integrated system id\n",
    "analytics_engine = subscription_entity.get(\"analytics_engine\", {})\n",
    "if analytics_engine and analytics_engine.get(\"integrated_system_id\"):\n",
    "    print(\"Found integrated system for analytics engine with type: {}\".format(\n",
    "        analytics_engine.get(\"type\")))\n",
    "    integrated_systems_id.append(analytics_engine.get(\"integrated_system_id\"))\n",
    "\n",
    "# add data source integrated system ids\n",
    "data_sources = subscription_entity.get(\"data_sources\", [])\n",
    "for data_source in data_sources:\n",
    "    if not data_source.get(\"connection\"):\n",
    "        continue\n",
    "\n",
    "    if not data_source.get(\"connection\").get(\"integrated_system_id\"):\n",
    "        continue\n",
    "\n",
    "    integrated_system_id = data_source.get(\"connection\").get(\"integrated_system_id\")\n",
    "    if integrated_system_id in integrated_systems_id:\n",
    "        continue\n",
    "\n",
    "    print(\"Found integrated system for data source with type: {}\".format(\n",
    "        data_source.get(\"type\")))\n",
    "    integrated_systems_id.append(integrated_system_id)\n",
    "    \n",
    "print(\"Integrated Systems to delete: {}\".format(integrated_systems_id))\n",
    "    \n",
    "# delete subscription\n",
    "client.subscriptions.delete(\n",
    "    subscription_id=subscription_id,\n",
    "    background_mode=False)\n",
    "\n",
    "# wait time for subscription delete to complete\n",
    "import time\n",
    "time.sleep(30)\n",
    "\n",
    "# delete all integrated systems\n",
    "for integrated_system_id in integrated_systems_id:\n",
    "    print(\"Deleting integrated system with id: {}\".format(integrated_system_id))\n",
    "    client.integrated_systems.delete(integrated_system_id)\n",
    "    \n",
    "    # wait time for integrated system delete to complete\n",
    "    time.sleep(10)\n",
    "    \n",
    "print(\"Cleanup Complete!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37b82c9850f337b3f8e26ef3a35bf87c9f2cfa1e4ad2c96ec00819afd6ebf7e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
