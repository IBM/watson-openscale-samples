{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Tutorial on generating an explanation for a text-based model on Watson OpenScale"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "This notebook includes steps for creating a text-based watson-machine-learning model, creating a subscription, configuring explainability, and finally generating an explanation for a transaction."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Contents\n",
       "- [1. Setup](#setup)\n",
       "- [2. Creating and deploying a text-based model](#deploy)\n",
       "- [3. Subscriptions](#subscription)\n",
       "- [4. Explainability](#explainability)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "**Note**: This notebook works correctly with kernel `Python 3.10.x` with pyspark 3.3.x."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "<a id=\"setup\"></a>\n",
       "## 1. Setup\n",
       "\n",
       "### 1.1 Install Watson OpenScale and WML packages"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install --upgrade ibm-watson-openscale --no-cache --user| tail -n 1"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install --upgrade ibm-watson-machine-learning --no-cache | tail -n 1"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Note: Restart the kernel to assure the new libraries are being used."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 1.2 Configure credentials"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "- WOS_CREDENTIALS (CP4D)\n",
       "- WML_CREDENTIALS (CP4D)\n",
       "- DATABASE_CREDENTIALS (DB2 on CP4D or Cloud Object Storage (COS))\n",
       "- SCHEMA_NAME"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "WOS_CREDENTIALS = {\n",
       "    \"url\": \"***\",\n",
       "    \"username\": \"***\",\n",
       "    \"password\": \"***\"\n",
       "}"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "WML_CREDENTIALS = {\n",
       "                   \"url\": \"***\",\n",
       "                   \"username\": \"***\",\n",
       "                   \"password\" : \"***\",\n",
       "                   \"instance_id\": \"wml_local\",\n",
       "                   \"version\" : \"4.6\" #If your env is CP4D 4.x.x then specify \"4.x.x\" instead of \"4.6\"\n",
       "                  }"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Creating and deploying a text-based model <a id=\"deploy\"></a>"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "The dataset used is the UCI-ML SMS Spam Collection Dataset which can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/00228/. It is a binary classification dataset with the labels being 'ham' and 'spam'."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.1 Loading the training data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!rm -rf SMSSpam.csv\n",
       "!wget 'https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/spam_detection/SMSSpam.csv'"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# The training data is downloaded and saved as 'SMSSpam.csv' in this step from public link\n",
       "\n",
       "# !pip install pandas\n",
       "# !rm smsspamcollection.zip\n",
       "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
       "# !unzip smsspamcollection.zip\n",
       "#pd.read_csv(\"smsspamcollection.zip\",sep=\"\\t\",header=None, encoding=\"utf-8\").to_csv(\"SMSSpam.csv\", header=[\"label\", \"text\"], sep=\",\", index=False)\n",
       "\n",
       "# !rm SMSSpamCollection\n",
       "# !rm readme\n",
       "# !rm smsspamcollection.zip"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.2 Creating a model"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "**Note**: Skip the pyspark install step below if you are using a Spark kernel on Watson Studio."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install --upgrade pyspark==3.3.0"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "**Note**: When running this notebook locally, If the `SparkSession` import fails below, set 'SPARK_HOME' environment variable with the path to `pyspark` installation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "from pyspark.sql import SparkSession\n",
       "\n",
       "spark = SparkSession.builder.getOrCreate()\n",
       "df = spark.read.csv(path=\"SMSSpam.csv\", header=True, multiLine=True, escape='\"')\n",
       "df.show(5, truncate = False)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "train_df, test_df = df.randomSplit([0.8, 0.2], seed=12345)\n",
       "print(\"Total count of data set: {}\".format(df.count()))\n",
       "print(\"Total count of training data set: {}\".format(train_df.count()))\n",
       "print(\"Total count of test data set: {}\".format(test_df.count()))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install nltk\n",
       "from pyspark.ml.feature import StringIndexer, IndexToString, CountVectorizer, Tokenizer, IDF, StopWordsRemover\n",
       "from pyspark.ml.classification import GBTClassifier\n",
       "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
       "from pyspark.ml import Pipeline, Model\n",
       "import nltk\n",
       "from nltk.tokenize import word_tokenize\n",
       "from nltk.corpus import stopwords\n",
       "\n",
       "nltk.download('punkt')\n",
       "nltk.download('stopwords')\n",
       "stop_words = list(set(stopwords.words('english')))\n",
       "\n",
       "stringIndexer_label = StringIndexer(inputCol=\"label\", outputCol=\"label_ix\").fit(df)\n",
       "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
       "stopword_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\").setStopWords(stop_words)\n",
       "count = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
       "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
       "nb = GBTClassifier(labelCol=\"label_ix\")\n",
       "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictionLabel\", labels=stringIndexer_label.labels)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "pipeline = Pipeline(stages=[stringIndexer_label, tokenizer, stopword_remover, count, idf, nb, labelConverter])\n",
       "model = pipeline.fit(train_df)\n",
       "predictions = model.transform(test_df)\n",
       "evaluator = BinaryClassificationEvaluator(labelCol=\"label_ix\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
       "auc = evaluator.evaluate(predictions)\n",
       "\n",
       "print(\"Area under ROC curve = %g\" % auc)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import json\n",
       "from ibm_watson_machine_learning import APIClient\n",
       "\n",
       "wml_client = APIClient(WML_CREDENTIALS)\n",
       "wml_client.version"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "wml_client.spaces.list(limit=10)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "WML_SPACE_ID='***' # use space id here\n",
       "wml_client.set.default_space(WML_SPACE_ID)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "MODEL_NAME = \"Text Binary Classifier\""
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_3.3\")\n",
       "print(\"Software Specification ID: {}\".format(software_spec_uid))\n",
       "model_props = {\n",
       "        wml_client._models.ConfigurationMetaNames.NAME:\"{}\".format(MODEL_NAME),\n",
       "        wml_client._models.ConfigurationMetaNames.TYPE: \"mllib_3.3\",\n",
       "        wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n",
       "        wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"label\",\n",
       "    }"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(\"Storing model ...\")\n",
       "published_model_details = wml_client.repository.store_model(\n",
       "    model=model, \n",
       "    meta_props=model_props, \n",
       "    training_data=train_df, \n",
       "    pipeline=pipeline)\n",
       "\n",
       "model_uid = wml_client.repository.get_model_id(published_model_details)\n",
       "print(\"Done\")\n",
       "print(\"Model ID: {}\".format(model_uid))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.3 Deploying the model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "deployment_details = wml_client.deployments.create(\n",
       "    model_uid, \n",
       "    meta_props={\n",
       "        wml_client.deployments.ConfigurationMetaNames.NAME: \"{}\".format(MODEL_NAME + \" deployment\"),\n",
       "        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
       "    }\n",
       ")\n",
       "scoring_url = wml_client.deployments.get_scoring_href(deployment_details)\n",
       "deployment_uid=wml_client.deployments.get_id(deployment_details)\n",
       "\n",
       "print(\"Scoring URL:\" + scoring_url)\n",
       "print(\"Model id: {}\".format(model_uid))\n",
       "print(\"Deployment id: {}\".format(deployment_uid))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Subscriptions <a id=\"subscription\"></a>"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 3.1 Configuring OS"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
       "from ibm_watson_openscale import APIClient\n",
       "\n",
       "from ibm_watson_openscale import *\n",
       "from ibm_watson_openscale.supporting_classes.enums import *\n",
       "from ibm_watson_openscale.supporting_classes import *\n",
       "\n",
       "\n",
       "authenticator = CloudPakForDataAuthenticator(\n",
       "        url=WOS_CREDENTIALS['url'],\n",
       "        username=WOS_CREDENTIALS['username'],\n",
       "        password=WOS_CREDENTIALS['password'],\n",
       "        disable_ssl_verification=True\n",
       "    )\n",
       "\n",
       "wos_client = APIClient(service_url=WOS_CREDENTIALS['url'],authenticator=authenticator)\n",
       "wos_client.version"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "**Note**: Please re-run the above cell if it doesn't work the first time."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "#DB_CREDENTIALS= {\"hostname\":\"\",\"username\":\"\",\"password\":\"\",\"database\":\"\",\"port\":\"\",\"ssl\":True,\"sslmode\":\"\",\"certificate_base64\":\"\"}\n",
       "DB_CREDENTIALS = None\n",
       "KEEP_MY_INTERNAL_POSTGRES = True"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "data_marts = wos_client.data_marts.list().result.data_marts\n",
       "if len(data_marts) == 0:\n",
       "    if DB_CREDENTIALS is not None:\n",
       "        if SCHEMA_NAME is None: \n",
       "            print(\"Please specify the SCHEMA_NAME and rerun the cell\")\n",
       "\n",
       "        print('Setting up external datamart')\n",
       "        added_data_mart_result = wos_client.data_marts.add(\n",
       "                background_mode=False,\n",
       "                name=\"WOS Data Mart\",\n",
       "                description=\"Data Mart created by WOS tutorial notebook\",\n",
       "                database_configuration=DatabaseConfigurationRequest(\n",
       "                  database_type=DatabaseType.POSTGRESQL,\n",
       "                    credentials=PrimaryStorageCredentialsLong(\n",
       "                        hostname=DB_CREDENTIALS['hostname'],\n",
       "                        username=DB_CREDENTIALS['username'],\n",
       "                        password=DB_CREDENTIALS['password'],\n",
       "                        db=DB_CREDENTIALS['database'],\n",
       "                        port=DB_CREDENTIALS['port'],\n",
       "                        ssl=True,\n",
       "                        sslmode=DB_CREDENTIALS['sslmode'],\n",
       "                        certificate_base64=DB_CREDENTIALS['certificate_base64']\n",
       "                    ),\n",
       "                    location=LocationSchemaName(\n",
       "                        schema_name= SCHEMA_NAME\n",
       "                    )\n",
       "                )\n",
       "             ).result\n",
       "    else:\n",
       "        print('Setting up internal datamart')\n",
       "        added_data_mart_result = wos_client.data_marts.add(\n",
       "                background_mode=False,\n",
       "                name=\"WOS Data Mart\",\n",
       "                description=\"Data Mart created by WOS tutorial notebook\", \n",
       "                internal_database = True).result\n",
       "        \n",
       "    data_mart_id = added_data_mart_result.metadata.id\n",
       "    \n",
       "else:\n",
       "    data_mart_id=data_marts[0].metadata.id\n",
       "    print('Using existing datamart {}'.format(data_mart_id))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "SERVICE_PROVIDER_NAME = \"Watson Machine Learning V2_test\"\n",
       "SERVICE_PROVIDER_DESCRIPTION = \"Added by tutorial WOS notebook.\""
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "service_providers = wos_client.service_providers.list().result.service_providers\n",
       "for service_provider in service_providers:\n",
       "    service_instance_name = service_provider.entity.name\n",
       "    if service_instance_name == SERVICE_PROVIDER_NAME:\n",
       "        service_provider_id = service_provider.metadata.id\n",
       "        wos_client.service_providers.delete(service_provider_id)\n",
       "        print(\"Deleted existing service_provider for WML instance: {}\".format(service_provider_id))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "added_service_provider_result = wos_client.service_providers.add(\n",
       "        name=SERVICE_PROVIDER_NAME,\n",
       "        description=SERVICE_PROVIDER_DESCRIPTION,\n",
       "        service_type=ServiceTypes.WATSON_MACHINE_LEARNING,\n",
       "        deployment_space_id = WML_SPACE_ID,\n",
       "        operational_space_id = \"production\",\n",
       "        credentials=WMLCredentialsCP4D(\n",
       "            url=WML_CREDENTIALS[\"url\"],\n",
       "            username=WML_CREDENTIALS[\"username\"],\n",
       "            password=WML_CREDENTIALS[\"password\"],\n",
       "            instance_id=None\n",
       "        ),\n",
       "        background_mode=False\n",
       "    ).result\n",
       "service_provider_id = added_service_provider_result.metadata.id"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "asset_deployment_details_list = wos_client.service_providers.list_assets(data_mart_id=data_mart_id, service_provider_id=service_provider_id, deployment_space_id = WML_SPACE_ID).result['resources']\n",
       "DEPLOYMENT_NAME='Text Binary Classifier deployment' # use the model name here \n",
       "asset_deployment_details = [asset for asset in asset_deployment_details_list if asset['entity'][\"name\"]==DEPLOYMENT_NAME]\n",
       "\n",
       "if len(asset_deployment_details)>0:\n",
       "    [asset_deployment_details] = asset_deployment_details\n",
       "else:\n",
       "    raise ValueError('deployment with name \"{}\" not found.'.format(DEPLOYMENT_NAME))\n",
       "asset_deployment_details"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "model_asset_details_from_deployment=wos_client.service_providers.get_deployment_asset(data_mart_id=data_mart_id,service_provider_id=service_provider_id,deployment_id=deployment_uid,deployment_space_id=WML_SPACE_ID)\n",
       "model_asset_details_from_deployment"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 3.2 Subscribe the asset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "subscriptions = wos_client.subscriptions.list().result.subscriptions\n",
       "for subscription in subscriptions:\n",
       "    sub_model_id = subscription.entity.asset.asset_id\n",
       "    if sub_model_id == model_uid:\n",
       "        wos_client.subscriptions.delete(subscription.metadata.id)\n",
       "        print('Deleted existing subscription for model', sub_model_id)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import ScoringEndpointRequest"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "subscription_details = wos_client.subscriptions.add(\n",
       "        data_mart_id=data_mart_id,\n",
       "        service_provider_id=service_provider_id,\n",
       "        asset=Asset(\n",
       "            asset_id=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"asset_id\"],\n",
       "            name=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"name\"],\n",
       "            url=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"url\"],\n",
       "            asset_type=AssetTypes.MODEL,\n",
       "            input_data_type=InputDataType.UNSTRUCTURED_TEXT,\n",
       "            problem_type=ProblemType.BINARY_CLASSIFICATION\n",
       "        ),\n",
       "        deployment=AssetDeploymentRequest(\n",
       "            deployment_id=asset_deployment_details['metadata']['guid'],\n",
       "            name=asset_deployment_details['entity']['name'],\n",
       "            deployment_type= DeploymentTypes.ONLINE,\n",
       "            url=model_asset_details_from_deployment['entity']['asset']['url'],\n",
       "            scoring_endpoint=ScoringEndpointRequest(url=scoring_url) # scoring model without shadow deployment\n",
       "        ),\n",
       "        asset_properties=AssetPropertiesRequest(\n",
       "            label_column='label',\n",
       "            probability_fields=['probability'],\n",
       "            prediction_field='predictionLabel',\n",
       "            feature_fields = [\"text\"],\n",
       "            categorical_fields = [\"text\"],\n",
       "            training_data_schema=SparkStruct.from_dict(model_asset_details_from_deployment[\"entity\"][\"asset_properties\"][\"training_data_schema\"])\n",
       "        )\n",
       "    ).result\n",
       "subscription_id = subscription_details.metadata.id\n",
       "subscription_id"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import time\n",
       "\n",
       "time.sleep(5)\n",
       "payload_data_set_id = None\n",
       "payload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n",
       "                                                target_target_id=subscription_id, \n",
       "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
       "if payload_data_set_id is None:\n",
       "    print(\"Payload data set not found. Please check subscription status.\")\n",
       "else:\n",
       "    print(\"Payload data set id: \", payload_data_set_id)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 3.3 Get subscription"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "wos_client.subscriptions.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "wos_client.subscriptions.get(subscription_id).result.to_dict()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 3.4 Score the model and get transaction-id"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "text = \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\n",
       "payload = {\"input_data\": [{\"fields\": [\"text\"], \"values\": [[text]]}]}\n",
       "\n",
       "response = wml_client.deployments.score(deployment_uid,payload)\n",
       "print(response)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "wos_client.data_sets.get_records_count(payload_data_set_id)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Explainability"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 4.1 Configure Explainability"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "target = Target(\n",
       "    target_type=TargetTypes.SUBSCRIPTION,\n",
       "    target_id=subscription_id\n",
       ")\n",
       "parameters = {\n",
       "    \"enabled\": True\n",
       "}\n",
       "explainability_details = wos_client.monitor_instances.create(\n",
       "    data_mart_id=data_mart_id,\n",
       "    background_mode=False,\n",
       "    monitor_definition_id=wos_client.monitor_definitions.MONITORS.EXPLAINABILITY.ID,\n",
       "    target=target,\n",
       "    parameters=parameters\n",
       ").result\n",
       "\n",
       "explainability_monitor_id = explainability_details.metadata.id"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 4.2 Get explanation for the transaction"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "pl_records_resp = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id, limit=1, offset=0).result\n",
       "scoring_ids = [pl_records_resp[\"records\"][0][\"entity\"][\"values\"][\"scoring_id\"]]\n",
       "print(\"Running explanations on scoring IDs: {}\".format(scoring_ids))\n",
       "explanation_types = [\"lime\", \"contrastive\"]\n",
       "result = wos_client.monitor_instances.explanation_tasks(scoring_ids=scoring_ids, explanation_types=explanation_types, subscription_id=subscription_id).result\n",
       "print(result)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "explanation_task_id=result.to_dict()['metadata']['explanation_task_ids'][0]\n",
       "wos_client.monitor_instances.get_explanation_tasks(explanation_task_id=explanation_task_id, subscription_id=subscription_id).result.to_dict()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
     },
     "varInspector": {
      "cols": {
       "lenName": 16,
       "lenType": 16,
       "lenVar": 40
      },
      "kernels_config": {
       "python": {
        "delete_cmd_postfix": "",
        "delete_cmd_prefix": "del ",
        "library": "var_list.py",
        "varRefreshCmd": "print(var_dic_list())"
       },
       "r": {
        "delete_cmd_postfix": ") ",
        "delete_cmd_prefix": "rm(",
        "library": "var_list.r",
        "varRefreshCmd": "cat(var_dic_list()) "
       }
      },
      "types_to_exclude": [
       "module",
       "function",
       "builtin_function_or_method",
       "instance",
       "_Feature"
      ],
      "window_display": false
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   