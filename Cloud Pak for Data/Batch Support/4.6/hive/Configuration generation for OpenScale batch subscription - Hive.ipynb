{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating configuration for batch subscriptions in IBM Watson OpenScale in IBM Cloud Pak for Data v4.6\n",
    "\n",
    "This notebook shows how to generate the following artefacts:\n",
    "1. Configuration JSON needed to configure an IBM Watson OpenScale subscription.\n",
    "2. Drift Configuration Archive\n",
    "3. Explainability Perturbations Archive\n",
    "3. DDLs for creating Feedback, Payload, Drifted Transactions and Explanations tables\n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) and download the generated artefacts. These artefacts \n",
    "have to be then uploaded to IBM Watson OpenScale UI. \n",
    "\n",
    "PS: This notebook can only generate artefacts for one model at a time. For multiple models, this notebook needs to be run for each model separately.\n",
    "\n",
    "**Contents:**\n",
    "1. [Installing Dependencies](#Installing-Dependencies)\n",
    "2. [Select IBM Watson OpenScale Services](#Select-IBM-Watson-OpenScale-Services)\n",
    "3. [Read sample scoring data](#Read-sample-scoring-data)\n",
    "4. [Specify Model Inputs](#Specify-Model-Inputs)\n",
    "5. [Generate Common Configuration](#Generate-Common-Configuration)\n",
    "6. [Generate DDL for creating Scored Training data table](#Generate-DDL-for-creating-Scored-Training-data-table)\n",
    "6. [Generate DDL for creating Feedback table](#Generate-DDL-for-creating-Feedback-table)\n",
    "7. [Generate DDL for creating Payload table](#Generate-DDL-for-creating-Payload-table)\n",
    "8. [Provide Spark Connection Details](#Provide-Spark-Connection-Details)\n",
    "9. [Provide Storage Inputs](#Provide-Storage-Inputs)\n",
    "10. [Provide Spark Resource Settings [Optional]](#Provide-Spark-Resource-Settings-[Optional])\n",
    "11. [Provide Additional Spark Settings [Optional]](#Provide-Additional-Spark-Settings-[Optional])\n",
    "12. [Provide Drift Parameters [Optional]](#Provide-Drift-Parameters-[Optional])\n",
    "13. [Provide Fairness Parameters [Optional]](#Provide-Fairness-Parameters-[Optional])\n",
    "14. [Run Configuration Job](#Run-Configuration-Job)\n",
    "15. [Download Configuration JSON](#Download-Configuration-JSON)\n",
    "16. [Download Drift Archive](#Download-Drift-Archive)\n",
    "17. [Generate DDL for creating Drifted Transactions Table](#Generate-DDL-for-creating-Drifted-Transactions-table)\n",
    "18. [Generate Perturbations csv](#Generate-Perturbations-csv)\n",
    "19. [Generate DDL for creating Explanations Queue table](#Generate-DDL-for-creating-Explanations-Queue-table)\n",
    "20. [Generate DDL for creating Explanations Table](#Generate-DDL-for-creating-Explanations-Table)\n",
    "21. [Create Configuration Archive](#Create-Configuration-Archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: Restart kernel after the dependencies are installed\n",
    "import sys\n",
    "\n",
    "PYTHON = sys.executable\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts pyspark | tail -n 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For IBM Watson OpenScale Cloud Pak for Data version 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==1.0.2 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=1.0.2\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts \"ibm-wos-utils>=4.6.0\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select IBM Watson OpenScale Services\n",
    "\n",
    "Details of the service-specific flags available:\n",
    "\n",
    "- ENABLE_QUALITY: Flag to allow generation of common configuration details needed if quality alone is selected\n",
    "- ENABLE_FAIRNESS : Flag to allow generation of fairness specific data distribution needed for configuration\n",
    "- ENABLE_MODEL_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Model Drift.\n",
    "- ENABLE_DATA_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Data Drift.\n",
    "- ENABLE_EXPLAINABILITY : Flag to allow generation of explainability configuration and perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5737-H76\n",
    "# Copyright IBM Corp. 2020, 2022\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = \"hive-2.0.6\"\n",
    "\n",
    "# Version history\n",
    "\n",
    "# hive-2.0.6 : Upgrade ibm-wos-utils to 4.6.0 and update explainability archive with stats.\n",
    "# hive-2.0.5 : Changed the way drift archive is created.\n",
    "# hive-2.0.4 : Upgrade ibm-wos-utils to 4.1.1 (scikit-learn has been upgraded to 1.0.2)\n",
    "# hive-2.0.3 : Add two drift tuning parameters: max_ranges_modifier and tail_discard_threshold; Upgrade ibm-wos-utils to 4.0.34\n",
    "# hive-2.0.2 : Upgrade ibm-wos-utils to 4.0.31\n",
    "# hive-2.0.1 : Make notebook compatible for zLinux environments; Upgrade ibm-wos-utils to 4.0.25\n",
    "# hive-2.0   : Upgrade ibm-wos-utils to 4.0.24\n",
    "# 2.0        : Added support for fairness and explainability\n",
    "# 1.0        : Initial release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Input: Keep an identifiable name. This id is used to append to various table creation DDLs.\n",
    "# A random UUID is used if this is not present.\n",
    "# NOTEBOOK_RUN_ID = \"some_identifiable_name\"\n",
    "NOTEBOOK_RUN_ID = None\n",
    "\n",
    "\n",
    "# Service Configuration Flags\n",
    "ENABLE_QUALITY = True\n",
    "ENABLE_MODEL_DRIFT = True\n",
    "ENABLE_DATA_DRIFT = True\n",
    "ENABLE_EXPLAINABILITY = True\n",
    "ENABLE_FAIRNESS = True\n",
    "\n",
    "RUN_JOB = ENABLE_QUALITY or ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT or ENABLE_EXPLAINABILITY or ENABLE_FAIRNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Common Configuration Generation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample scoring data\n",
    "\n",
    "A sample scoring data is required to infer the schema of the complete data, so the size of the sample should be chosen accordingly. \n",
    "\n",
    "Additionally, the sample scoring data should have the following fields:\n",
    "1. Feature Columns\n",
    "2. Label/Target Column\n",
    "3. Prediction Column (with same data type as the label column)\n",
    "4. Probability Column (an array of model probabilities for all the class labels. Not required for regression models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORAGE_FORMAT** : One of [\"csv\", \"parquet\", \"orc\"]\n",
    "\n",
    "**Note:** \n",
    "1. Please select the format in which your training data is stored in Hive. The same format will be used to generate the various CREATE DDLs in this notebook.\n",
    "2. ORC format is not supported for zLinux environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_FORMAT = \"csv\"\n",
    "# STORAGE_FORMAT = \"parquet\"\n",
    "# STORAGE_FORMAT = \"orc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The sample data should be of type `pyspark.sql.dataframe.DataFrame`. The cell below gives samples on:\n",
    "- how to read a CSV file from the local system into a Pyspark Dataframe.\n",
    "- how to read parquet files in a directory from the local system into a Pyspark Dataframe.\n",
    "- how to read orc files in a directory from the local system into a Pyspark Dataframe. [Not supported for zLinux environments]\n",
    "\n",
    "It is important that the same storage format is chosen as the training data, otherwise there could be schema mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STORAGE_FORMAT == \"csv\":\n",
    "    # Load a csv or a directory containing csv files as PySpark DataFrame\n",
    "    # spark_df = spark.read.csv(\"/path/to/dir/containing/csv/files\", header=True, inferSchema=True)\n",
    "    pass\n",
    "\n",
    "elif STORAGE_FORMAT == \"parquet\":\n",
    "    # Load a directory containing parquet files as PySpark DataFrame\n",
    "    # spark_df = spark.read.parquet(\"/path/to/dir/containing/parquet/files\")\n",
    "    pass\n",
    "    \n",
    "elif STORAGE_FORMAT == \"orc\":\n",
    "    # Load a directory containing orc files as PySpark DataFrame\n",
    "    # spark_df = spark.read.orc(\"/path/to/dir/containing/orc/files\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # Load data from any source which matches the schema of the training data\n",
    "    pass\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model Type\n",
    "\n",
    "- Specify **binary** if the model is a binary classifier.\n",
    "- Specify **multiclass** if the model is a multi-class classifier.\n",
    "- Specify **regression** if the model is a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"binary\"\n",
    "# MODEL_TYPE = \"multiclass\"\n",
    "# MODEL_TYPE = \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide Column Details \n",
    "\n",
    "To proceed with this notebook, the following information is required.:\n",
    "\n",
    "- **LABEL_COLUMN**: The column which contains the target field (also known as label column or the class label).\n",
    "- **PREDICTION_COLUMN**: The column containing the model output. This should be of the same data type as the label column.\n",
    "- **PROBABILITY_COLUMN**: The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models.\n",
    "- **CLASS_PROBABILITIES**: The columns (of type double) containing the model probabilities of class labels. This is not required for regression models. For example, for Go Sales model deployed in MS Azure ML Studio, value of this property would be `[\"Scored Probabilities for Class \\\"Camping Equipment\\\"\", \"Scored Probabilities for Class \\\"Mountaineering Equipment\\\"\", \"Scored Probabilities for Class \\\"Personal Accessories\\\"\"]`. Please note escaping double quotes is a must-have requirement for above example.\n",
    "- **PROTECTED_ATTRIBUTES**: [Optional] The columns which exist in training data but are not used to train the model. This is required to monitor fairness on non-feature columns i.e Indirect Bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"<label_column>\"\n",
    "PREDICTION_COLUMN = \"<model prediction column>\"\n",
    "PROBABILITY_COLUMN = \"<model probability column. ignored in case of regression models>\"\n",
    "CLASS_PROBABILITIES = [\"<list of columns containing class probabilities. Ignored in case of regression models>\"]\n",
    "# [Optional] Provide list of protected attributes i.e non-feature columns present in the data.\n",
    "PROTECTED_ATTRIBUTES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample data and key columns provided above, the notebook will deduce the feature columns and the categorical columns. They will be printed in the output of this cell. If you wish to make changes to them, you can do so in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "feature_columns = spark_df.columns.copy()\n",
    "feature_columns.remove(LABEL_COLUMN)\n",
    "feature_columns.remove(PREDICTION_COLUMN)\n",
    "\n",
    "if MODEL_TYPE != \"regression\":\n",
    "    feature_columns.remove(PROBABILITY_COLUMN)\n",
    "\n",
    "if PROTECTED_ATTRIBUTES:\n",
    "    for protected_attribute in PROTECTED_ATTRIBUTES:\n",
    "        feature_columns.remove(protected_attribute)\n",
    "\n",
    "print(\"Feature Columns : {}\".format(feature_columns))\n",
    "\n",
    "categorical_columns = [f.name for f in spark_df.schema.fields if isinstance(f.dataType, (BooleanType, StringType)) and f.name in feature_columns]\n",
    "print(\"Categorical Columns : {}\".format(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_info = {\n",
    "    \"problem_type\": MODEL_TYPE,\n",
    "    \"model_type\": MODEL_TYPE,\n",
    "    \"label_column\": LABEL_COLUMN,\n",
    "    \"prediction\": PREDICTION_COLUMN,\n",
    "    \"probability\": PROBABILITY_COLUMN,\n",
    "    \"class_probabilities\": CLASS_PROBABILITIES\n",
    "}\n",
    "\n",
    "config_info[\"feature_columns\"] = feature_columns\n",
    "config_info[\"categorical_columns\"] = categorical_columns\n",
    "config_info[\"protected_attributes\"] = PROTECTED_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import validate_config_info\n",
    "validate_config_info(config_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Common Configuration\n",
    "\n",
    "IBM Watson OpenScale requires two additional fields - a unique identifier for each record in your feedback/payload tables (\"scoring_id\") and a timestamp field (\"scoring_timestamp\") denoting when that record entered the table. These fields are automatically added in the common configuration. \n",
    "\n",
    "Please make sure that these fields are present in the respective tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import generate_schemas\n",
    "\n",
    "common_config = config_info.copy()\n",
    "common_configuration = generate_schemas(spark_df, common_config)\n",
    "\n",
    "config_json = {}\n",
    "config_json[\"common_configuration\"] = common_configuration\n",
    "config_json[\"batch_notebook_version\"] = VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Scored Training data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_scored_training_table_ddl\n",
    "\n",
    "# Database Name where Scored Training Table should be created. If None or \"\", the default database is used.\n",
    "SCORED_TRAINING_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Scored Training Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the scored training data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "create_ddl = generate_scored_training_table_ddl(config_json, database_name=SCORED_TRAINING_DATABASE_NAME,\\\n",
    "                                         table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                         path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                         table_properties=table_properties)\n",
    "print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Feedback table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_feedback_table_ddl\n",
    "\n",
    "# Database Name where Feedback Table should be created. If None or \"\", the default database is used.\n",
    "FEEDBACK_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Feedback Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the feedback data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_QUALITY:\n",
    "    create_ddl = generate_feedback_table_ddl(config_json, database_name=FEEDBACK_DATABASE_NAME,\\\n",
    "                                             table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                             path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                             table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Payload table\n",
    "\n",
    "_Please make sure that the `scoring_timestamp` column in your payload data does not have NULL values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_payload_table_ddl\n",
    "\n",
    "# Database Name where Payload Table should be created. If None or \"\", the default database is used.\n",
    "PAYLOAD_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Payload Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the payload data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT or ENABLE_EXPLAINABILITY or ENABLE_FAIRNESS:\n",
    "    create_ddl = generate_payload_table_ddl(config_json, database_name=PAYLOAD_DATABASE_NAME,\\\n",
    "                                            table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                            path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                            table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Connection Details\n",
    "\n",
    "1. If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "    \n",
    "    - **IAE_SPARK_DISPLAY_NAME**: Display Name of the Spark instance in IBM Analytics Engine\n",
    "    - **IAE_SPARK_JOBS_ENDPOINT**: Spark Jobs Endpoint for IBM Analytics Engine\n",
    "    - **IBM_CPD_VOLUME**: IBM Cloud Pak for Data storage volume name\n",
    "    - **IBM_CPD_USERNAME**: IBM Cloud Pak for Data username\n",
    "    - **IBM_CPD_APIKEY**: IBM Cloud Pak for Data API key\n",
    "\n",
    "\n",
    "2. If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "    - **SPARK_MANAGER_ENDPOINT**: Endpoint URL where the Spark Manager Application is running\n",
    "    - **SPARK_MANAGER_USERNAME**: Username to connect to Spark Manager Application\n",
    "    - **SPARK_MANAGER_PASSWORD**: Password to connect to Spark Manager Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in IAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "IAE_SPARK_DISPLAY_NAME = \"<Display Name of the Spark instance in IBM Analytics Engine>\"\n",
    "IAE_SPARK_JOBS_ENDPOINT = \"<Spark Jobs Endpoint for IBM Analytics Engine>\"\n",
    "IBM_CPD_VOLUME = \"<IBM Cloud Pak for Data storage volume name>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username>\"\n",
    "IBM_CPD_APIKEY = \"<IBM Cloud Pak for Data API key>\"\n",
    "\n",
    "# Credentials Block for Spark in IAE\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"display_name\": IAE_SPARK_DISPLAY_NAME,\n",
    "        \"endpoint\": IAE_SPARK_JOBS_ENDPOINT,\n",
    "        \"location_type\": SparkType.IAE_SPARK.value,\n",
    "        \"volume\": IBM_CPD_VOLUME\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": IBM_CPD_USERNAME,\n",
    "        \"apikey\": IBM_CPD_APIKEY\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in Remote Hadoop Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "SPARK_MANAGER_ENDPOINT = \"<Endpoint URL where Spark Manager Application is running>\"\n",
    "SPARK_MANAGER_USERNAME = \"<Username to connect to Spark Manager Application>\"\n",
    "SPARK_MANAGER_PASSWORD = \"<Password to connect to Spark Manager Application>\"\n",
    "\n",
    "# Credentials Block for Spark in Remote Hadoop Ecosystem\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": SPARK_MANAGER_ENDPOINT,\n",
    "        \"location_type\": SparkType.REMOTE_SPARK.value\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": SPARK_MANAGER_USERNAME,\n",
    "        \"password\": SPARK_MANAGER_PASSWORD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Storage Inputs\n",
    "\n",
    "Enter Hive details. \n",
    " - **HIVE_METASTORE_URI**: Thrift URI for Hive Metastore to connect to\n",
    " - **TRAINING_DATABASE_NAME**: Name of the Database in Hive that has training table/view\n",
    " - **TRAINING_TABLE_NAME**: Name of the Table in HIve that has the scored training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVE_METASTORE_URI = \"<Thrift URI for Hive Metastore to connect to>\"\n",
    "TRAINING_DATABASE_NAME = \"<Name of the Database in Hive that has training table/view>\"\n",
    "TRAINING_TABLE_NAME = \"<Name of the Table in Hive that has the scored training data>\"\n",
    "\n",
    "# Flag to indicate if the Hive is secured with kerberos and spark in IAE is used\n",
    "KERBEROS_ENABLED = False \n",
    "# Provide Hadoop delegation token details if KERBEROS_ENABLED is True\n",
    "# Provide either secret_urn of the CP4D vault OR the delegation token endpoint. One of the two fields is mandatory to fetch the delegation token.\n",
    "HIVE_KERBEROS_PRINCIPAL = \"<The kerberos principal used to generate the delegation token>\"\n",
    "DELEGATION_TOKEN_SECRET_URN = \"<The secret_urn of the CP4D vault where the token is stored>\"\n",
    "DELEGATION_TOKEN_ENDPOINT = \"<The REST endpoint which generates and returns the delegation token>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_credentials = {}\n",
    "storage_details = {\n",
    "    \"type\": \"hive\",\n",
    "    \"connection\": {\n",
    "        \"metastore_url\": HIVE_METASTORE_URI,\n",
    "    },\n",
    "    \"credentials\": storage_credentials\n",
    "}\n",
    "\n",
    "# If KERBEROS_ENABLED is True, add the Hadoop delegation token details to storage_details\n",
    "if KERBEROS_ENABLED is True:\n",
    "    storage_details[\"connection\"][\"kerberos_enabled\"] = True\n",
    "    storage_details[\"credentials\"][\"kerberos_principal\"] = HIVE_KERBEROS_PRINCIPAL\n",
    "    if DELEGATION_TOKEN_SECRET_URN:\n",
    "        storage_details[\"credentials\"][\"delegation_token_urn\"] = DELEGATION_TOKEN_SECRET_URN\n",
    "    if DELEGATION_TOKEN_ENDPOINT:\n",
    "        storage_details[\"credentials\"][\"delegation_token_endpoint\"] = DELEGATION_TOKEN_ENDPOINT\n",
    "\n",
    "tables = [\n",
    "    {\n",
    "        \"database\": TRAINING_DATABASE_NAME,\n",
    "        \"table\": TRAINING_TABLE_NAME,\n",
    "        \"type\": \"training\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Resource Settings [Optional]\n",
    "\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `None` or `{}` if no customisation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": 2,\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": 1,\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": 2,\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": 1,\n",
    "    \n",
    "    #driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": 2,\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": 1\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Additional Spark Settings [Optional]\n",
    "\n",
    "Any other Spark property that can be set via **SparkConf**, provide them in the next cell. These properties are sent to the Spark cluster verbatim. Leave the variable `conf` to `None` or `{}` if no additional property is required.\n",
    "\n",
    "- [A list of available properties for Spark 2.4.6](https://spark.apache.org/docs/2.4.6/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "conf = {\n",
    "    \"spark.yarn.maxAppAttempts\": 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "conf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Drift Parameters [Optional]\n",
    "\n",
    "Provide the optional drift parameters in this cell. Leave the variable `drift_parameters` to `None` or `{}` if no additional parameter is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drift_parameters = {\n",
    "    \"model_drift\": {\n",
    "        # enable_drift_model_tuning - Controls whether there will be Hyper-Parameter \n",
    "        # Optimisation in the Drift Detection Model. Default: False\n",
    "        \"enable_drift_model_tuning\": True,\n",
    "        \n",
    "        # max_bins - Specify the maximum number of categories in categorical columns.\n",
    "        # Default: OpenScale will determine an approximate value. Use this only in cases\n",
    "        # where OpenScale approximation fails.\n",
    "        \"max_bins\": 10,\n",
    "    },\n",
    "    \"data_drift\": {\n",
    "        # enable_two_col_learner - Enable learning of data constraints on two column \n",
    "        # combinations. Default: True\n",
    "        \"enable_two_col_learner\": True,\n",
    "        \n",
    "        # use_alt_learner - Boolean parameter which switches learning method to help \n",
    "        # with performance during constraint learning process. Default: False\n",
    "        \"use_alt_learner\": False,\n",
    "        \n",
    "        # categorical_unique_threshold - Used to discard categorical columns with a\n",
    "        # large number of unique values relative to total rows in the column.\n",
    "        # Should be between 0 and 1. Default: 0.8\n",
    "        \"categorical_unique_threshold\": 0.7,\n",
    "        \n",
    "        # max_distinct_categories - Used to discard categorical columns with a large\n",
    "        # absolute number of unique categories. Also, used for not learning\n",
    "        # categorical-categorical constraint, if potential combinations of two columns\n",
    "        # are more than this number. Default: 100000\n",
    "        \"max_distinct_categories\": 10000\n",
    "\n",
    "        # max_ranges_modifier - Affects the number of ranges we find for a numerical column.\n",
    "        # For a numerical column, we learn multiple ranges instead of one min-max depending\n",
    "        # on how sparse data is. This modifier combined with approximate distinct values in\n",
    "        # the column defines the upper limit on how many bins to divide data into during\n",
    "        # multiple ranges computation. This can either be a float or a dictionary of column\n",
    "        # names and float values. Its value should be greater than 0. Default: 0.01\n",
    "        # 1. float: This value is applied for all numerical columns. Default value of 0.01\n",
    "        # indicates total number of bins used during computation of ranges are not more than\n",
    "        # 1% of distinct values in the column.\n",
    "        # 2. dict of str -> float: A column name -> value, dict can be used to over-ride\n",
    "        # individual modifier for each column. If not provided for a column, default value\n",
    "        # of 0.01 will be used.\n",
    "        \"max_ranges_modifier\": 0.01,\n",
    "            \n",
    "        # tail_discard_threshold -- Used to discard off values from either end of data\n",
    "        # distribution in a column if the data is found to have large ranges which results in\n",
    "        # data being divided into a large number of bins for multiple ranges computation. This\n",
    "        # threshold will be used if the these bins are found be greater than\n",
    "        # `max_ranges_modifier * approx_distinct_count` for a column. Default value indicates\n",
    "        # that 1 percentile data from either ends will be discarded. Its value can be between\n",
    "        # 0 and 0.1. Default: 0.01\n",
    "        \"tail_discard_threshold\": 0.01,\n",
    "        \n",
    "        # user_overrides - Used to override drift constraint learning to selectively learn \n",
    "        # constraints on feature columns. Its a list of configuration, each specifying \n",
    "        # whether to learn distribution and/or range constraint on given set of columns.\n",
    "        # First configuration of a given column would take preference.\n",
    "        # \n",
    "        # \"constraint_type\" can have two possible values : single|double - signifying \n",
    "        # if this configuration is for single column or two column constraint learning.\n",
    "        #\n",
    "        # \"learn_distribution_constraint\" : True|False - signifying whether to learn \n",
    "        # distribution constraint for given config or not.\n",
    "        #\n",
    "        # \"learn_range_constraint\" : True|False - signifying whether to learn range \n",
    "        # constraint for given config or not. Only applicable to numerical feature columns.\n",
    "        # \n",
    "        # \"features\" : [] - provides either a list of feature columns to be governed by \n",
    "        # given configuration for constraint learning.\n",
    "        # Its a list of strings containing feature column names if \"constraint_type\" is \"single\".\n",
    "        # Its a list of list of strings containing feature column names if \"constraint_type\" if \n",
    "        # \"double\". If only one column name is provided, all of the two column constraints \n",
    "        # involving this column will be dictated by given configuration during constraint learning.\n",
    "        # This list is case-insensitive.\n",
    "        #\n",
    "        # In the example below, first config block says do not learn distribution and range single \n",
    "        # column constraints for features \"MARITAL_STATUS\", \"PROFESSION\", \"IS_TENT\" and \"age\".\n",
    "        # Second config block says do not learn distribution and range two column constraints \n",
    "        # where \"IS_TENT\", \"PROFESSION\", and \"AGE\" are one of the two columns. Whereas, specifically, \n",
    "        # do not learn two column distribution and range constraint on combination of \"MARITAL_STATUS\" \n",
    "        # and \"PURCHASE_AMOUNT\".\n",
    "        \"user_overrides\": [\n",
    "            {\n",
    "                \"constraint_type\": \"single\",\n",
    "                \"learn_distribution_constraint\": False,\n",
    "                \"learn_range_constraint\": False,\n",
    "                \"features\": [\n",
    "                  \"MARITAL_STATUS\",\n",
    "                  \"PROFESSION\",\n",
    "                  \"IS_TENT\",\n",
    "                  \"age\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"constraint_type\": \"double\",\n",
    "                \"learn_distribution_constraint\": False,\n",
    "                \"learn_range_constraint\": False,\n",
    "                \"features\": [\n",
    "                  [\n",
    "                    \"IS_TENT\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"MARITAL_STATUS\"\n",
    "                    \"PURCHASE_AMOUNT\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"PROFESSION\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"AGE\"\n",
    "                  ]\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "drift_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Fairness Parameters [REQUIRED if `ENABLE_FAIRNESS` is set to True]\n",
    "\n",
    "Provide the fairness parameters in this cell. Leave the variable `fairness_parameters` to `None` or `{}` if fairness is not to be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fairness_parameters = {\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"feature\": \"<The fairness attribute name>\", # The feature on which the fairness check is to be done\n",
    "            \"majority\": [<majority groups/ranges for categorical/numerical columns respectively>],\n",
    "            \"minority\": [<minority groups/ranges for categorical/numerical columns respectively>],\n",
    "            \"threshold\": <The threshold value between 0 and 1> [OPTIONAL, default value is 0.8]\n",
    "        }\n",
    "    ],\n",
    "    \"class_label\": LABEL_COLUMN,\n",
    "    \"favourable_class\": [<favourable classes/ranges for classification/regression models repectively>],\n",
    "    \"unfavourable_class\": [<unfavourable classes/ranges for classification/regression models repectively>],\n",
    "    \"min_records\": <The minimum number of records on which the fairness check is to be done> [OPTIONAL]\n",
    "\n",
    "    # The following parameters are only supported for subscriptions with a synchronous scoring endpoint.\n",
    "    \n",
    "    \"perform_perturbation\": <(Boolean) Whether the user wants to calculate the balanced (payload + perturbed) data.>,\n",
    "    \"sample_size_percent\": <(Integer 1-100) How much percentage of data to be read for balanced data calculation.>,\n",
    "    \"numerical_perturb_count_per_row\": <[Optional] The number of perturbed rows to be generated per row for numerical perturbation. [Default: 2]>,\n",
    "    \"float_decimal_place_precision\": <[Optional] The decimal place precision to be used for numerical perturbation when data is float.>,\n",
    "    \"numerical_perturb_seed\": <[Optional] The seed to be used for numerical perturbation while picking up random values.>,\n",
    "    \"scoring_page_size\": <[Optional] The size of the page in the number of rows. [Default: 1000]>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "fairness_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Configuration Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Flag to enable or disable Queuing of IAE Spark jobs. \n",
    "# This feature ensures that if available resources are not enough to run an IAE spark job, the job gets queued and is scheduled to run whenever sufficient resources are available instead of failing the job.\n",
    "ENABLE_IAE_JOBS_QUEUING = False\n",
    "if ENABLE_IAE_JOBS_QUEUING is True:\n",
    "    os.environ[\"ENABLE_IAE_JOBS_QUEUING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PROGRESS = True\n",
    "\n",
    "arguments = {\n",
    "    \"batch_notebook_version\": VERSION,\n",
    "    \"common_configuration\" : common_configuration,\n",
    "    \"enable_data_drift\": ENABLE_DATA_DRIFT,\n",
    "    \"enable_model_drift\": ENABLE_MODEL_DRIFT,\n",
    "    \"enable_explainability\": ENABLE_EXPLAINABILITY,\n",
    "    \"enable_fairness\": ENABLE_FAIRNESS,\n",
    "    \"monitoring_run_id\": NOTEBOOK_RUN_ID,\n",
    "    \"storage\": storage_details,\n",
    "    \"tables\": tables,\n",
    "    \"show_progress\": SHOW_PROGRESS\n",
    "}\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    arguments[\"drift_parameters\"] = drift_parameters\n",
    "    \n",
    "if ENABLE_FAIRNESS:\n",
    "    if fairness_parameters is None or fairness_parameters == {}:\n",
    "        raise ValueError(\"Fairness parameters are required if fairness is enabled.\")\n",
    "    arguments[\"fairness_parameters\"] = fairness_parameters\n",
    "\n",
    "job_params = {\n",
    "    \"arguments\": arguments,\n",
    "    \"spark_settings\": spark_settings,\n",
    "    \"dependency_zip\": [],\n",
    "    \"conf\": conf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run the Configuration job. If `SHOW_PROGRESS` is `True`, it will also print the status of job in the output section. Please wait for the status to be **FINISHED**.\n",
    "\n",
    "A successful job status goes through the following values:\n",
    "1. STARTED\n",
    "2. Model Drift Configuration STARTED\n",
    "3. Data Drift Configuration STARTED\n",
    "    - Data Drift: Summary Stats Calculated\n",
    "    - Data Drift: Column Stats calculated.\n",
    "    - Data Drift: (number/total) CategoricalDistributionConstraint columns processed\n",
    "    - Data Drift: (number/total) NumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CategoricalNumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CatCatDistributionConstraint columns processed\n",
    "4. Explainability Configuration STARTED\n",
    "5. Explainability Configuration COMPLETED\n",
    "6. Fairness Configuration STARTED\n",
    "7. Fairness Configuration COMPLETED\n",
    "8. FINISHED\n",
    "\n",
    "If at anytime there is a failure, you will see a **FAILED** status with an exception trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.clients.engine_client import EngineClient\n",
    "from ibm_wos_utils.common.batch.jobs.configuration import Configuration\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import JobStatus\n",
    "\n",
    "if RUN_JOB:\n",
    "    job_name=\"Configuration_Job\"\n",
    "    client = EngineClient(credentials=credentials)\n",
    "    job_response = client.engine.run_job(job_name=job_name, job_class=Configuration,\n",
    "                                        job_args=job_params, background=True)\n",
    "\n",
    "    # Print Job Status.\n",
    "    if SHOW_PROGRESS:\n",
    "        JobStatus(client, job_response).print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `SHOW_PROGRESS` is `False`, you can run the below cell to check the job status at any point manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SHOW_PROGRESS and RUN_JOB:\n",
    "    job_id = job_response.get(\"id\")\n",
    "    print(client.engine.get_job_status(job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Configuration JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "\n",
    "if RUN_JOB:\n",
    "        configuration = client.engine.get_file(job_response.get(\n",
    "                \"output_file_path\") + \"/configuration.json\")\n",
    "        config=json.loads(json.loads(configuration).get(\"configuration\"))\n",
    "else:\n",
    "        config = config_json\n",
    "        \n",
    "# handle class probabilities explicitly\n",
    "from ibm_wos_utils.joblib.utils.param_utils import get\n",
    "\n",
    "class_probabilities = get(common_configuration, \"class_probabilities\")\n",
    "if class_probabilities:\n",
    "    # clean up any class probability columns already added\n",
    "    updated_output_data_schema_fields = []\n",
    "    for field in get(config, \"common_configuration.output_data_schema.fields\"):\n",
    "        if get(field, \"metadata.modeling_role\") == \"class_probability\":\n",
    "            continue\n",
    "\n",
    "        updated_output_data_schema_fields.append(field)\n",
    "\n",
    "    # add class probabilities to output_data_schema\n",
    "    for class_probability in class_probabilities:\n",
    "        updated_output_data_schema_fields.append({\n",
    "            \"name\": class_probability,\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": True,\n",
    "            \"metadata\": {\n",
    "                \"modeling_role\": \"class_probability\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    config[\"common_configuration\"][\"output_data_schema\"][\"fields\"] = updated_output_data_schema_fields\n",
    "    config[\"common_configuration\"][\"probability_fields\"] = class_probabilities\n",
    "\n",
    "display(create_download_link(config, \"config\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Drift Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "    \n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    drift_archive = client.engine.get_file(job_response.get(\n",
    "            \"output_file_path\") + \"/drift_configuration\")\n",
    "\n",
    "    with open(\"tmp_drift.tar.gz\", mode=\"wb\") as tf:\n",
    "        tf.write(drift_archive)\n",
    "        tf.flush()\n",
    "        drift_archive = spark.sparkContext.sequenceFile(tf.name).collect()[0][1]\n",
    "    os.remove(\"tmp_drift.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `ENABLE_MODEL_DRIFT` is True, and the `MODEL_TYPE` is not `regression`, the below cell checks the training quality of the drift detection model that helps detect the drop in the accuracy. If the trained drift detection model did not meet the quality standards, a message is displayed to the user saying that the drop in the accuracy cannot be detected. By default, the drift model is generated without any hyperparameter optimisation, i.e. `enable_drift_model_tuning` is `False`. The user can try running the configuration job again by setting `enable_drift_model_tuning` as `True` in the `drift_parameters` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import check_for_ddm_quality\n",
    "\n",
    "if ENABLE_MODEL_DRIFT and (MODEL_TYPE != \"regression\"):\n",
    "    check_for_ddm_quality(drift_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    display(create_download_link(drift_archive, \"drift\", client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Drifted Transactions table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_drift_table_ddl\n",
    "\n",
    "# Database Name where Drifted Transactions Table should be created. If None or \"\", the default database is used.\n",
    "DRIFT_DATABASE_NAME = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    print(generate_drift_table_ddl(drift_archive, database_name=DRIFT_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Perturbations csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibm_wos_utils.explainability.utils.perturbations import Perturbations\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    perturbations=Perturbations(training_stats=config.get(\"explainability_configuration\"), problem_type=MODEL_TYPE)\n",
    "    perturbs_df = perturbations.generate_perturbations()\n",
    "    perturbs_df.to_csv(\"perturbations.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perturbations required for explainability are stored in the file perturbations.csv in the above step.\n",
    "The user should score these perturbations against the user model and provide the scoring output as a dataframe with **probability** and **prediction** columns.\n",
    "\n",
    "Please note that the probability and prediction column names in the data frame should be same as PREDICTION_COLUMN and PROBABILITY_COLUMN provided in this notebook.\n",
    "\n",
    "Note: For regression model probability column is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_archive\n",
    "from json import dumps\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    # Load a csv output of scored perturbations as pandas DataFrame\n",
    "    scored_perturbations = pd.read_csv(\"scored_perturbations.csv\")\n",
    "    archive_data = {\n",
    "        \"perturbations.csv\": scored_perturbations.to_csv(index=False),\n",
    "        \"training_statistics.json\": dumps({\"training_statistics\": config.get(\"explainability_configuration\")})\n",
    "    }\n",
    "    display(create_archive(data=archive_data, archive_name=\"explainability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Explanations Queue table [Optional]\n",
    "\n",
    "Provide details for creating a separate Explanations Queue table. IBM Watson OpenScale will be generating Explanations for all the transactions in this table. Alternatively, the payload table created in the notebook above can also be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_payload_table_ddl\n",
    "\n",
    "# Database Name where Explanations Queue Table should be created. If None or \"\", the default database is used.\n",
    "EXPLANATIONS_QUEUE_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Explanations Queue Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the payload data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    create_ddl = generate_payload_table_ddl(config_json, database_name=EXPLANATIONS_QUEUE_DATABASE_NAME,\\\n",
    "                                            table_prefix=\"explanations_queue\",table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                            path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                            table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Explanations Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_explanations_table_ddl\n",
    "\n",
    "# Database Name where Explanations Table should be created. If None or \"\", the default database is used.\n",
    "EXPLANATIONS_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Explanations table Data in HDFS. If not provided hive will determine automatically.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    # For zLinux environments, please uncomment this to generate DDL.\n",
    "    # print(generate_explanations_table_ddl(database_name=EXPLANATIONS_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID, path_to_hdfs_directory=path_to_hdfs_directory, stored_as=\"csv\"))\n",
    "    \n",
    "    # For all other environments, please use this to generate DDL.\n",
    "    print(generate_explanations_table_ddl(database_name=EXPLANATIONS_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID, path_to_hdfs_directory=path_to_hdfs_directory))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Configuration Archive\n",
    "Collect all the artefacts generated above - configuration json, drift archive, explain archive - and bundle them into an archive. This archive is used as is by IBM Watson OpenScale UI/SDK to onboard model for monitoring. \n",
    "UI/SDK will identify the different artefacts and appropriately upload to respective monitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import json\n",
    "\n",
    "# update flags in configuration json\n",
    "config[\"common_configuration\"][\"enable_drift\"] = True if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT else False\n",
    "config[\"common_configuration\"][\"enable_explainability\"] = ENABLE_EXPLAINABILITY\n",
    "config[\"common_configuration\"][\"enable_fairness\"] = ENABLE_FAIRNESS\n",
    "config[\"common_configuration\"][\"enable_quality\"] = ENABLE_QUALITY\n",
    "\n",
    "# write to local\n",
    "with open(\"common_configuration.json\", \"wb\") as f:\n",
    "    f.write(json.dumps(config).encode('utf-8'))\n",
    "\n",
    "if ENABLE_FAIRNESS:\n",
    "    # write fairness_statistics.json to local\n",
    "    with open(\"fairness_statistics.json\", \"wb\") as f:\n",
    "        f.write(json.dumps(config.get(\"fairness_configuration\")).encode('utf-8'))\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    # build and write drift archive to local\n",
    "    with open(\"drift_archive.tar.gz\", \"wb\") as f:\n",
    "        from ibm_wos_utils.joblib.utils.notebook_utils import bundle_drift_model\n",
    "        f.write(bundle_drift_model(drift_archive, client))\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    # build and write explain archive to local\n",
    "    from io import BytesIO\n",
    "    with BytesIO() as archive:\n",
    "        with tarfile.open(fileobj=archive, mode=\"w:gz\") as tf:\n",
    "            for filename, filedata in archive_data.items():\n",
    "                content = BytesIO(filedata.encode(\"utf8\"))\n",
    "                tarinfo = tarfile.TarInfo(filename)\n",
    "                tarinfo.size = len(content.getvalue())\n",
    "                tf.addfile(\n",
    "                    tarinfo=tarinfo, fileobj=content)\n",
    "\n",
    "        with open(\"explainability.tar.gz\", \"wb\") as f:\n",
    "            f.write(archive.getvalue())\n",
    "\n",
    "with tarfile.open(\"configuration_archive.tar.gz\", \"w:gz\") as f:\n",
    "    # collect all files from local and write to configuration archive\n",
    "    f.add(\"common_configuration.json\", arcname=\"common_configuration.json\")\n",
    "    \n",
    "    if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "        f.add(\"drift_archive.tar.gz\", arcname=\"drift_archive.tar.gz\")\n",
    "        \n",
    "    if ENABLE_EXPLAINABILITY:\n",
    "        f.add(\"explainability.tar.gz\", arcname=\"explainability.tar.gz\")\n",
    "        \n",
    "    if ENABLE_FAIRNESS:\n",
    "        f.add(\"fairness_statistics.json\", arcname=\"fairness_statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create download link for configuration package\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "data = None\n",
    "with open('configuration_archive.tar.gz', 'rb') as f:\n",
    "    # read configuration archive from local\n",
    "    data = f.read()\n",
    "\n",
    "format_args = {\n",
    "    \"payload\": base64.b64encode(data).decode(),\n",
    "    \"title\": \"Download Configuration Archive\",\n",
    "    \"filename\": \"configuration_archive.tar.gz\"\n",
    "}\n",
    "\n",
    "from IPython.display import HTML\n",
    "html = '<a download=\"{filename}\" href=\"data:text/json;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "HTML(html.format(**format_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "Developed by [Prem Piyush Goyal](mailto:prempiyush@in.ibm.com), [Pratap Kishore Varma V](mailto:pvemulam@in.ibm.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "301f5018d300c6eb6119f5ed999379f124efc62cb3f23918facbbc5aa749f0e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
