{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating configuration for batch subscriptions in IBM Watson OpenScale in IBM Cloud Pak for Data v4.0\n",
    "\n",
    "This notebook shows how to generate the following artefacts:\n",
    "1. Configuration JSON needed to configure an IBM Watson OpenScale subscription.\n",
    "2. Drift Configuration Archive\n",
    "3. Explainabiity Perturbations Archive\n",
    "3. DDLs for creating Feedback, Payload, Drifted Transactions and Explanations tables\n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) and download the generated artefacts. These artefacts \n",
    "have to be then uploaded to IBM Watson OpenScale UI. \n",
    "\n",
    "PS: This notebook can only generate artefacts for one model at a time. For multiple models, this notebook needs to be run for each model separately.\n",
    "\n",
    "**Contents:**\n",
    "1. [Installing Dependencies](#Installing-Dependencies)\n",
    "2. [Select IBM Watson OpenScale Services](#Select-IBM-Watson-OpenScale-Services)\n",
    "3. [Read sample scoring data](#Read-sample-scoring-data)\n",
    "4. [Specify Model Inputs](#Specify-Model-Inputs)\n",
    "5. [Generate Common Configuration](#Generate-Common-Configuration)\n",
    "6. [Generate DDL for creating Scored Training data table](#Generate-DDL-for-creating-Scored-Training-data-table)\n",
    "6. [Generate DDL for creating Feedback table](#Generate-DDL-for-creating-Feedback-table)\n",
    "7. [Generate DDL for creating Payload table](#Generate-DDL-for-creating-Payload-table)\n",
    "8. [Provide Spark Connection Details](#Provide-Spark-Connection-Details)\n",
    "9. [Provide Storage Inputs](#Provide-Storage-Inputs)\n",
    "10. [Provide Spark Resource Settings [Optional]](#Provide-Spark-Resource-Settings-[Optional])\n",
    "11. [Provide Additional Spark Settings [Optional]](#Provide-Additional-Spark-Settings-[Optional])\n",
    "12. [Provide Drift Parameters [Optional]](#Provide-Drift-Parameters-[Optional])\n",
    "13. [Provide Fairness Parameters [Optional]](#Provide-Fairness-Parameters-[Optional])\n",
    "14. [Run Configuration Job](#Run-Configuration-Job)\n",
    "15. [Download Configuration JSON](#Download-Configuration-JSON)\n",
    "16. [Download Drift Archive](#Download-Drift-Archive)\n",
    "17. [Generate DDL for creating Drifted Transactions Table](#Generate-DDL-for-creating-Drifted-Transactions-table)\n",
    "18. [Generate Perturbations csv](#Generate-Perturbations-csv)\n",
    "19. [Generate DDL for creating Explanations Queue table](#Generate-DDL-for-creating-Explanations-Queue-table)\n",
    "20. [Generate DDL for creating Explanations Table](#Generate-DDL-for-creating-Explanations-Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: Restart kernel after the dependencies are installed\n",
    "import sys\n",
    "\n",
    "PYTHON = sys.executable\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts pyspark | tail -n 1  \n",
    "\n",
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==0.24.2 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=0.24.2\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts \"ibm-wos-utils==4.0.31\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select IBM Watson OpenScale Services\n",
    "\n",
    "Details of the service-specific flags available:\n",
    "\n",
    "- ENABLE_QUALITY: Flag to allow generation of common configuration details needed if quality alone is selected\n",
    "- ENABLE_FAIRNESS : Flag to allow generation of fairness specific data distribution needed for configuration\n",
    "- ENABLE_MODEL_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Model Drift.\n",
    "- ENABLE_DATA_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Data Drift.\n",
    "- ENABLE_EXPLAINABILITY : Flag to allow generation of explainability configuration and perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5737-H76\n",
    "# Copyright IBM Corp. 2020, 2022\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = \"hive-2.0.2\"\n",
    "\n",
    "# Version history\n",
    "\n",
    "# hive-2.0.2 : Upgrade ibm-wos-utils to 4.0.31\n",
    "# hive-2.0.1 : Make notebook compatible for zLinux environments; Upgrade ibm-wos-utils to 4.0.25\n",
    "# hive-2.0   : Upgrade ibm-wos-utils to 4.0.24\n",
    "# 2.0        : Added support for fairness and explainability\n",
    "# 1.0        : Initial release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Input: Keep an identifiable name. This id is used to append to various table creation DDLs.\n",
    "# A random UUID is used if this is not present.\n",
    "# NOTEBOOK_RUN_ID = \"some_identifiable_name\"\n",
    "NOTEBOOK_RUN_ID = None\n",
    "\n",
    "\n",
    "# Service Configuration Flags\n",
    "ENABLE_QUALITY = True\n",
    "ENABLE_MODEL_DRIFT = True\n",
    "ENABLE_DATA_DRIFT = True\n",
    "ENABLE_EXPLAINABILITY = True\n",
    "ENABLE_FAIRNESS = True\n",
    "\n",
    "RUN_JOB = ENABLE_QUALITY or ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT or ENABLE_EXPLAINABILITY or ENABLE_FAIRNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Common Configuration Generation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample scoring data\n",
    "\n",
    "A sample scoring data is required to infer the schema of the complete data, so the size of the sample should be chosen accordingly. \n",
    "\n",
    "Additionally, the sample scoring data should have the following fields:\n",
    "1. Feature Columns\n",
    "2. Label/Target Column\n",
    "3. Prediction Column (with same data type as the label column)\n",
    "4. Probability Column (an array of model probabilities for all the class labels. Not required for regression models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORAGE_FORMAT** : One of [\"csv\", \"parquet\", \"orc\"]\n",
    "\n",
    "**Note:** \n",
    "1. Please select the format in which your training data is stored in Hive. The same format will be used to generate the various CREATE DDLs in this notebook.\n",
    "2. ORC format is not supported for zLinux environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_FORMAT = \"csv\"\n",
    "# STORAGE_FORMAT = \"parquet\"\n",
    "# STORAGE_FORMAT = \"orc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The sample data should be of type `pyspark.sql.dataframe.DataFrame`. The cell below gives samples on:\n",
    "- how to read a CSV file from the local system into a Pyspark Dataframe.\n",
    "- how to read parquet files in a directory from the local system into a Pyspark Dataframe.\n",
    "- how to read orc files in a directory from the local system into a Pyspark Dataframe. [Not supported for zLinux environments]\n",
    "\n",
    "It is important that the same storage format is chosen as the training data, otherwise there could be schema mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STORAGE_FORMAT == \"csv\":\n",
    "    # Load a csv or a directory containing csv files as PySpark DataFrame\n",
    "    # spark_df = spark.read.csv(\"/path/to/dir/containing/csv/files\", header=True, inferSchema=True)\n",
    "    pass\n",
    "\n",
    "elif STORAGE_FORMAT == \"parquet\":\n",
    "    # Load a directory containing parquet files as PySpark DataFrame\n",
    "    # spark_df = spark.read.parquet(\"/path/to/dir/containing/parquet/files\")\n",
    "    pass\n",
    "    \n",
    "elif STORAGE_FORMAT == \"orc\":\n",
    "    # Load a directory containing orc files as PySpark DataFrame\n",
    "    # spark_df = spark.read.orc(\"/path/to/dir/containing/orc/files\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # Load data from any source which matches the schema of the training data\n",
    "    pass\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model Type\n",
    "\n",
    "- Specify **binary** if the model is a binary classifier.\n",
    "- Specify **multiclass** if the model is a multi-class classifier.\n",
    "- Specify **regression** if the model is a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"binary\"\n",
    "# MODEL_TYPE = \"multiclass\"\n",
    "# MODEL_TYPE = \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide Column Details \n",
    "\n",
    "To proceed with this notebook, the following information is required.:\n",
    "\n",
    "- **LABEL_COLUMN**: The column which contains the target field (also known as label column or the class label).\n",
    "- **PREDICTION_COLUMN**: The column containing the model output. This should be of the same data type as the label column.\n",
    "- **PROBABILITY_COLUMN**: The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"<label_column>\"\n",
    "PREDICTION_COLUMN = \"<model prediction column>\"\n",
    "PROBABILITY_COLUMN = \"<model probability column. ignored in case of regression models>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample data and key columns provided above, the notebook will deduce the feature columns and the categorical columns. They will be printed in the output of this cell. If you wish to make changes to them, you can do so in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "feature_columns = spark_df.columns.copy()\n",
    "feature_columns.remove(LABEL_COLUMN)\n",
    "feature_columns.remove(PREDICTION_COLUMN)\n",
    "\n",
    "if MODEL_TYPE != \"regression\":\n",
    "    feature_columns.remove(PROBABILITY_COLUMN)\n",
    "\n",
    "print(\"Feature Columns : {}\".format(feature_columns))\n",
    "\n",
    "categorical_columns = [f.name for f in spark_df.schema.fields if isinstance(f.dataType, (BooleanType, StringType)) and f.name in feature_columns]\n",
    "print(\"Categorical Columns : {}\".format(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_info = {\n",
    "    \"problem_type\": MODEL_TYPE,\n",
    "    \"label_column\": LABEL_COLUMN,\n",
    "    \"prediction\": PREDICTION_COLUMN,\n",
    "    \"probability\": PROBABILITY_COLUMN\n",
    "}\n",
    "\n",
    "config_info[\"feature_columns\"] = feature_columns\n",
    "config_info[\"categorical_columns\"] = categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import validate_config_info\n",
    "validate_config_info(config_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Common Configuration\n",
    "\n",
    "IBM Watson OpenScale requires two additional fields - a unique identifier for each record in your feedback/payload tables (\"scoring_id\") and a timestamp field (\"scoring_timestamp\") denoting when that record entered the table. These fields are automatically added in the common configuration. \n",
    "\n",
    "Please make sure that these fields are present in the respective tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import generate_schemas\n",
    "\n",
    "common_config = config_info.copy()\n",
    "common_configuration = generate_schemas(spark_df, common_config)\n",
    "\n",
    "config_json = {}\n",
    "config_json[\"common_configuration\"] = common_configuration\n",
    "config_json[\"batch_notebook_version\"] = VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Scored Training data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_scored_training_table_ddl\n",
    "\n",
    "# Database Name where Scored Training Table should be created. If None or \"\", the default database is used.\n",
    "SCORED_TRAINING_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Scored Training Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the scored training data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "create_ddl = generate_scored_training_table_ddl(config_json, database_name=SCORED_TRAINING_DATABASE_NAME,\\\n",
    "                                         table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                         path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                         table_properties=table_properties)\n",
    "print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Feedback table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_feedback_table_ddl\n",
    "\n",
    "# Database Name where Feedback Table should be created. If None or \"\", the default database is used.\n",
    "FEEDBACK_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Feedback Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the feedback data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_QUALITY:\n",
    "    create_ddl = generate_feedback_table_ddl(config_json, database_name=FEEDBACK_DATABASE_NAME,\\\n",
    "                                             table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                             path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                             table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Payload table\n",
    "\n",
    "_Please make sure that the `scoring_timestamp` column in your payload data does not have NULL values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_payload_table_ddl\n",
    "\n",
    "# Database Name where Payload Table should be created. If None or \"\", the default database is used.\n",
    "PAYLOAD_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Payload Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the payload data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT or ENABLE_EXPLAINABILITY or ENABLE_FAIRNESS:\n",
    "    create_ddl = generate_payload_table_ddl(config_json, database_name=PAYLOAD_DATABASE_NAME,\\\n",
    "                                            table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                            path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                            table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Connection Details\n",
    "\n",
    "1. If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "    \n",
    "    - **IAE_SPARK_DISPLAY_NAME**: Display Name of the Spark instance in IBM Analytics Engine\n",
    "    - **IAE_SPARK_JOBS_ENDPOINT**: Spark Jobs Endpoint for IBM Analytics Engine\n",
    "    - **IBM_CPD_VOLUME**: IBM Cloud Pak for Data storage volume name\n",
    "    - **IBM_CPD_USERNAME**: IBM Cloud Pak for Data username\n",
    "    - **IBM_CPD_APIKEY**: IBM Cloud Pak for Data API key\n",
    "\n",
    "\n",
    "2. If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "    - **SPARK_MANAGER_ENDPOINT**: Endpoint URL where the Spark Manager Application is running\n",
    "    - **SPARK_MANAGER_USERNAME**: Username to connect to Spark Manager Application\n",
    "    - **SPARK_MANAGER_PASSWORD**: Password to connect to Spark Manager Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in IAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "IAE_SPARK_DISPLAY_NAME = \"<Display Name of the Spark instance in IBM Analytics Engine>\"\n",
    "IAE_SPARK_JOBS_ENDPOINT = \"<Spark Jobs Endpoint for IBM Analytics Engine>\"\n",
    "IBM_CPD_VOLUME = \"<IBM Cloud Pak for Data storage volume name>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username>\"\n",
    "IBM_CPD_APIKEY = \"<IBM Cloud Pak for Data API key>\"\n",
    "\n",
    "# Credentials Block for Spark in IAE\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"display_name\": IAE_SPARK_DISPLAY_NAME,\n",
    "        \"endpoint\": IAE_SPARK_JOBS_ENDPOINT,\n",
    "        \"location_type\": SparkType.IAE_SPARK.value,\n",
    "        \"volume\": IBM_CPD_VOLUME\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": IBM_CPD_USERNAME,\n",
    "        \"apikey\": IBM_CPD_APIKEY\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in Remote Hadoop Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "SPARK_MANAGER_ENDPOINT = \"<Endpoint URL where Spark Manager Application is running>\"\n",
    "SPARK_MANAGER_USERNAME = \"<Username to connect to Spark Manager Application>\"\n",
    "SPARK_MANAGER_PASSWORD = \"<Password to connect to Spark Manager Application>\"\n",
    "\n",
    "# Credentials Block for Spark in Remote Hadoop Ecosystem\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": SPARK_MANAGER_ENDPOINT,\n",
    "        \"location_type\": SparkType.REMOTE_SPARK.value\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": SPARK_MANAGER_USERNAME,\n",
    "        \"password\": SPARK_MANAGER_PASSWORD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Storage Inputs\n",
    "\n",
    "Enter Hive details. \n",
    " - **HIVE_METASTORE_URI**: Thrift URI for Hive Metastore to connect to\n",
    " - **TRAINING_DATABASE_NAME**: Name of the Database in Hive that has training table/view\n",
    " - **TRAINING_TABLE_NAME**: Name of the Table in HIve that has the scored training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVE_METASTORE_URI = \"<Thrift URI for Hive Metastore to connect to>\"\n",
    "TRAINING_DATABASE_NAME = \"<Name of the Database in Hive that has training table/view>\"\n",
    "TRAINING_TABLE_NAME = \"<Name of the Table in HIve that has the scored training data>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_details = {\n",
    "    \"type\": \"hive\",\n",
    "    \"connection\": {\n",
    "        \"metastore_url\": HIVE_METASTORE_URI,\n",
    "    }\n",
    "}\n",
    "\n",
    "tables = [\n",
    "    {\n",
    "        \"database\": TRAINING_DATABASE_NAME,\n",
    "        \"table\": TRAINING_TABLE_NAME,\n",
    "        \"type\": \"training\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Resource Settings [Optional]\n",
    "\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `None` or `{}` if no customisation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": 2,\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": 1,\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": 2,\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": 1,\n",
    "    \n",
    "    #driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": 2,\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": 1\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Additional Spark Settings [Optional]\n",
    "\n",
    "Any other Spark property that can be set via **SparkConf**, provide them in the next cell. These properties are sent to the Spark cluster verbatim. Leave the variable `conf` to `None` or `{}` if no additional property is required.\n",
    "\n",
    "- [A list of available properties for Spark 2.4.6](https://spark.apache.org/docs/2.4.6/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "conf = {\n",
    "    \"spark.yarn.maxAppAttempts\": 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "conf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Drift Parameters [Optional]\n",
    "\n",
    "Provide the optional drift parameters in this cell. Leave the variable `drift_parameters` to `None` or `{}` if no additional parameter is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drift_parameters = {\n",
    "    \"model_drift\": {\n",
    "        # enable_drift_model_tuning - Controls whether there will be Hyper-Parameter \n",
    "        # Optimisation in the Drift Detection Model. Default: False\n",
    "        \"enable_drift_model_tuning\": True,\n",
    "        \n",
    "        # max_bins - Specify the maximum number of categories in categorical columns.\n",
    "        # Default: OpenScale will determine an approximate value. Use this only in cases\n",
    "        # where OpenScale approximation fails.\n",
    "        \"max_bins\": 10,\n",
    "    },\n",
    "    \"data_drift\": {\n",
    "        # enable_two_col_learner - Enable learning of data constraints on two column \n",
    "        # combinations. Default: True\n",
    "        \"enable_two_col_learner\": True,\n",
    "        \n",
    "        # categorical_unique_threshold - Used to discard categorical columns with a\n",
    "        # large number of unique values relative to total rows in the column.\n",
    "        # Should be between 0 and 1. Default: 0.8\n",
    "        \"categorical_unique_threshold\": 0.7,\n",
    "        \n",
    "        # max_distinct_categories - Used to discard categorical columns with a large\n",
    "        # absolute number of unique categories. Also, used for not learning\n",
    "        # categorical-categorical constraint, if potential combinations of two columns\n",
    "        # are more than this number. Default: 100000\n",
    "        \"max_distinct_categories\": 10000\n",
    "        \n",
    "        # user_overrides - Used to override drift constraint learning to selectively learn \n",
    "        # constraints on feature columns. Its a list of configuration, each specifying \n",
    "        # whether to learn distribution and/or range constraint on given set of columns.\n",
    "        # First configuration of a given column would take preference.\n",
    "        # \n",
    "        # \"constraint_type\" can have two possible values : single|double - signifying \n",
    "        # if this configuration is for single column or two column constraint learning.\n",
    "        #\n",
    "        # \"learn_distribution_constraint\" : True|False - signifying whether to learn \n",
    "        # distribution constraint for given config or not.\n",
    "        #\n",
    "        # \"learn_range_constraint\" : True|False - signifying whether to learn range \n",
    "        # constraint for given config or not. Only applicable to numerical feature columns.\n",
    "        # \n",
    "        # \"features\" : [] - provides either a list of feature columns to be governed by \n",
    "        # given configuration for constraint learning.\n",
    "        # Its a list of strings containing feature column names if \"constraint_type\" is \"single\".\n",
    "        # Its a list of list of strings containing feature column names if \"constraint_type\" if \n",
    "        # \"double\". If only one column name is provided, all of the two column constraints \n",
    "        # involving this column will be dictated by given configuration during constraint learning.\n",
    "        # This list is case-insensitive.\n",
    "        #\n",
    "        # In the example below, first config block says do not learn distribution and range single \n",
    "        # column constraints for features \"MARITAL_STATUS\", \"PROFESSION\", \"IS_TENT\" and \"age\".\n",
    "        # Second config block says do not learn distribution and range two column constraints \n",
    "        # where \"IS_TENT\", \"PROFESSION\", and \"AGE\" are one of the two columns. Whereas, specifically, \n",
    "        # do not learn two column distribution and range constraint on combination of \"MARITAL_STATUS\" \n",
    "        # and \"PURCHASE_AMOUNT\".\n",
    "        \"user_overrides\": [\n",
    "            {\n",
    "                \"constraint_type\": \"single\",\n",
    "                \"learn_distribution_constraint\": False,\n",
    "                \"learn_range_constraint\": False,\n",
    "                \"features\": [\n",
    "                  \"MARITAL_STATUS\",\n",
    "                  \"PROFESSION\",\n",
    "                  \"IS_TENT\",\n",
    "                  \"age\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"constraint_type\": \"double\",\n",
    "                \"learn_distribution_constraint\": False,\n",
    "                \"learn_range_constraint\": False,\n",
    "                \"features\": [\n",
    "                  [\n",
    "                    \"IS_TENT\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"MARITAL_STATUS\"\n",
    "                    \"PURCHASE_AMOUNT\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"PROFESSION\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"AGE\"\n",
    "                  ]\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "drift_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Fairness Parameters [REQUIRED if `ENABLE_FAIRNESS` is set to True]\n",
    "\n",
    "Provide the fairness parameters in this cell. Leave the variable `fairness_parameters` to `None` or `{}` if fairness is not to be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fairness_parameters = {\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"feature\": \"<The fairness attribute name>\", # The feature on which the fairness check is to be done\n",
    "            \"majority\": [<majority groups/ranges for categorical/numerical columns respectively>],\n",
    "            \"minority\": [<minority groups/ranges for categorical/numerical columns respectively>],\n",
    "            \"threshold\": <The threshold value between 0 and 1> [OPTIONAL, default value is 0.8]\n",
    "        }\n",
    "    ],\n",
    "    \"class_label\": LABEL_COLUMN,\n",
    "    \"favourable_class\": [<favourable classes/ranges for classification/regression models repectively>],\n",
    "    \"unfavourable_class\": [<unfavourable classes/ranges for classification/regression models repectively>],\n",
    "    \"min_records\": <The minimum number of records on which the fairness check is to be done> [OPTIONAL]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "fairness_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Configuration Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PROGRESS = True\n",
    "\n",
    "arguments = {\n",
    "    \"batch_notebook_version\": VERSION,\n",
    "    \"common_configuration\" : common_configuration,\n",
    "    \"enable_data_drift\": ENABLE_DATA_DRIFT,\n",
    "    \"enable_model_drift\": ENABLE_MODEL_DRIFT,\n",
    "    \"enable_explainability\": ENABLE_EXPLAINABILITY,\n",
    "    \"enable_fairness\": ENABLE_FAIRNESS,\n",
    "    \"monitoring_run_id\": NOTEBOOK_RUN_ID,\n",
    "    \"storage\": storage_details,\n",
    "    \"tables\": tables,\n",
    "    \"show_progress\": SHOW_PROGRESS\n",
    "}\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    arguments[\"drift_parameters\"] = drift_parameters\n",
    "    \n",
    "if ENABLE_FAIRNESS:\n",
    "    if fairness_parameters is None or fairness_parameters == {}:\n",
    "        raise ValueError(\"Fairness parameters are required if fairness is enabled.\")\n",
    "    arguments[\"fairness_parameters\"] = fairness_parameters\n",
    "\n",
    "job_params = {\n",
    "    \"arguments\": arguments,\n",
    "    \"spark_settings\": spark_settings,\n",
    "    \"dependency_zip\": [],\n",
    "    \"conf\": conf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run the Configuration job. If `SHOW_PROGRESS` is `True`, it will also print the status of job in the output section. Please wait for the status to be **FINISHED**.\n",
    "\n",
    "A successful job status goes through the following values:\n",
    "1. STARTED\n",
    "2. Model Drift Configuration STARTED\n",
    "3. Data Drift Configuration STARTED\n",
    "    - Data Drift: Summary Stats Calculated\n",
    "    - Data Drift: Column Stats calculated.\n",
    "    - Data Drift: (number/total) CategoricalDistributionConstraint columns processed\n",
    "    - Data Drift: (number/total) NumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CategoricalNumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CatCatDistributionConstraint columns processed\n",
    "4. Explainability Configuration STARTED\n",
    "5. Explainability Configuration COMPLETED\n",
    "6. Fairness Configuration STARTED\n",
    "7. Fairness Configuration COMPLETED\n",
    "8. FINISHED\n",
    "\n",
    "If at anytime there is a failure, you will see a **FAILED** status with an exception trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.clients.engine_client import EngineClient\n",
    "from ibm_wos_utils.common.batch.jobs.configuration import Configuration\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import JobStatus\n",
    "\n",
    "if RUN_JOB:\n",
    "    job_name=\"Configuration_Job\"\n",
    "    client = EngineClient(credentials=credentials)\n",
    "    job_response = client.engine.run_job(job_name=job_name, job_class=Configuration,\n",
    "                                        job_args=job_params, background=True)\n",
    "\n",
    "    # Print Job Status.\n",
    "    if SHOW_PROGRESS:\n",
    "        JobStatus(client, job_response).print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `SHOW_PROGRESS` is `False`, you can run the below cell to check the job status at any point manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SHOW_PROGRESS and RUN_JOB:\n",
    "    job_id = job_response.get(\"id\")\n",
    "    print(client.engine.get_job_status(job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Configuration JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "\n",
    "if RUN_JOB:\n",
    "        configuration = client.engine.get_file(job_response.get(\n",
    "                \"output_file_path\") + \"/configuration.json\")\n",
    "        config=json.loads(json.loads(configuration).get(\"configuration\"))\n",
    "else:\n",
    "        config = config_json\n",
    "display(create_download_link(config, \"config\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Drift Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "    \n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    drift_archive = client.engine.get_file(job_response.get(\n",
    "            \"output_file_path\") + \"/drift_configuration\")\n",
    "\n",
    "    with NamedTemporaryFile() as tf:\n",
    "        tf.write(drift_archive)\n",
    "        tf.flush()\n",
    "        drift_archive = spark.sparkContext.sequenceFile(tf.name).collect()[0][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `ENABLE_MODEL_DRIFT` is True, and the `MODEL_TYPE` is not `regression`, the below cell checks the training quality of the drift detection model that helps detect the drop in the accuracy. If the trained drift detection model did not meet the quality standards, a message is displayed to the user saying that the drop in the accuracy cannot be detected. By default, the drift model is generated without any hyperparameter optimisation, i.e. `enable_drift_model_tuning` is `False`. The user can try running the configuration job again by setting `enable_drift_model_tuning` as `True` in the `drift_parameters` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import check_for_ddm_quality\n",
    "\n",
    "if ENABLE_MODEL_DRIFT and (MODEL_TYPE != \"regression\"):\n",
    "    check_for_ddm_quality(drift_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    display(create_download_link(drift_archive, \"drift\", client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Drifted Transactions table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_drift_table_ddl\n",
    "\n",
    "# Database Name where Drifted Transactions Table should be created. If None or \"\", the default database is used.\n",
    "DRIFT_DATABASE_NAME = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    print(generate_drift_table_ddl(drift_archive, database_name=DRIFT_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Perturbations csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibm_wos_utils.explainability.utils.perturbations import Perturbations\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    perturbations=Perturbations(training_stats=config.get(\"explainability_configuration\"), problem_type=MODEL_TYPE)\n",
    "    perturbs_df = perturbations.generate_perturbations()\n",
    "    perturbs_df.to_csv(\"perturbations.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perturbations required for explainability are stored in the file perturbations.csv in the above step.\n",
    "The user should score these perturbations against the user model and provide the scoring output as a dataframe with **probability** and **prediction** columns.\n",
    "\n",
    "Note: For regression model probability column is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_archive\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    # Load a csv output of scored perturbations as pandas DataFrame\n",
    "    scored_perturbations = pd.read_csv(\"scored_perturbations.csv\")\n",
    "    \n",
    "    display(create_archive(scored_perturbations.to_csv(index=False), \"perturbations.csv\", \"perturbations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Explanations Queue table [Optional]\n",
    "\n",
    "Provide details for creating a separate Explanations Queue table. IBM Watson OpenScale will be generating Explanations for all the transactions in this table. Alternatively, the payload table created in the notebook above can also be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_payload_table_ddl\n",
    "\n",
    "# Database Name where Explanations Queue Table should be created. If None or \"\", the default database is used.\n",
    "EXPLANATIONS_QUEUE_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Explanations Queue Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the payload data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    create_ddl = generate_payload_table_ddl(config_json, database_name=EXPLANATIONS_QUEUE_DATABASE_NAME,\\\n",
    "                                            table_prefix=\"explanations_queue\",table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                            path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                            table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Explanations Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_explanations_table_ddl\n",
    "\n",
    "# Database Name where Explanations Table should be created. If None or \"\", the default database is used.\n",
    "EXPLANATIONS_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Explanations table Data in HDFS. If not provided hive will determine automatically.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "if ENABLE_EXPLAINABILITY:\n",
    "    # For zLinux environments, please uncomment this to generate DDL.\n",
    "    # print(generate_explanations_table_ddl(database_name=EXPLANATIONS_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID, path_to_hdfs_directory=path_to_hdfs_directory, stored_as=\"csv\"))\n",
    "    \n",
    "    # For all other environments, please use this to generate DDL.\n",
    "    print(generate_explanations_table_ddl(database_name=EXPLANATIONS_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID, path_to_hdfs_directory=path_to_hdfs_directory))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "Developed by [Prem Piyush Goyal](mailto:prempiyush@in.ibm.com), [Pratap Kishore Varma V](mailto:pvemulam@in.ibm.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
