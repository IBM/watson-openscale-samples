{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating configuration package for batch subscriptions in IBM Watson OpenScale in IBM Cloud Pak for Data v4.7\n",
    "\n",
    "This notebook shows how to generate the configuration package containing following artefacts:\n",
    "1. Configuration JSON needed to configure an IBM Watson OpenScale subscription.\n",
    "2. Drift Configuration Archive\n",
    "3. Explainability Configuration Archive\n",
    "\n",
    "Optionally, user can generate following using Configuration JSON:\n",
    "1. DDLs for creating Feedback, Payload, Drifted Transactions and Explanations tables\n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) and download the generated configuration package.\n",
    "This package contains artefacts for different monitors which have to be then uploaded to IBM Watson OpenScale UI during configuration. \n",
    "\n",
    "PS: This notebook can only generate configuration package for one model at a time. For multiple models, this notebook needs to be run for each model separately.\n",
    "\n",
    "**Contents:**\n",
    "1. [Install Pre-requisites and required dependencies](#Installing-Dependencies)\n",
    "2. [Specify Model Details](#Specify-Model-Details)\n",
    "3. [Select IBM Watson OpenScale Services and provide configuration options](#Select-IBM-Watson-OpenScale-Services)\n",
    "4. [Provide Spark Connection Details](#Provide-Spark-Connection-Details)\n",
    "7. [Provide Storage Inputs](#Provide-Storage-Inputs)\n",
    "8. [Generate Configuration Package](#Generate-Configuration-Package)\n",
    "    1. [Download Configuration Package](#Download-Configuration-Package)\n",
    "9. [Generate DDLs for tables](#Generate-DDLs-For-Tables)\n",
    "10. [Helper Methods](#Helper-Methods)\n",
    "    1. [Use sample data and get feature and categorical columns](#Use-Sample-Data-And-Get-Feature-And-Categorical-Columns)\n",
    "    2. [Generate DDL for creating Scored Training data table](#Generate-DDL-for-creating-Scored-Training-data-table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Restart kernel after the dependencies are installed\n",
    "import sys\n",
    "\n",
    "PYTHON = sys.executable\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts pyspark | tail -n 1\n",
    "\n",
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==1.1.1 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=1.1.1\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts \"ibm-metrics-plugin>=4.7.0\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Model Details\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| label_column | The column which contains the target field (also known as label column or the class label). | |\n",
    "| model_type | Enumeration classifying if your model is a binary or a multi-class classifier or a regressor. | `binary`, `multiclass`, `regression` |\n",
    "| feature_columns | Columns identified as features by model. The order of the feature columns should be same as that of the subscription. Use helper methods to compute these if required.| A list of column names |\n",
    "| categorical_columns | Feature columns identified as categorical by model. Use helper methods to compute these if required.| A list of column names |\n",
    "| prediction | The column containing the model output. This should be of the same data type as the label column. | |\n",
    "| probability | The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models. | |\n",
    "| class_probabilities | The columns (of type double) containing the model probabilities of class labels. This is not required for regression models. For example, for Go Sales model deployed in MS Azure ML Studio, value of this property would be `[\"Scored Probabilities for Class \\\"Camping Equipment\\\"\", \"Scored Probabilities for Class \\\"Mountaineering Equipment\\\"\", \"Scored Probabilities for Class \\\"Personal Accessories\\\"\"]`. Please note escaping double quotes is a must-have requirement for above example. | |\n",
    "| protected_attributes | [Optional] The columns which exist in training data but are not used to train the model. This is required to monitor fairness on non-feature columns i.e Indirect Bias.| A list of non-feature column names|\n",
    "\n",
    "\n",
    "## Select IBM Watson OpenScale services\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| enable_quality | Boolean value to allow generation of common configuration details needed if quality alone is selected | `True` or `False` |\n",
    "| enable_fairness | Boolean value to allow generation of fairness specific data distribution needed for configuration | `True` or `False` |\n",
    "| enable_drift | Boolean value to allow generation of Drift Archive containing relevant information for Model and Data Drift. | `True` or `False` |\n",
    "| enable_explainability | Boolean value to allow generation of explainability configuration  | `True` or `False` |\n",
    "\n",
    "\n",
    "### Provide Drift Parameters [Required if enable_drift is set to True]\n",
    "Provide the drift parameters. `model_drift.enable` and `data_drift.enable` flags must be set if drift is enabled.\n",
    "\n",
    "### Provide Fairness Parameters [Required if enable_fairness is set to True]\n",
    "Provide the fairness parameters. Leave the variable `fairness_parameters` to `None` or `{}` if fairness is not to be enabled.\n",
    "\n",
    "### Provide a method to use for scoring [Required if enable_explainability is set to True]\n",
    "As part of configuration, explainability requires a scoring function to be defined which should take data frame as input and output couple of arrays.\n",
    "\n",
    "- Input dataframe is expected to contain feature columns\n",
    "- Score function must return output as prediction column array and probability column array.\n",
    "- The data type of the label column and prediction column should be same . User needs to make sure that label column and prediction column array should have the same unique class labels\n",
    "- Please update the score function below with the help of templates documented [here](https://github.com/IBM-Watson/aios-data-distribution/blob/master/Score%20function%20templates%20for%20drift%20detection.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    \"model_type\" : \"<to_be_edited>\",\n",
    "    \"label_column\" : \"<to_be_edited>\",\n",
    "    \"feature_columns\": [\"<to_be_edited>\"],\n",
    "    \"categorical_columns\": [\"<to_be_edited>\"],\n",
    "    \"prediction\" : \"<to_be_edited>\",\n",
    "    \"probability\" : \"<to_be_edited>\",\n",
    "    \"class_probabilities\": [\"<to_be_edited>\"],\n",
    "    \"class_labels\": [\"<to_be_edited>\"], # [Optional]. The list of unique class labels in the order of model prediction.\n",
    "    \"enable_quality\" : True,\n",
    "    \"enable_drift\" : True,\n",
    "    \"enable_fairness\" : True,\n",
    "    \"enable_explainability\" : True\n",
    "}\n",
    "# [Optional] Provide list of protected attributes i.e non-feature columns present in the data.\n",
    "protected_attributes = []\n",
    "common_params[\"protected_attributes\"] = protected_attributes\n",
    "\n",
    "\"\"\"\n",
    "drift_parameters = {\n",
    "    \"model_drift\": {\n",
    "        \"enable\": True,\n",
    "        # enable_drift_model_tuning - Controls whether there will be Hyper-Parameter \n",
    "        # Optimisation in the Drift Detection Model. Default: False\n",
    "        \"enable_drift_model_tuning\": True,\n",
    "        \n",
    "        # max_bins - Specify the maximum number of categories in categorical columns.\n",
    "        # Default: OpenScale will determine an approximate value. Use this only in cases\n",
    "        # where OpenScale approximation fails.\n",
    "        \"max_bins\": 10,\n",
    "    },\n",
    "    \"data_drift\": {\n",
    "        \"enable\": True,\n",
    "        # enable_two_col_learner - Enable learning of data constraints on two column \n",
    "        # combinations. Default: True\n",
    "        \"enable_two_col_learner\": True,\n",
    "        \n",
    "        # use_alt_learner - Boolean parameter which switches learning method to help \n",
    "        # with performance during constraint learning process. Default: False\n",
    "        \"use_alt_learner\": False,\n",
    "        \n",
    "        # categorical_unique_threshold - Used to discard categorical columns with a\n",
    "        # large number of unique values relative to total rows in the column.\n",
    "        # Should be between 0 and 1. Default: 0.8\n",
    "        \"categorical_unique_threshold\": 0.7,\n",
    "        \n",
    "        # max_distinct_categories - Used to discard categorical columns with a large\n",
    "        # absolute number of unique categories. Also, used for not learning\n",
    "        # categorical-categorical constraint, if potential combinations of two columns\n",
    "        # are more than this number. Default: 100000\n",
    "        \"max_distinct_categories\": 10000\n",
    "\n",
    "        # max_ranges_modifier - Affects the number of ranges we find for a numerical column.\n",
    "        # For a numerical column, we learn multiple ranges instead of one min-max depending\n",
    "        # on how sparse data is. This modifier combined with approximate distinct values in\n",
    "        # the column defines the upper limit on how many bins to divide data into during\n",
    "        # multiple ranges computation. This can either be a float or a dictionary of column\n",
    "        # names and float values. Its value should be greater than 0. Default: 0.01\n",
    "        # 1. float: This value is applied for all numerical columns. Default value of 0.01\n",
    "        # indicates total number of bins used during computation of ranges are not more than\n",
    "        # 1% of distinct values in the column.\n",
    "        # 2. dict of str -> float: A column name -> value, dict can be used to over-ride\n",
    "        # individual modifier for each column. If not provided for a column, default value\n",
    "        # of 0.01 will be used.\n",
    "        \"max_ranges_modifier\": 0.01,\n",
    "            \n",
    "        # tail_discard_threshold -- Used to discard off values from either end of data\n",
    "        # distribution in a column if the data is found to have large ranges which results in\n",
    "        # data being divided into a large number of bins for multiple ranges computation. This\n",
    "        # threshold will be used if the these bins are found be greater than\n",
    "        # `max_ranges_modifier * approx_distinct_count` for a column. Default value indicates\n",
    "        # that 1 percentile data from either ends will be discarded. Its value can be between\n",
    "        # 0 and 0.1. Default: 0.01\n",
    "        \"tail_discard_threshold\": 0.01,\n",
    "        \n",
    "        # user_overrides - Used to override drift constraint learning to selectively learn \n",
    "        # constraints on feature columns. Its a list of configuration, each specifying \n",
    "        # whether to learn distribution and/or range constraint on given set of columns.\n",
    "        # First configuration of a given column would take preference.\n",
    "        # \n",
    "        # \"constraint_type\" can have two possible values : single|double - signifying \n",
    "        # if this configuration is for single column or two column constraint learning.\n",
    "        #\n",
    "        # \"learn_distribution_constraint\" : True|False - signifying whether to learn \n",
    "        # distribution constraint for given config or not.\n",
    "        #\n",
    "        # \"learn_range_constraint\" : True|False - signifying whether to learn range \n",
    "        # constraint for given config or not. Only applicable to numerical feature columns.\n",
    "        # \n",
    "        # \"features\" : [] - provides either a list of feature columns to be governed by \n",
    "        # given configuration for constraint learning.\n",
    "        # Its a list of strings containing feature column names if \"constraint_type\" is \"single\".\n",
    "        # Its a list of list of strings containing feature column names if \"constraint_type\" if \n",
    "        # \"double\". If only one column name is provided, all of the two column constraints \n",
    "        # involving this column will be dictated by given configuration during constraint learning.\n",
    "        # This list is case-insensitive.\n",
    "        #\n",
    "        # In the example below, first config block says do not learn distribution and range single \n",
    "        # column constraints for features \"MARITAL_STATUS\", \"PROFESSION\", \"IS_TENT\" and \"age\".\n",
    "        # Second config block says do not learn distribution and range two column constraints \n",
    "        # where \"IS_TENT\", \"PROFESSION\", and \"AGE\" are one of the two columns. Whereas, specifically, \n",
    "        # do not learn two column distribution and range constraint on combination of \"MARITAL_STATUS\" \n",
    "        # and \"PURCHASE_AMOUNT\".\n",
    "        \"user_overrides\": [\n",
    "            {\n",
    "                \"constraint_type\": \"single\",\n",
    "                \"learn_distribution_constraint\": False,\n",
    "                \"learn_range_constraint\": False,\n",
    "                \"features\": [\n",
    "                  \"MARITAL_STATUS\",\n",
    "                  \"PROFESSION\",\n",
    "                  \"IS_TENT\",\n",
    "                  \"age\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"constraint_type\": \"double\",\n",
    "                \"learn_distribution_constraint\": False,\n",
    "                \"learn_range_constraint\": False,\n",
    "                \"features\": [\n",
    "                  [\n",
    "                    \"IS_TENT\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"MARITAL_STATUS\"\n",
    "                    \"PURCHASE_AMOUNT\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"PROFESSION\"\n",
    "                  ],\n",
    "                  [\n",
    "                    \"AGE\"\n",
    "                  ]\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "drift_parameters = {\n",
    "    \"model_drift\": {\n",
    "        \"enable\": True\n",
    "    },\n",
    "    \"data_drift\": {\n",
    "        \"enable\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "fairness_parameters = {\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"feature\": \"<The fairness attribute name>\", # The feature on which the fairness check is to be done\n",
    "            \"majority\": [<majority groups/ranges for categorical/numerical columns respectively>],\n",
    "            \"minority\": [<minority groups/ranges for categorical/numerical columns respectively>],\n",
    "            \"metric_ids\": [<list of metrics ids (metrics) to be computed],\n",
    "            \"threshold\": <The threshold value between 0 and 1> #this is needed for disparate impact\n",
    "            # Valid metrics are fairness_value (disparate impact),statistical_parity_difference,average_odds_difference, false_discovery_rate_difference, error_rate_difference, false_negative_rate_difference, false_omission_rate_difference, false_positive_rate_difference, true_positive_rate_difference, average_abs_odds_difference\n",
    "        }\n",
    "    ],\n",
    "    \"thresholds\" : [\n",
    "        {\n",
    "        \"metric_id\": \"<metric_id>\",\n",
    "        \"specific_values\": [\n",
    "            {\n",
    "                \"applies_to\": [\n",
    "                    {\n",
    "                        \"key\": \"feature\",\n",
    "                        \"type\": \"tag\",\n",
    "                        \"value\": \"<fairness attribute name>\"\n",
    "                    }\n",
    "                ],\n",
    "                \"value\": <lower value>\n",
    "            }\n",
    "        ],\n",
    "        \"type\": \"lower_limit\",\n",
    "        \"value\": <lower value>\n",
    "    },\n",
    "    {\n",
    "        \"metric_id\": \"<metric_id>\",\n",
    "        \"specific_values\": [\n",
    "            {\n",
    "                \"applies_to\": [\n",
    "                    {\n",
    "                        \"key\": \"feature\",\n",
    "                        \"type\": \"tag\",\n",
    "                        \"value\": \"<fairness attribute name>\"\n",
    "                    }\n",
    "                ],\n",
    "                \"value\": <upper_value>\n",
    "            }\n",
    "        ],\n",
    "        \"type\": \"upper_limit\",\n",
    "        \"value\": <upper_value>\n",
    "    }\n",
    "    ],\n",
    "#    #example of fairness configuration:\n",
    "#     \"features\": [\n",
    "#         {\n",
    "#             \"feature\": \"Sex\", # The feature on which the fairness check is to be done\n",
    "#             \"majority\": [\"male\"],\n",
    "#             \"minority\": [\"female\"]\n",
    "#             \"metric_ids\": [\"fairness_value\",\"statistical_parity_difference\"]\n",
    "#         }\n",
    "#     ],\n",
    "#     \"thresholds\" = [{\n",
    "#         \"metric_id\": \"fairness_value\",\n",
    "#         \"specific_values\": [\n",
    "#             {\n",
    "#                 \"applies_to\": [\n",
    "#                     {\n",
    "#                         \"key\": \"feature\",\n",
    "#                         \"type\": \"tag\",\n",
    "#                         \"value\": \"Sex\"\n",
    "#                     }\n",
    "#                 ],\n",
    "#                 \"value\": 85\n",
    "#             }\n",
    "#         ],\n",
    "#         \"type\": \"lower_limit\",\n",
    "#         \"value\": 85\n",
    "#     },\n",
    "#     {\n",
    "#         \"metric_id\": \"fairness_value\",\n",
    "#         \"specific_values\": [\n",
    "#             {\n",
    "#                 \"applies_to\": [\n",
    "#                     {\n",
    "#                         \"key\": \"feature\",\n",
    "#                         \"type\": \"tag\",\n",
    "#                         \"value\": \"Sex\"\n",
    "#                     }\n",
    "#                 ],\n",
    "#                 \"value\": 125\n",
    "#             }\n",
    "#         ],\n",
    "#         \"type\": \"upper_limit\",\n",
    "#         \"value\": 125\n",
    "#     },\n",
    "#     {\n",
    "#         \"metric_id\": \"statistical_parity_difference\",\n",
    "#         \"specific_values\": [\n",
    "#             {\n",
    "#                 \"applies_to\": [\n",
    "#                     {\n",
    "#                         \"key\": \"feature\",\n",
    "#                         \"type\": \"tag\",\n",
    "#                         \"value\": \"Sex\"\n",
    "#                     }\n",
    "#                 ],\n",
    "#                 \"value\": -0.3\n",
    "#             }\n",
    "#         ],\n",
    "#         \"type\": \"lower_limit\",\n",
    "#         \"value\": -0.3\n",
    "#     },\n",
    "#     {\n",
    "#         \"metric_id\": \"statistical_parity_difference\",\n",
    "#         \"specific_values\": [\n",
    "#             {\n",
    "#                 \"applies_to\": [\n",
    "#                     {\n",
    "#                         \"key\": \"feature\",\n",
    "#                         \"type\": \"tag\",\n",
    "#                         \"value\": \"Sex\"\n",
    "#                     }\n",
    "#                 ],\n",
    "#                 \"value\": 0.3\n",
    "#             }\n",
    "#         ],\n",
    "#         \"type\": \"upper_limit\",\n",
    "#         \"value\": 0.3\n",
    "#     }],\n",
    "    \n",
    "    \"class_label\": common_params.get(\"label_column\"),\n",
    "    \"favourable_class\": [<favourable classes/ranges for classification/regression models repectively>],\n",
    "    \"unfavourable_class\": [<unfavourable classes/ranges for classification/regression models repectively>],\n",
    "    \"min_records\": <The minimum number of records on which the fairness check is to be done>,\n",
    "\n",
    "    # The following parameters are only supported for subscriptions with a synchronous scoring endpoint.\n",
    "    \n",
    "    \"perform_perturbation\": <(Boolean) Whether the user wants to calculate the balanced (payload + perturbed) data.>,\n",
    "    \"sample_size_percent\": <(Integer 1-100) How much percentage of data to be read for balanced data calculation.>,\n",
    "    \"numerical_perturb_count_per_row\": <[Optional] The number of perturbed rows to be generated per row for numerical perturbation. [Default: 2]>,\n",
    "    \"float_decimal_place_precision\": <[Optional] The decimal place precision to be used for numerical perturbation when data is float.>,\n",
    "    \"numerical_perturb_seed\": <[Optional] The seed to be used for numerical perturbation while picking up random values.>,\n",
    "    \"scoring_page_size\": <[Optional] The size of the page in the number of rows. [Default: 1000]>\n",
    "}\n",
    "\"\"\"\n",
    "fairness_parameters = {}\n",
    "\n",
    "\"\"\"\n",
    "# Lime global explanation feature is available from Cloud Pak for Data version 4.6.4 onwards.\n",
    "# Set the below explainability parameters to enable lime global explanation generation.\n",
    "# Note: When LIME global explanation is enabled, the explainability archive upload and explainability monitor enablement should be done using python sdk/api. \n",
    "# LIME global explanation configuration is not supported from IBM Watson OpenScale GUI.\n",
    "explainability_parameters = {\n",
    "    \"global_explanation\": {\n",
    "        \"enabled\": True, # Enable global explanation\n",
    "        \"explanation_method\": \"lime\", # The explanation method to use\n",
    "        \"training_data_sample_size\": 1000, # [Optional] The sample size of records to be used for generating training data global explanation. If not specified entire training data is used.\n",
    "        \"sample_size\": 1000, # [Optional] The sample size of records to be used for generating payload data global explanation. If not specified entire data in the payload window is used.\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "explainability_parameters = {}\n",
    "scoring_fn = None\n",
    "\n",
    "common_params[\"drift_parameters\"] = drift_parameters\n",
    "common_params[\"fairness_parameters\"] = fairness_parameters\n",
    "common_params[\"explainability_parameters\"] = explainability_parameters\n",
    "common_params[\"score_function\"] = scoring_fn\n",
    "common_params[\"score_batch_size\"] = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Spark Connection Details\n",
    "\n",
    "To generate configuration for monitoring models in IBM Watson OpenScale, a spark compute engine is required. It can be either IBM Analytics Engine or your own Spark Cluster. Provide details of any one of them in this section.\n",
    "\n",
    "Please note, if you are using your own Spark cluster, checkout IBM Watson OpenScale documentation on how to setup spark manager API to enable interface for use with IBM Watson OpenScale services.\n",
    "\n",
    "### Parameters for IBM Analytics Engine\n",
    "If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| display_name | Display Name of the Spark instance in IBM Analytics Engine | |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For IBM IAE, this must be set to `cpd_iae`. | `cpd_iae` |\n",
    "| endpoint | Spark Jobs Endpoint for IBM Analytics Engine | |\n",
    "| volume | IBM Cloud Pak for Data storage volume name | |\n",
    "| username | IBM Cloud Pak for Data username | |\n",
    "| apikey | IBM Cloud Pak for Data API key | |\n",
    "\n",
    "### Parameters for Remote Spark Cluster\n",
    "If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For Remote Spark, this must be set to `custom`. | `custom` |\n",
    "| endpoint | Endpoint URL where the Spark Manager Application is running | |\n",
    "| username | Username to connect to Spark Manager Application | |\n",
    "| password | Password to connect to Spark Manager Application | |\n",
    "\n",
    "\n",
    "### Provide Spark Resource Settings [Optional]\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `{}` if no customisation is required.\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| max_num_executors | Maximum Number of executors to launch for this session |\n",
    "| min_executors | Minimum Number of executors to launch for this session |\n",
    "| executor_cores | Number of cores to use for each executor |\n",
    "| executor_memory | Amount of memory (in GBs) to use per executor process |\n",
    "| driver_cores | Number of cores to use for the driver process |\n",
    "| driver_memory | Amount of memory (in GBs) to use for the driver process |\n",
    "\n",
    "### Provide Additional Spark Settings [Optional]\n",
    "\n",
    "Any other Spark property that can be set via **SparkConf**. These properties are sent to the Spark cluster verbatim. Leave the variable `conf` to `None` or `{}` if no additional property is required.\n",
    "If `conf` is being set, please make sure to set some default values to `spark_settings` parameters.\n",
    "\n",
    "- [A list of available properties for Spark 2.4.6](https://spark.apache.org/docs/2.4.6/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_connection_info = {\n",
    "    \"credentials\": {\n",
    "        \"connection\": {\n",
    "            \"endpoint\": \"<to_be_edited>\",\n",
    "            \"location_type\": \"<to_be_edited>\",\n",
    "            \"display_name\": \"<to_be_edited>\",\n",
    "            \"volume\": \"<to_be_edited>\"\n",
    "        },\n",
    "        \"credentials\": {\n",
    "            \"username\": \"<to_be_edited>\",\n",
    "            \"password\": \"<to_be_edited>\",\n",
    "            \"apikey\": \"<to_be_edited>\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": \"2\",\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": \"1\",\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": \"2\",\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": \"2\",\n",
    "    \n",
    "    # driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": \"2\",\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": \"1\"\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = {}\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "\n",
    "conf = {\n",
    "    \"spark.yarn.maxAppAttempts\": 1\n",
    "}\n",
    "\"\"\"\n",
    "# conf = {}\n",
    "# spark_settings[\"conf\"] = conf\n",
    "\n",
    "spark_connection_info[\"spark_settings\"] = spark_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Scored Training Data Table location\n",
    "\n",
    "For generating configuration for monitoring model in IBM Watson OpenScale, scored training data table location is required. Supported locations are hive or DB2. If you do not have any such table already available, please refer to helper methods section on how to generate a DDL for scoring training data table. Using this DDL, create your table, load data and provide location here.\n",
    "\n",
    "### Provide DB2 table details where training data is hosted\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| type | Describes the type of storage being used. For DB2, this must be set to `jdbc`. | `jdbc` |\n",
    "| jdbc_url | Connection string for jdbc. Example: `jdbc:db2://jdbc_host:jdbc_port/database_name` | |\n",
    "| jdbc_driver | Optional. Class name of the JDBC driver to use to connect. Example: for DB2 use `com.ibm.db2.jcc.DB2Driver` ||\n",
    "| use_ssl | Boolean Flag to indicate whether to use SSL while connecting | `True` or `False` |\n",
    "| certificate | SSL Certificate [Base64 encoded string] of the JDBC Connection. Ignored if `use_ssl` is `False`. |\n",
    "| location_type | Identifies the type of location for connection to use. For DB2, this must be set to `jdbc`. | `jdbc` |\n",
    "| username | Username of the JDBC Connection | |\n",
    "| password | Password of the JDBC Connection | |\n",
    "| database | Name of database hosting training data table | |\n",
    "| schema | Name of schema hosting training data table | |\n",
    "| table | Name of training data table | |\n",
    "| partition_column | The column to help Spark read and write data using multiple workers in your JDBC storage. This will help improve the performance of your Spark jobs. Please be careful when choosing an existing feature column as partition column. If data in this feature column is not properly divided across various possible values, it could lead to data-skew problem with Spark computation. Which means, majority of data is sent to one worker for computation - leading to wastage of compute resources and increased computation time. It is recommended to use a column with monotonically increasing value as partition column. | |\n",
    "| num_partitions | The maximum number of partitions that Spark can divide the data into. In JDBC, it also means the maximum number of connections that Spark can make to the JDBC store for reading/writing data. The recommended value is calculated as: 3 * num_executors * num_cores_per_executor. | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_connection = {\n",
    "    \"storage_details\" : {\n",
    "        \"type\": \"jdbc\",\n",
    "        \"connection\": {\n",
    "            \"location_type\": \"jdbc\",\n",
    "            \"jdbc_url\": \"<to_be_edited>\",\n",
    "            \"jdbc_driver\": \"<to_be_edited>\",\n",
    "            \"use_ssl\": \"<to_be_edited>\",\n",
    "            \"certificate\": \"<to_be_edited>\"\n",
    "        },\n",
    "        \"credentials\": {\n",
    "            \"username\": \"<to_be_edited>\",\n",
    "            \"password\": \"<to_be_edited>\"\n",
    "        }\n",
    "    },\n",
    "    \"tables\" : [{\n",
    "        \"type\": \"training\",\n",
    "        \"database\": \"<to_be_edited>\",\n",
    "        \"schema\": \"<to_be_edited>\",\n",
    "        \"table\": \"<to_be_edited>\",\n",
    "        \"parameters\": {\n",
    "            \"partition_column\": \"<to_be_edited>\",\n",
    "            \"num_partitions\": \"<to_be_edited>\"\n",
    "        }\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate common configuration archive containing all the artefacts required for enabling monitors\n",
    "\n",
    "The following cell will run the Configuration job. It will also print the status of job in the output section if available. Please wait for the status to be **FINISHED**.\n",
    "\n",
    "A successful job status goes through the following values:\n",
    "1. STARTED\n",
    "2. Model Drift Configuration STARTED\n",
    "3. Data Drift Configuration STARTED\n",
    "    - Data Drift: Summary Stats Calculated\n",
    "    - Data Drift: Column Stats calculated.\n",
    "    - Data Drift: (number/total) CategoricalDistributionConstraint columns processed\n",
    "    - Data Drift: (number/total) NumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CategoricalNumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CatCatDistributionConstraint columns processed\n",
    "4. Explainability Configuration STARTED\n",
    "5. Explainability Configuration COMPLETED\n",
    "6. Fairness Configuration STARTED\n",
    "7. Fairness Configuration COMPLETED\n",
    "8. FINISHED\n",
    "\n",
    "If at anytime there is a failure, you will see a **FAILED** status with an exception trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ibm_metrics_plugin.common.utils.configuration_utility import ConfigurationUtility\n",
    "config_utility = ConfigurationUtility(common_params, training_data_connection,spark_connection_info)\n",
    "config_utility.generate_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download generated configuration archive.\n",
    "**Note:**\n",
    "\n",
    "**When LIME global explanation is enabled, the configuration archive upload and explainability monitor enablement should be done using python sdk/api.**\n",
    "\n",
    "**LIME global explanation configuration is not supported from IBM Watson OpenScale GUI.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./archives/configuration_archive.tar.gz\", \"rb\") as binary_file:\n",
    "    configuration_archive = binary_file.read()\n",
    "\n",
    "display(config_utility.create_download_link(configuration_archive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL\n",
    "\n",
    "Following cells must be executed only if you are going to create required tables on your own. Otherwise, you can also choose to create these tables in IBM Watson OpenScale UI.\n",
    "\n",
    "Using configuration package:\n",
    "1. Load common configuration json\n",
    "2. Load drift archive\n",
    "\n",
    "**STORAGE_FORMAT** : One of [`csv`, `parquet`, `orc`]\n",
    "\n",
    "**Note:** \n",
    "1. Please select the format in which your training data is stored in Hive. The same format will be used to generate the various CREATE DDLs in this notebook.\n",
    "2. ORC format is not supported for zLinux environments\n",
    "\n",
    "**Generate DDLs for creating required tables:**\n",
    "1. Feedback Table\n",
    "2. Payload Table\n",
    "3. Drifted Transactions Table\n",
    "4. Explanations Queue Table\n",
    "5. Explanations Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "\n",
    "# Provide the path to the configuration file.\n",
    "configuration_archive = \"./archives/configuration_archive.tar.gz\"\n",
    "\n",
    "config_json = None\n",
    "with tarfile.open(configuration_archive, 'r:gz') as tar:\n",
    "    if \"common_configuration.json\" not in tar.getnames():\n",
    "        raise Exception(\"common_configuration.json file is missing in archive file\")\n",
    "\n",
    "    json_content = tar.extractfile('common_configuration.json')\n",
    "    data = json_content.read().decode()\n",
    "    config_json = json.loads(data)\n",
    "    \n",
    "# print(config_json)\n",
    "\n",
    "# Optional Input: Keep an identifiable name. This id is used to append to various table creation DDLs.\n",
    "# A random UUID is used if this is not present.\n",
    "# NOTEBOOK_RUN_ID = \"some_identifiable_name\"\n",
    "NOTEBOOK_RUN_ID = None\n",
    "\n",
    "# The column to help Spark read and write data using multiple workers in your JDBC storage.\n",
    "# This will help improve the performance of your Spark jobs. \n",
    "# The default value is set to `wos_partition_column`. \n",
    "# Included in CREATE TABLE DDLs and ALTER TABLE DDLs for your data source. \n",
    "# This column will not be used for computation purposes.\n",
    "\n",
    "# Note: Please be careful when choosing an existing feature column as partition column. \n",
    "# If data in this feature column is not properly divided across various possible values, \n",
    "# it could lead to data-skew problem with Spark computation. \n",
    "# Which means, majority of data is sent to one worker for computation - leading to wastage \n",
    "# of compute resources and increased computation time. It is recommended to use a column \n",
    "# with monotonically increasing value as partition column.\n",
    "\n",
    "PARTITION_COLUMN = \"<to_be_edited>\"\n",
    "\n",
    "from ibm_wos_utils.joblib.utils.ddl_utils_db2 import generate_feedback_table_ddl\n",
    "from ibm_wos_utils.joblib.utils.ddl_utils_db2 import generate_payload_table_ddl\n",
    "from ibm_wos_utils.joblib.utils.ddl_utils_db2 import generate_drift_table_ddl\n",
    "from ibm_wos_utils.joblib.utils.ddl_utils_db2 import generate_explanations_table_ddl\n",
    "\n",
    "# Schema Name where tables should be created.\n",
    "SCHEMA_NAME = \"<to_be_edited>\"\n",
    "\n",
    "# FEEDBACK TABLE DDL\n",
    "#######################\n",
    "print(\"Feedback Table DDLs:\")\n",
    "generate_feedback_table_ddl(\n",
    "    config_json,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_suffix=NOTEBOOK_RUN_ID,\n",
    "    partition_column=PARTITION_COLUMN)\n",
    "print(\"=========================\")\n",
    "\n",
    "# PAYLOAD TABLE DDL\n",
    "#######################\n",
    "print(\"Payload Table DDLs:\")\n",
    "generate_payload_table_ddl(\n",
    "    config_json,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_suffix=NOTEBOOK_RUN_ID,\n",
    "    partition_column=PARTITION_COLUMN)\n",
    "print(\"=========================\")\n",
    "\n",
    "if config_json[\"common_configuration\"][\"enable_drift\"]:\n",
    "    # DRIFTED TRANSACTIONS TABLE DDL\n",
    "    #######################\n",
    "    drift_archive = None\n",
    "    with tarfile.open(configuration_archive, 'r:gz') as tar:\n",
    "        if \"drift_archive.tar.gz\" not in tar.getnames():\n",
    "            raise Exception(\"drift_archive.tar.gz file is missing in archive file\")\n",
    "\n",
    "        drift_archive = tar.extractfile(\"drift_archive.tar.gz\").read()\n",
    "\n",
    "    print(\"Drifted Transactions Table DDLs:\")\n",
    "    generate_drift_table_ddl(\n",
    "        drift_archive,\n",
    "        schema_name=SCHEMA_NAME,\n",
    "        table_suffix=NOTEBOOK_RUN_ID,\n",
    "        partition_column=PARTITION_COLUMN)\n",
    "    print(\"=========================\")\n",
    "\n",
    "\n",
    "if config_json[\"common_configuration\"][\"enable_explainability\"]:\n",
    "    # EXPLAIN TABLES DDL\n",
    "    #######################\n",
    "\n",
    "    # Explain Queue Table - IBM Watson OpenScale will be generating Explanations for \n",
    "    # all the transactions in this table. Alternatively, the payload table created in the \n",
    "    # notebook above can also be used for this purpose.\n",
    "\n",
    "    print(\"Explanations Queue Table DDLs:\")\n",
    "    generate_payload_table_ddl(\n",
    "        config_json,\n",
    "        schema_name=SCHEMA_NAME,\n",
    "        table_prefix=\"explanations_queue\",\n",
    "        table_suffix=NOTEBOOK_RUN_ID,\n",
    "        partition_column=PARTITION_COLUMN)\n",
    "    print(\"=========================\")\n",
    "\n",
    "    print(\"Explanations Table DDLs:\")\n",
    "    generate_explanations_table_ddl(\n",
    "        schema_name=SCHEMA_NAME,\n",
    "        table_suffix=NOTEBOOK_RUN_ID,\n",
    "        partition_column=PARTITION_COLUMN)\n",
    "    print(\"=========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sample scored training data to get feature and categorical columns\n",
    "\n",
    "A sample scoring data is required to infer the schema of the complete data, so the size of the sample should be chosen accordingly. \n",
    "\n",
    "Additionally, the sample scoring data should have the following fields:\n",
    "1. Feature Columns\n",
    "2. Label/Target Column\n",
    "3. Prediction Column (with same data type as the label column)\n",
    "4. Probability Column (an array of model probabilities for all the class labels. Not required for regression models)\n",
    "\n",
    "**STORAGE_FORMAT** : One of [\"csv\", \"parquet\", \"orc\"]\n",
    "\n",
    "**Note:** \n",
    "1. Please select the format in which your training data is stored in Hive. The same format will be used to generate the various CREATE DDLs in this notebook.\n",
    "2. ORC format is not supported for zLinux environments\n",
    "\n",
    "The sample data should be of type `pyspark.sql.dataframe.DataFrame`. The cell below gives samples on:\n",
    "- how to read a CSV file from the local system into a Pyspark Dataframe.\n",
    "- how to read parquet files in a directory from the local system into a Pyspark Dataframe.\n",
    "- how to read orc files in a directory from the local system into a Pyspark Dataframe. [Not supported for zLinux environments]\n",
    "\n",
    "It is important that the same storage format is chosen as the training data, otherwise there could be schema mismatches.\n",
    "\n",
    "#### Specify the Model Type\n",
    "\n",
    "- Specify **binary** if the model is a binary classifier.\n",
    "- Specify **multiclass** if the model is a multi-class classifier.\n",
    "- Specify **regression** if the model is a regressor.\n",
    "\n",
    "#### Provide Column Details \n",
    "\n",
    "To proceed with this notebook, the following information is required.:\n",
    "\n",
    "- **LABEL_COLUMN**: The column which contains the target field (also known as label column or the class label).\n",
    "- **PREDICTION_COLUMN**: The column containing the model output. This should be of the same data type as the label column.\n",
    "- **PROBABILITY_COLUMN**: The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models.\n",
    "\n",
    "Based on the sample data and key columns provided above, the notebook will deduce the feature columns and the categorical columns. They will be printed in the output of this cell. If you wish to make changes to them, you can do so in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "# Read sample scoring data and identify feature categorical columns\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Common Configuration Generation\").getOrCreate()\n",
    "\n",
    "STORAGE_FORMAT = \"csv\"\n",
    "# STORAGE_FORMAT = \"parquet\"\n",
    "# STORAGE_FORMAT = \"orc\"\n",
    "\n",
    "if STORAGE_FORMAT == \"csv\":\n",
    "    # Load a csv or a directory containing csv files as PySpark DataFrame\n",
    "    # spark_df = spark.read.csv(\"/path/to/dir/containing/csv/files\", header=True, inferSchema=True)\n",
    "    pass\n",
    "\n",
    "elif STORAGE_FORMAT == \"parquet\":\n",
    "    # Load a directory containing parquet files as PySpark DataFrame\n",
    "    # spark_df = spark.read.parquet(\"/path/to/dir/containing/parquet/files\")\n",
    "    pass\n",
    "    \n",
    "elif STORAGE_FORMAT == \"orc\":\n",
    "    # Load a directory containing orc files as PySpark DataFrame\n",
    "    # spark_df = spark.read.orc(\"/path/to/dir/containing/orc/files\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # Load data from any source which matches the schema of the training data\n",
    "    pass\n",
    "\n",
    "spark_df.printSchema()\n",
    "\n",
    "MODEL_TYPE = \"binary\"\n",
    "# MODEL_TYPE = \"multiclass\"\n",
    "# MODEL_TYPE = \"regression\"\n",
    "\n",
    "LABEL_COLUMN = \"<to_be_edited>\"\n",
    "PREDICTION_COLUMN = \"<to_be_edited>\"\n",
    "PROBABILITY_COLUMN = \"<to_be_edited>\"\n",
    "# [Optional] Provide list of protected attributes i.e non-feature columns present in the data.\n",
    "PROTECTED_ATTRIBUTES = []\n",
    "\n",
    "feature_columns = spark_df.columns.copy()\n",
    "feature_columns.remove(LABEL_COLUMN)\n",
    "feature_columns.remove(PREDICTION_COLUMN)\n",
    "\n",
    "if MODEL_TYPE != \"regression\":\n",
    "    feature_columns.remove(PROBABILITY_COLUMN)\n",
    "\n",
    "if PROTECTED_ATTRIBUTES:\n",
    "    for protected_attribute in PROTECTED_ATTRIBUTES:\n",
    "        feature_columns.remove(protected_attribute)\n",
    "\n",
    "print(\"Feature Columns : {}\".format(feature_columns))\n",
    "\n",
    "categorical_columns = [f.name for f in spark_df.schema.fields if isinstance(f.dataType, (BooleanType, StringType)) and f.name in feature_columns]\n",
    "print(\"Categorical Columns : {}\".format(categorical_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Scored Training data table\n",
    "\n",
    "Read sample data to figure out feature columns, categorical columns and their datatypes.\n",
    "Using this information, generate DDL for scored training data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils_db2 import generate_scored_training_table_ddl\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import generate_schemas\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import get_max_length_categories\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import validate_config_info\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "# Read sample scoring data\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Read sample data and generate training data ddl\").getOrCreate()\n",
    "\n",
    "STORAGE_FORMAT = \"csv\"\n",
    "# STORAGE_FORMAT = \"parquet\"\n",
    "# STORAGE_FORMAT = \"orc\"\n",
    "\n",
    "if STORAGE_FORMAT == \"csv\":\n",
    "    # Load a csv or a directory containing csv files as PySpark DataFrame\n",
    "    # spark_df = spark.read.csv(\"/path/to/dir/containing/csv/files\", header=True, inferSchema=True)\n",
    "    pass\n",
    "\n",
    "elif STORAGE_FORMAT == \"parquet\":\n",
    "    # Load a directory containing parquet files as PySpark DataFrame\n",
    "    # spark_df = spark.read.parquet(\"/path/to/dir/containing/parquet/files\")\n",
    "    pass\n",
    "    \n",
    "elif STORAGE_FORMAT == \"orc\":\n",
    "    # Load a directory containing orc files as PySpark DataFrame\n",
    "    # spark_df = spark.read.orc(\"/path/to/dir/containing/orc/files\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # Load data from any source which matches the schema of the training data\n",
    "    pass\n",
    "\n",
    "# model details\n",
    "MODEL_TYPE = \"binary\"\n",
    "# MODEL_TYPE = \"multiclass\"\n",
    "# MODEL_TYPE = \"regression\"\n",
    "\n",
    "LABEL_COLUMN = \"<to_be_edited>\"\n",
    "PREDICTION_COLUMN = \"<to_be_edited>\"\n",
    "PROBABILITY_COLUMN = \"<to_be_edited>\"\n",
    "# [Optional] Provide list of protected attributes i.e non-feature columns present in the data.\n",
    "PROTECTED_ATTRIBUTES = []\n",
    "\n",
    "feature_columns = spark_df.columns.copy()\n",
    "feature_columns.remove(LABEL_COLUMN)\n",
    "feature_columns.remove(PREDICTION_COLUMN)\n",
    "\n",
    "if MODEL_TYPE != \"regression\":\n",
    "    feature_columns.remove(PROBABILITY_COLUMN)\n",
    "\n",
    "if PROTECTED_ATTRIBUTES:\n",
    "    for protected_attribute in PROTECTED_ATTRIBUTES:\n",
    "        feature_columns.remove(protected_attribute)\n",
    "\n",
    "print(\"Feature Columns : {}\".format(feature_columns))\n",
    "\n",
    "categorical_columns = [f.name for f in spark_df.schema.fields if isinstance(f.dataType, (BooleanType, StringType)) and f.name in feature_columns]\n",
    "print(\"Categorical Columns : {}\".format(categorical_columns))\n",
    "\n",
    "config_info = {\n",
    "    \"problem_type\": MODEL_TYPE,\n",
    "    \"label_column\": LABEL_COLUMN,\n",
    "    \"prediction\": PREDICTION_COLUMN,\n",
    "    \"probability\": PROBABILITY_COLUMN\n",
    "}\n",
    "\n",
    "config_info[\"feature_columns\"] = feature_columns\n",
    "config_info[\"categorical_columns\"] = categorical_columns\n",
    "config_info[\"protected_attributes\"] = PROTECTED_ATTRIBUTES\n",
    "\n",
    "# validation\n",
    "validate_config_info(config_info)\n",
    "\n",
    "# generate schema json using columns and their datatypes\n",
    "cmn_config_json = {\n",
    "    \"common_configuration\": generate_schemas(spark_df, config_info.copy())\n",
    "}\n",
    "\n",
    "# get length of values in different columns\n",
    "max_length_categories = get_max_length_categories(spark_df)\n",
    "\n",
    "# generate ddl using schema json\n",
    "# Schema Name where Scored Training Table should be created.\n",
    "SCORED_TRAINING_SCHEMA_NAME = \"<to_be_edited>\"\n",
    "\n",
    "# The column to help Spark read and write data using multiple workers in your JDBC storage.\n",
    "# This will help improve the performance of your Spark jobs.\n",
    "# Included in CREATE TABLE DDLs and ALTER TABLE DDLs for your data source. \n",
    "# This column will not be used for computation purposes.\n",
    "\n",
    "# Note: Please be careful when choosing an existing feature column as partition column. \n",
    "# If data in this feature column is not properly divided across various possible values, \n",
    "# it could lead to data-skew problem with Spark computation. \n",
    "# Which means, majority of data is sent to one worker for computation - leading to wastage \n",
    "# of compute resources and increased computation time. It is recommended to use a column \n",
    "# with monotonically increasing value as partition column.\n",
    "\n",
    "PARTITION_COLUMN = \"<to_be_edited>\"\n",
    "\n",
    "NOTEBOOK_RUN_ID = \"<to_be_edited>\" #optional\n",
    "\n",
    "scored_training_data_create_table_ddl = generate_scored_training_table_ddl(\n",
    "    cmn_config_json,\n",
    "    schema_name=SCORED_TRAINING_SCHEMA_NAME,\n",
    "    table_suffix=NOTEBOOK_RUN_ID,\n",
    "    max_length_categories=max_length_categories,\n",
    "    partition_column=PARTITION_COLUMN)\n",
    "\n",
    "print(\"Scored Training Data Table DDLs:\")\n",
    "print(scored_training_data_create_table_ddl)\n",
    "print(\"=========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "37b82c9850f337b3f8e26ef3a35bf87c9f2cfa1e4ad2c96ec00819afd6ebf7e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
