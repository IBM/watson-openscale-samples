{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Watson OpenScale and monitoring models with data in remote Hive  location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook must be run in the Python 3.10 runtime environment. It requires Watson OpenScale service credentials.\n",
    "\n",
    "The notebook demonstrates how to onboard a model (which stores its runtime data in a remote Hive table) for monitoring in IBM Watson OpenScale. Use the notebook to enable quality, drift, fairness and explainability monitoring. Before you can run the notebook, you must have the following resources:\n",
    "\n",
    "1. The configuration package (archive) containing common configuration JSON, drift archive and explainability archive generated by using the [common configuration notebook](https://github.com/IBM/watson-openscale-samples/blob/main/Cloud%20Pak%20for%20Data/Batch%20Support/Configuration%20generation%20for%20OpenScale%20batch%20subscription.ipynb).\n",
    "2. Feedback, payload, drifted transactions, explanations queue and result tables details (either existing or to be created) in Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Provide Spark Compute Engine Details](#spark)\n",
    "2. [Provide Storage Details](#backend-storage)\n",
    "3. [Provide Table Details](#table-details)\n",
    "4. [Connect to IBM Watson OpenScale Instance](#connect-openscale)\n",
    "5. [Connect service provider in IBM Watson OpenScale Instance](#create-service-provider)\n",
    "6. [Onboard model for monitoring in IBM Watson OpenScale Instance](#create-subscription)\n",
    "7. [Enable services to monitor model](#enable-monitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>\n",
    "\n",
    "### Installing Required Libraries\n",
    "\n",
    "First import some of the packages you need to use. After you finish installing the following software packages, restart the kernel.\n",
    "\n",
    "### Import configuration archive/package\n",
    "\n",
    "Configuration archive/package created using configuration notebook will be required to onboard model for monitoring in IBM Watson OpenScale. Provide path location of archive here.\n",
    "\n",
    "Please note if you are executing this notebook in IBM Watson Studio, first upload the configuration archive/package to project and use provided code snippet to download it to local directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Note: Restart kernel after the dependencies are installed\n",
    "!pip install --upgrade ibm-watson-openscale\n",
    "!pip install \"ibm_wos_utils>=4.8.0\"\n",
    "\n",
    "# # Download \"configuration_archive.tar.gz\" from project to local directory\n",
    "# from ibm_watson_studio_lib import access_project_or_space\n",
    "# wslib = access_project_or_space()\n",
    "# wslib.download_file(\"configuration_archive.tar.gz\")\n",
    "\n",
    "archive_file_path = \"configuration_archive.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Spark Connection Details <a name=\"spark\"></a>\n",
    "\n",
    "To generate configuration for monitoring models in IBM Watson OpenScale, a spark compute engine is required. It can be either IBM Analytics Engine or your own Spark Cluster. Provide details of any one of them in this section.\n",
    "\n",
    "Please note, if you are using your own Spark cluster, checkout IBM Watson OpenScale documentation on how to setup spark manager API to enable interface for use with IBM Watson OpenScale services.\n",
    "\n",
    "### Parameters for IBM Analytics Engine\n",
    "If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| display_name | Display Name of the Spark instance in IBM Analytics Engine | |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For IBM IAE, this must be set to `cpd_iae`. | `cpd_iae` |\n",
    "| endpoint | Spark Jobs Endpoint for IBM Analytics Engine | |\n",
    "| volume | IBM Cloud Pak for Data storage volume name | |\n",
    "| username | IBM Cloud Pak for Data username | |\n",
    "| apikey | IBM Cloud Pak for Data API key | |\n",
    "\n",
    "### Parameters for Remote Spark Cluster\n",
    "If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For Remote Spark, this must be set to `custom`. | `custom` |\n",
    "| endpoint | Endpoint URL where the Spark Manager Application is running | |\n",
    "| username | Username to connect to Spark Manager Application | |\n",
    "| password | Password to connect to Spark Manager Application | |\n",
    "\n",
    "\n",
    "### Provide Spark Resource Settings [Optional]\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `{}` if no customisation is required.\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| max_num_executors | Maximum Number of executors to launch for this session |\n",
    "| min_executors | Minimum Number of executors to launch for this session |\n",
    "| executor_cores | Number of cores to use for each executor |\n",
    "| executor_memory | Amount of memory (in GBs) to use per executor process |\n",
    "| driver_cores | Number of cores to use for the driver process |\n",
    "| driver_memory | Amount of memory (in GBs) to use for the driver process |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_connection_info = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": \"<to_be_edited>\",\n",
    "        \"location_type\": \"<to_be_edited>\",\n",
    "        \"display_name\": \"<to_be_edited>\",\n",
    "        \"volume\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": \"<to_be_edited>\",\n",
    "        \"password\": \"<to_be_edited>\",\n",
    "        \"apikey\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": \"2\",\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": \"1\",\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": \"2\",\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": \"2\",\n",
    "    \n",
    "    # driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": \"2\",\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": \"1\"\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = {}\n",
    "\n",
    "spark_connection_info[\"spark_settings\"] = spark_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Backend Storage Details <a name=\"backend-storage\"></a>\n",
    "\n",
    "IBM Watson OpenScale services monitors models by analyzing runtime data, i.e., the data model is making predictions on. To do this analysis, most of the services require access to this runtime data (also called payload data). In addition, some of the services may require access to manually labelled runtime data (also called feedback data). Hence, user needs to store such data in some backend storage and connect this storage to IBM Watson OpenScale.\n",
    "\n",
    "### Provide Hive database connection details\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| type | Describes the type of storage being used. For hive, this must be set to `hive`. | `hive` |\n",
    "| metastore_url | An optional string value specifying hive metastore url. Example: `thrift://localhost:9083` | |\n",
    "| location_type | Identifies the type of location for connection to use. For hive, this must be set to `metastore`. | `metastore` |\n",
    "\n",
    "#### Provide additional details related Hadoop delegation token if the Hive is Kerberos secured and Spark in IBM Analytics Engine is used [Optional]\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| kerberos_principal | The kerberos principal used to generate the delegation token. | |\n",
    "| delegation_token_urn | The secret_urn of the CP4D vault where the delegation token is stored. | |\n",
    "| delegation_token_endpoint | The REST endpoint which generates and returns the delegation token. | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datawarehouse_details = {\n",
    "    \"type\": \"hive\",\n",
    "    \"connection\": {\n",
    "        \"location_type\": \"metastore\",\n",
    "        \"metastore_url\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"credentials\": {}\n",
    "}\n",
    "\n",
    "# Flag to indicate if the Hive is secured with Kerberos and Spark in IAE is used\n",
    "kerberos_enabled = False\n",
    "\n",
    "# Provide Hadoop delegation token details if kerberos_enabled is True\n",
    "# Provide either secret_urn of the CP4D vault OR the delegation token endpoint. One of the two fields is mandatory to fetch the delegation token.\n",
    "kerberos_principal = \"<to_be_edited>\"\n",
    "delegation_token_urn = \"<to_be_edited>\"\n",
    "delegation_token_endpoint = \"<to_be_edited>\"\n",
    "\n",
    "if kerberos_enabled is True:\n",
    "    datawarehouse_details[\"connection\"][\"kerberos_enabled\"] = True\n",
    "    datawarehouse_details[\"credentials\"][\"kerberos_principal\"] = kerberos_principal\n",
    "    if delegation_token_urn:\n",
    "        datawarehouse_details[\"credentials\"][\"delegation_token_urn\"] = delegation_token_urn\n",
    "    if delegation_token_endpoint:\n",
    "        datawarehouse_details[\"credentials\"][\"delegation_token_endpoint\"] = delegation_token_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide details of different tables <a name=\"table-details\"></a>\n",
    "\n",
    "IBM Watson OpenScale services require different tables to perform their analysis. Depending on which services you have enabled, provide details of the corresponding tables.\n",
    "Tables are:\n",
    "\n",
    "| Table | Description |\n",
    "| :- | :- |\n",
    "| Payload Table | Hosts the runtime data predicted by model. Required for detecting fairness and drift in runtime data. |\n",
    "| Feedback Table | Hosts the manually labelled runtime data (also called feedback data) predicted by model. Required for tracking quality of monitor by analyzing feedback data. |\n",
    "| Drifted Transactions Table | Hosts the data identified to be drifted.|\n",
    "| Explain Queue Table | Hosts the data for which explanations are required to be generated. This can be same as payload table.|\n",
    "| Explain Results Table | Hosts the explanations generated for records in explain queue table. |\n",
    "\n",
    "For each of the table, following information is required:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| database | Name of the database hosting the schema. | |\n",
    "| schema | Name of the schema hosting the table. | |\n",
    "| table | Name of the table. | |\n",
    "| auto_create | Boolean value identifying if the table already exists or has to be created via IBM Watson OpenScale. | `True` or `False`|\n",
    "| hive_storage_format | Storage format to use for data in tables. Used only when tables are created using IBM Watson OpenScale. | `csv`, `parquet`, `orc` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_NAME=\"<to_be_edited>\"\n",
    "SCHEMA_NAME=\"<to_be_edited>\"\n",
    "\n",
    "# Payload table information\n",
    "payload_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Feedback table information\n",
    "feedback_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Drifted Transaction table. \n",
    "#Set this table information if drift is enabled\n",
    "drifted_transaction_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{}\n",
    "}\n",
    "\n",
    "#Explanation Result table\n",
    "#Set this table information if Explain is enabled\n",
    "explain_result_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Explanation Queue table\n",
    "#Set this table information if Explain is enabled\n",
    "explain_queue_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"hive_storage_format\": \"<to_be_edited>\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM Watson OpenScale instance <a name=\"connect-openscale\"></a>\n",
    "\n",
    "Following information is required to connect to IBM Watson OpenScale instance:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| url | Base url of your Cloud Pak for Data cluster hosting IBM Watson OpenScale instance. |\n",
    "| username | Username to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. |\n",
    "| password | Password to connect to your IBM Watson OpenScale instance in  Cloud Pak for Data cluster. One of `password` or `api_key` must be provided. |\n",
    "| api_key | API Key to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. One of `password` or `api_key` must be provided. |\n",
    "| service_instance_id | Id of your IBM Watson OpenScale Instance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import APIClient\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "service_instance_id = \"<SERVICE_INSTANCE_ID>\" #Default is 00000000-0000-0000-0000-000000000000\n",
    "service_credentials = {\n",
    "    \"url\": \"<to_be_edited>\",\n",
    "    \"username\": \"<to_be_edited>\",\n",
    "    \"password\": \"<to_be_edited>\",\n",
    "#     \"apikey\":\"<to_be_edited>\"\n",
    "}\n",
    "\n",
    "authenticator = CloudPakForDataAuthenticator(\n",
    "    url=service_credentials['url'],\n",
    "    username=service_credentials['username'],\n",
    "    password=service_credentials['password'],\n",
    "#     apikey=service_credentials['apikey'],\n",
    "    disable_ssl_verification=True\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    service_url=service_credentials['url'],\n",
    "    service_instance_id=service_instance_id,\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Machine Learning Provider in IBM Watson OpenScale instance <a name=\"create-service-provider\"></a>\n",
    "\n",
    "Before configuring model for monitoring in IBM Watson OpenScale, you need to connect your machine learning provider with IBM Watson OpenScale instance. Since, we are configuring a model for monitoring which has its runtime data located remotely to IBM Watson OpenScale, we'll create a custom machine learning provider in given instance.\n",
    "\n",
    "Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| name | Name of the machine learning provider being configured. This can be any string value. |\n",
    "| description | Description for the machine learning provider being configured. |\n",
    "| service_type | Identifies type of the machine learning provider. In this case, this value must be `ServiceTypes.CUSTOM_MACHINE_LEARNING` |\n",
    "| credentials | Optional input, stores username and password to connect to machine learning provider. |\n",
    "| operational_space_id | Defines the classification of machine learning provider. Possible values are `pre-production` and `production`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Delete existing service provider with the same name as provided\n",
    "\n",
    "# SERVICE_PROVIDER_NAME = \"<to_be_edited>\"\n",
    "# service_providers = client.service_providers.list().result.service_providers\n",
    "# for provider in service_providers:\n",
    "#     if provider.entity.name == SERVICE_PROVIDER_NAME:\n",
    "#         client.service_providers.delete(service_provider_id=provider.metadata.id)\n",
    "#         break\n",
    "\n",
    "# Add Service Provider\n",
    "from ibm_watson_openscale.supporting_classes.enums import ServiceTypes\n",
    "\n",
    "added_service_provider_result = client.service_providers.add(\n",
    "        name=\"<to_be_edited>\",\n",
    "        description=\"<to_be_edited>\",\n",
    "        service_type=ServiceTypes.CUSTOM_MACHINE_LEARNING,\n",
    "        credentials={},\n",
    "        operational_space_id=\"<to_be_edited>\",\n",
    "        background_mode=False\n",
    "    ).result\n",
    "\n",
    "service_provider_id = added_service_provider_result.metadata.id\n",
    "\n",
    "client.service_providers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onboard model for monitoring in IBM Watson OpenScale instance <a name=\"create-subscription\"></a>\n",
    "\n",
    "When you configure a model for monitoring in IBM Watson OpenScale instance, a corresponding subscription is created for this model. Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| subscription_name | Name of the subscription to use. This can be any string value typically identifying model being monitored. |\n",
    "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
    "| service_provider_id | Id of the machine learning provider instance created in IBM Watson OpenScale. |\n",
    "| configuration_archive | Path to configuration package archive. |\n",
    "| spark_credentials | Connection details of Spark compute engine to use for analysis by different IBM Watson OpenScale services. |\n",
    "| data_warehouse_connection | Details of the backend storage hosting tables for data, feedback data, etc. |\n",
    "| payload_table | Details of the payload table to be used with this subscription. |\n",
    "| feedback_table | Details of the feedback table to be used with this subscription. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to the configuration file. If executing in IBM Watson Studio then leave as it is\n",
    "subscription_id = client.subscriptions.create_subscription(\n",
    "    subscription_name=\"My SDK Batch Subscription-hive\",\n",
    "    datamart_id=service_instance_id,\n",
    "    service_provider_id=service_provider_id,\n",
    "    configuration_archive=archive_file_path,\n",
    "    spark_credentials=spark_connection_info,\n",
    "    data_warehouse_connection=datawarehouse_details,\n",
    "    payload_table=payload_table,\n",
    "    feedback_table=feedback_table\n",
    ")\n",
    "\n",
    "# print(\"Subscription id is {}\".format(subscription_id))\n",
    "\n",
    "# Wait for the subscription to get in active state and to create the \n",
    "# required tables in the background before moving onto enabling monitors\n",
    "\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "\n",
    "# subscription_status = None\n",
    "# while subscription_status not in (\"active\", \"error\"):\n",
    "#     subscription_status = client.subscriptions.get(subscription_id).result.entity.status.state\n",
    "#     if subscription_status not in (\"active\", \"error\"):\n",
    "#         print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)\n",
    "#         time.sleep(15)\n",
    "        \n",
    "# print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable different services to monitor model <a name=\"enable-monitors\"></a>\n",
    "\n",
    "Depending on the services enabled in configuration package and their corresponding artefacts availability, different services are enabled in given subscription. There services are called monitors.\n",
    "\n",
    "Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
    "| service_provider_id | Id of the machine learning provider instance created in IBM Watson OpenScale. |\n",
    "| subscription_id | Id of the subscription created for given model in IBM Watson OpenScale instance. |\n",
    "| configuration_archive | Path to configuration package archive. |\n",
    "| drifted_transaction_table | Details of the drifted transactions table to be used with this subscription. |\n",
    "| explain_queue_table | Details of the explain queue table to be used with this subscription. |\n",
    "| explain_results_table | Details of the explain results table to be used with this subscription. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_ids = client.monitor_instances.enable_monitors(\n",
    "    datamart_id=service_instance_id,\n",
    "    service_provider_id=service_provider_id,\n",
    "    subscription_id=subscription_id,\n",
    "    configuration_archive=archive_file_path,\n",
    "    drifted_transaction_table=drifted_transaction_table,\n",
    "    explain_queue_table=explain_queue_table,\n",
    "    explain_results_table=explain_result_table\n",
    ")\n",
    "\n",
    "print(instance_ids)\n",
    "\n",
    "## Track each monitor instance status\n",
    "# for key, value in instance_ids.items():\n",
    "#     monitor_instance_status = None\n",
    "\n",
    "#     while monitor_instance_status not in (\"active\", \"error\"):\n",
    "#         monitor_instance_details = client.monitor_instances.get(monitor_instance_id=value).result\n",
    "#         monitor_instance_status = monitor_instance_details.entity.status.state\n",
    "#         if monitor_instance_status not in (\"active\", \"error\"):\n",
    "#             print(datetime.now().strftime(\"%H:%M:%S\"), monitor_instance_status)\n",
    "#             time.sleep(30)\n",
    "\n",
    "#     print(key, monitor_instance_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "All the monitors have been enabled. It will take some time for monitors to get into active state. You can track the status of each monitor by using above code snippet.\n",
    "\n",
    "Once, all monitors are active, load data into payload or feedback table and either run on-demand evaluations or wait for scheduled evaluations to complete for each monitor. You can check more details in [IBM Watson OpenScale Dashboard](https://url-to-your-cp4d-cluster/aiopenscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup subscription and its related artefacts\n",
    "Crawls through subscription json and identifies entities to be deleted. Currently, following entities are identified and deleted:\n",
    "- Analytics Engine integrated system\n",
    "- Data Warehouse Connection integrated system(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and update following if you are running this at a later point of time or \n",
    "# # separate from this notebook with no subscription id and wos client session\n",
    "\n",
    "# from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "# from ibm_watson_openscale import APIClient\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# service_instance_id = \"<SERVICE_INSTANCE_ID>\" #Default is 00000000-0000-0000-0000-000000000000\n",
    "# service_credentials = {\n",
    "#     \"url\": \"<to_be_edited>\",\n",
    "#     \"username\": \"<to_be_edited>\",\n",
    "#     \"password\": \"<to_be_edited>\",\n",
    "# #     \"apikey\":\"<to_be_edited>\"\n",
    "# }\n",
    "\n",
    "# authenticator = CloudPakForDataAuthenticator(\n",
    "#     url=service_credentials['url'],\n",
    "#     username=service_credentials['username'],\n",
    "#     password=service_credentials['password'],\n",
    "# #     apikey=service_credentials['apikey'],\n",
    "#     disable_ssl_verification=True\n",
    "# )\n",
    "\n",
    "# client = APIClient(\n",
    "#     service_url=service_credentials['url'],\n",
    "#     service_instance_id=service_instance_id,\n",
    "#     authenticator=authenticator\n",
    "# )\n",
    "\n",
    "# print(client.version)\n",
    "\n",
    "# subscription_id = \"<to_be_edited>\"\n",
    "\n",
    "subscription_details = client.subscriptions.get(\n",
    "    subscription_id=subscription_id).result.to_dict()\n",
    "subscription_entity = subscription_details.get(\"entity\", {})\n",
    "\n",
    "integrated_systems_id = []\n",
    "\n",
    "# add analytics engine integrated system id\n",
    "analytics_engine = subscription_entity.get(\"analytics_engine\", {})\n",
    "if analytics_engine and analytics_engine.get(\"integrated_system_id\"):\n",
    "    print(\"Found integrated system for analytics engine with type: {}\".format(\n",
    "        analytics_engine.get(\"type\")))\n",
    "    integrated_systems_id.append(analytics_engine.get(\"integrated_system_id\"))\n",
    "\n",
    "# add data source integrated system ids\n",
    "data_sources = subscription_entity.get(\"data_sources\", [])\n",
    "for data_source in data_sources:\n",
    "    if not data_source.get(\"connection\"):\n",
    "        continue\n",
    "\n",
    "    if not data_source.get(\"connection\").get(\"integrated_system_id\"):\n",
    "        continue\n",
    "\n",
    "    integrated_system_id = data_source.get(\"connection\").get(\"integrated_system_id\")\n",
    "    if integrated_system_id in integrated_systems_id:\n",
    "        continue\n",
    "\n",
    "    print(\"Found integrated system for data source with type: {}\".format(\n",
    "        data_source.get(\"type\")))\n",
    "    integrated_systems_id.append(integrated_system_id)\n",
    "    \n",
    "print(\"Integrated Systems to delete: {}\".format(integrated_systems_id))\n",
    "    \n",
    "# delete subscription\n",
    "client.subscriptions.delete(\n",
    "    subscription_id=subscription_id,\n",
    "    background_mode=False)\n",
    "\n",
    "# wait time for subscription delete to complete\n",
    "import time\n",
    "time.sleep(30)\n",
    "\n",
    "# delete all integrated systems\n",
    "for integrated_system_id in integrated_systems_id:\n",
    "    print(\"Deleting integrated system with id: {}\".format(integrated_system_id))\n",
    "    client.integrated_systems.delete(integrated_system_id)\n",
    "    \n",
    "    # wait time for integrated system delete to complete\n",
    "    time.sleep(10)\n",
    "    \n",
    "print(\"Cleanup Complete!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c04270fa9d24993c68ad3b96eeec6e3073fb63b345ea656e7bfdc6a0dcc03e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
