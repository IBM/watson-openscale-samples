{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating configuration for batch subscriptions in IBM Watson OpenScale\n",
    "\n",
    "Use this notebook to generate the following artefacts:\n",
    "1. Common configuration JSON needed to configure an IBM Watson OpenScale subscription.\n",
    "2. Drift Configuration Archive\n",
    "3. DDLs for creating Feedback, Payload and Drifted Transactions tables\n",
    "\n",
    "You must provide the necessary inputs (where marked) and download the generated artefacts, which you then must upload to the IBM Watson OpenScale UI. \n",
    "\n",
    "**Note**: This notebook can only generate artefacts for one model at a time. For multiple models, this notebook needs to be run for each model separately.\n",
    "\n",
    "**Contents:**\n",
    "1. [Installing Dependencies](#Installing-Dependencies)\n",
    "2. [Select IBM Watson OpenScale Services](#Select-IBM-Watson-OpenScale-Services)\n",
    "3. [Read sample scoring data](#Read-sample-scoring-data)\n",
    "4. [Specify Model Inputs](#Specify-Model-Inputs)\n",
    "5. [Generate Common Configuration JSON](#Generate-Common-Configuration-JSON)\n",
    "6. [Generate DDL for creating Scored Training data table](#Generate-DDL-for-creating-Scored-Training-data-table)\n",
    "6. [Generate DDL for creating Feedback table](#Generate-DDL-for-creating-Feedback-table)\n",
    "7. [Generate DDL for creating Payload table](#Generate-DDL-for-creating-Payload-table)\n",
    "8. [Provide Spark Connection Details](#Provide-Spark-Connection-Details)\n",
    "9. [Provide Storage Inputs](#Provide-Storage-Inputs)\n",
    "10. [Provide Spark Resource Settings [Optional]](#Provide-Spark-Resource-Settings-[Optional])\n",
    "11. [Provide Additional Spark Settings [Optional]](#Provide-Additional-Spark-Settings-[Optional])\n",
    "12. [Provide Drift Parameters [Optional]](#Provide-Drift-Parameters-[Optional])\n",
    "13. [Run Drift Configuration Job](#Run-Drift-Configuration-Job)\n",
    "14. [Download Drift Archive](#Download-Drift-Archive)\n",
    "15. [Generate DDL for creating Drifted Transactions Table](#Generate-DDL-for-creating-Drifted-Transactions-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies\n",
    "\n",
    "Let's start by loading some of the required software libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: Restart kernel after the dependencies are installed\n",
    "!pip install pyspark\n",
    "!pip install ibm-cos-sdk-core==2.4.4 requests\n",
    "!pip install --extra-index-url https://test.pypi.org/simple/ --upgrade ibm-wos-utils==2.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select IBM Watson OpenScale Services\n",
    "\n",
    "Because batch processing only supports quality and drift analysis, this notebook only generates configuration information for the two supported services, quality and drift. You can select a subset of the supported services, by setting the following flags:\n",
    "\n",
    "- ENABLE_QUALITY: Flag to allow generation of common configuration details needed if quality alone is selected\n",
    "<!-- - ENABLE_FAIRNESS : Flag to allow generation of fairness specific data distribution needed for configuration -->\n",
    "<!-- - ENABLE_EXPLAIN : Flag to allow generation of explainability specific information -->\n",
    "- ENABLE_MODEL_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Model Drift.\n",
    "- ENABLE_DATA_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Data Drift.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5900-A3Q, 5737-J33\n",
    "# Copyright IBM Corp. 2020\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = 1.0\n",
    "\n",
    "\n",
    "# Optional Input: Keep an identifiable name. This id is used to append to various table creation DDLs.\n",
    "# A random UUID is used if this is not present.\n",
    "# NOTEBOOK_RUN_ID = \"some_identifiable_name\"\n",
    "NOTEBOOK_RUN_ID = None\n",
    "\n",
    "\n",
    "# Service Configuration Flags\n",
    "ENABLE_QUALITY = True\n",
    "ENABLE_MODEL_DRIFT = True\n",
    "ENABLE_DATA_DRIFT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Common Configuration Generation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample scoring data\n",
    "\n",
    "To infer the schema of the complete data, you must supply sample scoring data. You must supply a sample of sufficient size and quality that includes the following fields:\n",
    "\n",
    "1. Feature Columns\n",
    "2. Label/Target Column\n",
    "3. Prediction Column (with same data type as the label column)\n",
    "4. Probability Column (an array of model probabilities for all the class labels. Not required for regression models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORAGE_FORMAT** : One of [\"csv\", \"parquet\", \"orc\"]\n",
    "\n",
    "Select the format of your training data that is stored in Hive. The same format is used to generate the various CREATE DDLs in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_FORMAT = \"csv\"\n",
    "# STORAGE_FORMAT = \"parquet\"\n",
    "# STORAGE_FORMAT = \"orc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The sample data should be of type `pyspark.sql.dataframe.DataFrame`. The following cell shows you how to read from the possible file types:\n",
    "- a CSV file from the local system into a Pyspark Dataframe\n",
    "- parquet files in a directory from the local system into a Pyspark Dataframe\n",
    "- orc files in a directory from the local system into a Pyspark Dataframe\n",
    "\n",
    "It is important that the same storage format is chosen as the training data, otherwise there could be schema mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STORAGE_FORMAT == \"csv\":\n",
    "    # Load a csv or a directory containing csv files as PySpark DataFrame\n",
    "    # spark_df = spark.read.csv(\"/path/to/dir/containing/csv/files\", header=True, inferSchema=True)\n",
    "    pass\n",
    "\n",
    "elif STORAGE_FORMAT == \"parquet\":\n",
    "    # Load a directory containing parquet files as PySpark DataFrame\n",
    "    # spark_df = spark.read.parquet(\"/path/to/dir/containing/parquet/files\")\n",
    "    pass\n",
    "    \n",
    "elif STORAGE_FORMAT == \"orc\":\n",
    "    # Load a directory containing orc files as PySpark DataFrame\n",
    "    # spark_df = spark.read.orc(\"/path/to/dir/containing/orc/files\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # Load data from any source which matches the schema of the training data\n",
    "    pass\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Model Inputs\n",
    "\n",
    "To properly read the data, you must provide some specifics about the model inputs, for example the model type and column details.\n",
    "\n",
    "#### Specify the Model Type\n",
    "\n",
    "Use the following values to specify the type of model:\n",
    "\n",
    "- **binary** if the model is a binary classifier\n",
    "- **multiclass** if the model is a multi-class classifier\n",
    "- **regression** if the model is a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TYPE = \"binary\"\n",
    "MODEL_TYPE = \"multiclass\"\n",
    "# MODEL_TYPE = \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide Column Details \n",
    "\n",
    "To proceed with this notebook, the following information is required.:\n",
    "\n",
    "- **LABEL_COLUMN**: The column which contains the target field (also known as label column or the class label).\n",
    "- **PREDICTION_COLUMN**: The column containing the model output. This should be of the same data type as the label column.\n",
    "- **PROBABILITY_COLUMN**: The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"<label_column>\"\n",
    "PREDICTION_COLUMN = \"<model prediction column>\"\n",
    "PROBABILITY_COLUMN = \"<model probability column. ignored in case of regression models>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample data and key columns, the notebook deduces the feature columns and the categorical columns. They are printed in the output of this cell. If you need to make ajustment to them, you can make changes in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "feature_columns = spark_df.columns.copy()\n",
    "feature_columns.remove(LABEL_COLUMN)\n",
    "feature_columns.remove(PREDICTION_COLUMN)\n",
    "\n",
    "if MODEL_TYPE != \"regression\":\n",
    "    feature_columns.remove(PROBABILITY_COLUMN)\n",
    "\n",
    "print(\"Feature Columns : {}\".format(feature_columns))\n",
    "\n",
    "categorical_columns = [f.name for f in spark_df.schema.fields if isinstance(f.dataType, (BooleanType, StringType)) and f.name in feature_columns]\n",
    "print(\"Categorical Columns : {}\".format(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_info = {\n",
    "    \"problem_type\": MODEL_TYPE,\n",
    "    \"label_column\": LABEL_COLUMN,\n",
    "    \"prediction\": PREDICTION_COLUMN,\n",
    "    \"probability\": PROBABILITY_COLUMN\n",
    "}\n",
    "\n",
    "config_info[\"feature_columns\"] = feature_columns\n",
    "config_info[\"categorical_columns\"] = categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import validate_config_info\n",
    "validate_config_info(config_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Common Configuration JSON\n",
    "\n",
    "IBM Watson OpenScale requires the following additional fields:\n",
    "\n",
    "- `scoring_id` - a unique identifier for each record in your feedback/payload tables  \n",
    "- `scoring_timestamp` - a timestamp field that denotes when a record entered the table. \n",
    "\n",
    "These fields are automatically added in the common configuration JSON. Ensure that these fields are present in the respective tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import generate_schemas\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "\n",
    "common_config = config_info.copy()\n",
    "common_configuration = generate_schemas(spark_df, common_config)\n",
    "\n",
    "config_json = {}\n",
    "config_json[\"common_configuration\"] = common_configuration\n",
    "config_json[\"batch_notebook_version\"] = VERSION\n",
    "\n",
    "create_download_link(config_json, \"config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Scored Training data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_scored_training_table_ddl\n",
    "\n",
    "# Database Name where Scored Training Table should be created. If None or \"\", the default database is used.\n",
    "SCORED_TRAINING_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Scored Training Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the scored training data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "create_ddl = generate_scored_training_table_ddl(config_json, database_name=SCORED_TRAINING_DATABASE_NAME,\\\n",
    "                                         table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                         path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                         table_properties=table_properties)\n",
    "print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Feedback table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_feedback_table_ddl\n",
    "\n",
    "# Database Name where Feedback Table should be created. If None or \"\", the default database is used.\n",
    "FEEDBACK_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Feedback Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the feedback data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_QUALITY:\n",
    "    create_ddl = generate_feedback_table_ddl(config_json, database_name=FEEDBACK_DATABASE_NAME,\\\n",
    "                                             table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                             path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                             table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Payload table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_payload_table_ddl\n",
    "\n",
    "# Database Name where Payload Table should be created. If None or \"\", the default database is used.\n",
    "PAYLOAD_DATABASE_NAME = \"\"\n",
    "\n",
    "# Path to the Payload Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the payload data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    create_ddl = generate_payload_table_ddl(config_json, database_name=PAYLOAD_DATABASE_NAME,\\\n",
    "                                            table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                            path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                            table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Connection Details\n",
    "\n",
    "You must provide the Spark connection details. Choose from the following options:\n",
    "\n",
    "- To run on a Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details into the Credentials Block for Spark in IBM Analytics Engine:\n",
    "    \n",
    "    - **IAE_SPARK_DISPLAY_NAME**: Display Name of the Spark instance in IBM Analytics Engine\n",
    "    - **IAE_SPARK_JOBS_ENDPOINT**: Spark Jobs Endpoint for IBM Analytics Engine\n",
    "    - **IBM_CPD_VOLUME**: IBM Cloud Pak for Data storage volume name\n",
    "    - **IBM_CPD_USERNAME**: IBM Cloud Pak for Data username\n",
    "    - **IBM_CPD_APIKEY**: IBM Cloud Pak for Data API key\n",
    "\n",
    "\n",
    "- To run on a Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details into the Credentials Block for Spark in Remote Hadoop Ecosystem:\n",
    "\n",
    "    - **SPARK_MANAGER_ENDPOINT**: Endpoint URL where the Spark Manager Application is running\n",
    "    - **SPARK_MANAGER_USERNAME**: Username to connect to Spark Manager Application\n",
    "    - **SPARK_MANAGER_PASSWORD**: Password to connect to Spark Manager Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in IBM Analytics Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "IAE_SPARK_DISPLAY_NAME = \"<Display Name of the Spark instance in IBM Analytics Engine>\"\n",
    "IAE_SPARK_JOBS_ENDPOINT = \"<Spark Jobs Endpoint for IBM Analytics Engine>\"\n",
    "IBM_CPD_VOLUME = \"<IBM Cloud Pak for Data storage volume name>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username>\"\n",
    "IBM_CPD_APIKEY = \"<IBM Cloud Pak for Data API key>\"\n",
    "\n",
    "# Credentials Block for Spark in IBM Analytics Engine\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"display_name\": IAE_SPARK_DISPLAY_NAME,\n",
    "        \"endpoint\": IAE_SPARK_JOBS_ENDPOINT,\n",
    "        \"location_type\": SparkType.IAE_SPARK.value,\n",
    "        \"volume\": IBM_CPD_VOLUME\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": IBM_CPD_USERNAME,\n",
    "        \"apikey\": IBM_CPD_APIKEY\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in Remote Hadoop Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "SPARK_MANAGER_ENDPOINT = \"<Endpoint URL where Spark Manager Application is running>\"\n",
    "SPARK_MANAGER_USERNAME = \"<Username to connect to Spark Manager Application>\"\n",
    "SPARK_MANAGER_PASSWORD = \"<Password to connect to Spark Manager Application>\"\n",
    "\n",
    "# Credentials Block for Spark in Remote Hadoop Ecosystem\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": SPARK_MANAGER_ENDPOINT,\n",
    "        \"location_type\": SparkType.REMOTE_SPARK.value\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": SPARK_MANAGER_USERNAME,\n",
    "        \"password\": SPARK_MANAGER_PASSWORD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Storage Inputs\n",
    "\n",
    "You must provide the URI, database name, and training table name for the Apache Hive data warehouse. \n",
    " - **HIVE_METASTORE_URI**: Thrift URI for Hive Metastore to connect to\n",
    " - **TRAINING_DATABASE_NAME**: Name of the Database in Hive that has training table/view\n",
    " - **TRAINING_TABLE_NAME**: Name of the Table in HIve that has the scored training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVE_METASTORE_URI = \"<Thrift URI for Hive Metastore to connect to>\"\n",
    "TRAINING_DATABASE_NAME = \"<Name of the Database in Hive that has training table/view>\"\n",
    "TRAINING_TABLE_NAME = \"<Name of the Table in HIve that has the scored training data>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_details = {\n",
    "    \"type\": \"hive\",\n",
    "    \"connection\": {\n",
    "        \"metastore_url\": HIVE_METASTORE_URI,\n",
    "    }\n",
    "}\n",
    "\n",
    "tables = [\n",
    "    {\n",
    "        \"database\": TRAINING_DATABASE_NAME,\n",
    "        \"table\": TRAINING_TABLE_NAME,\n",
    "        \"type\": \"training\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Resource Settings [Optional]\n",
    "\n",
    "By setting the number of executors, cores, and the amount of memory, you can control how much of your Spark Cluster resources this job consumes. Leave the variable `spark_settings` to `None` or `{}` if no customisation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": 2,\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": 1,\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": 2,\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": 1,\n",
    "    \n",
    "    #driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": 2,\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": 1\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Additional Spark Settings [Optional]\n",
    "\n",
    "Any other Spark property that can be set via **SparkConf**, provide them in the next cell. These properties are sent to the Spark cluster verbatim. Leave the variable `conf` to `None` or `{}` if no additional property is required.\n",
    "\n",
    "- For more information, see [A list of available properties for Spark 2.4.6](https://spark.apache.org/docs/2.4.6/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "conf = {\n",
    "    \"spark.yarn.maxAppAttempts\": 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "conf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Drift Parameters [Optional]\n",
    "\n",
    "Provide the optional drift parameters in the following cell. Leave the variable `drift_parameters` to `None` or `{}` if no additional parameter is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drift_parameters = {\n",
    "    \"model_drift\": {\n",
    "        # enable_drift_model_tuning - Controls whether there will be Hyper-Parameter \n",
    "        # Optimisation in the Drift Detection Model. Default: False\n",
    "        \"enable_drift_model_tuning\": True,\n",
    "        \n",
    "        # max_bins - Specify the maximum number of categories in categorical columns.\n",
    "        # Default: OpenScale will determine an approximate value. Use this only in cases\n",
    "        # where OpenScale approximation fails.\n",
    "        \"max_bins\": 10,\n",
    "    },\n",
    "    \"data_drift\": {\n",
    "        # enable_two_col_learner - Enable learning of data constraints on two column \n",
    "        # combinations. Default: True\n",
    "        \"enable_two_col_learner\": True,\n",
    "        \n",
    "        # categorical_unique_threshold - Used to discard categorical columns with a\n",
    "        # large number of unique values relative to total rows in the column.\n",
    "        # Should be between 0 and 1. Default: 0.8\n",
    "        \"categorical_unique_threshold\": 0.7,\n",
    "        \n",
    "        # max_distinct_categories - Used to discard categorical columns with a large\n",
    "        # absolute number of unique categories. Also, used for not learning\n",
    "        # categorical-categorical constraint, if potential combinations of two columns\n",
    "        # are more than this number. Default: 100000\n",
    "        \"max_distinct_categories\": 10000 \n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "drift_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Drift Configuration Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"common_configuration\": common_configuration,\n",
    "    \"enable_data_drift\": ENABLE_DATA_DRIFT,\n",
    "    \"enable_model_drift\": ENABLE_MODEL_DRIFT,\n",
    "    \"drift_parameters\": drift_parameters,\n",
    "    \"monitoring_run_id\": NOTEBOOK_RUN_ID,\n",
    "    \"storage\": storage_details,\n",
    "    \"tables\": tables\n",
    "}\n",
    "\n",
    "job_params = {\n",
    "    \"arguments\": arguments,\n",
    "    \"spark_settings\": spark_settings,\n",
    "    \"dependency_zip\": [],\n",
    "    \"conf\": conf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the Drift configuration job. It will also print the status of the job in the output section. Before running any remaining code cells, wait for the status to be **FINISHED**.\n",
    "\n",
    "A successful job status goes through the following values:\n",
    "1. STARTED\n",
    "2. Model Drift Configuration STARTED\n",
    "3. Data Drift Configuration STARTED\n",
    "    - Data Drift: Summary Stats Calculated\n",
    "    - Data Drift: Column Stats calculated.\n",
    "    - Data Drift: (number/total) CategoricalDistributionConstraint columns processed\n",
    "    - Data Drift: (number/total) NumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CategoricalNumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CatCatDistributionConstraint columns processed\n",
    "4. FINISHED\n",
    "\n",
    "If at anytime there is a failure, you will see a **FAILED** status with an exception trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.clients.engine_client import EngineClient\n",
    "from ibm_wos_utils.drift.batch.jobs.drift_configuration import DriftConfiguration\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import JobStatus\n",
    "\n",
    "client = EngineClient(credentials=credentials)\n",
    "job_response = client.engine.run_job(job_name=\"Drift_Configuration_Job\", job_class=DriftConfiguration,\n",
    "                                     job_args=job_params, background=True)\n",
    "\n",
    "# Print Job Status. \n",
    "JobStatus(client, job_response).print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Drift Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "    \n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    drift_archive = client.engine.get_file(job_response.get(\n",
    "            \"output_file_path\") + \"/drift_configuration\")\n",
    "\n",
    "    with NamedTemporaryFile() as tf:\n",
    "        tf.write(drift_archive)\n",
    "        tf.flush()\n",
    "        drift_archive = spark.sparkContext.sequenceFile(tf.name).collect()[0][1]\n",
    "\n",
    "    display(create_download_link(drift_archive, \"drift\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Drifted Transactions table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_drift_table_ddl\n",
    "\n",
    "# Database Name where Drifted Transactions Table should be created. If None or \"\", the default database is used.\n",
    "DRIFT_DATABASE_NAME = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    print(generate_drift_table_ddl(drift_archive, database_name=DRIFT_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}