{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating configuration for batch subscriptions in IBM Watson OpenScale in IBM Cloud Pak for Data v3.5\n",
    "\n",
    "This notebook shows how to generate the following artefacts:\n",
    "1. Common configuration JSON needed to configure an IBM Watson OpenScale subscription.\n",
    "2. Drift Configuration Archive\n",
    "3. DDLs for creating Feedback, Payload and Drifted Transactions tables\n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) and download the generated artefacts. These artefacts \n",
    "have to be then uploaded to IBM Watson OpenScale UI. \n",
    "\n",
    "PS: This notebook can only generate artefacts for one model at a time. For multiple models, this notebook needs to be run for each model separately.\n",
    "\n",
    "**Contents:**\n",
    "1. [Installing Dependencies](#Installing-Dependencies)\n",
    "2. [Select IBM Watson OpenScale Services](#Select-IBM-Watson-OpenScale-Services)\n",
    "3. [Read sample scoring data](#Read-sample-scoring-data)\n",
    "4. [Specify Model Inputs](#Specify-Model-Inputs)\n",
    "5. [Generate Common Configuration JSON](#Generate-Common-Configuration-JSON)\n",
    "6. [Generate DDL for creating Scored Training data table](#Generate-DDL-for-creating-Scored-Training-data-table)\n",
    "6. [Generate DDL for creating Feedback table](#Generate-DDL-for-creating-Feedback-table)\n",
    "7. [Generate DDL for creating Payload table](#Generate-DDL-for-creating-Payload-table)\n",
    "8. [Provide Spark Connection Details](#Provide-Spark-Connection-Details)\n",
    "9. [Provide Storage Inputs](#Provide-Storage-Inputs)\n",
    "10. [Provide Spark Resource Settings [Optional]](#Provide-Spark-Resource-Settings-[Optional])\n",
    "11. [Provide Additional Spark Settings [Optional]](#Provide-Additional-Spark-Settings-[Optional])\n",
    "12. [Provide Drift Parameters [Optional]](#Provide-Drift-Parameters-[Optional])\n",
    "13. [Run Drift Configuration Job](#Run-Drift-Configuration-Job)\n",
    "14. [Download Drift Archive](#Download-Drift-Archive)\n",
    "15. [Generate DDL for creating Drifted Transactions Table](#Generate-DDL-for-creating-Drifted-Transactions-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: Restart kernel after the dependencies are installed\n",
    "!pip install pyspark\n",
    "!pip install ibm-cos-sdk-core==2.4.4 requests\n",
    "!pip install ibm-wos-utils~=2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select IBM Watson OpenScale Services\n",
    "\n",
    "Currently, this notebook has support to generate configuration information related to quality and drift service. \n",
    "\n",
    "Details of the service-specific flags available:\n",
    "\n",
    "- ENABLE_QUALITY: Flag to allow generation of common configuration details needed if quality alone is selected\n",
    "<!-- - ENABLE_FAIRNESS : Flag to allow generation of fairness specific data distribution needed for configuration -->\n",
    "<!-- - ENABLE_EXPLAIN : Flag to allow generation of explainability specific information -->\n",
    "- ENABLE_MODEL_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Model Drift.\n",
    "- ENABLE_DATA_DRIFT: Flag to allow generation of Drift Archive containing relevant information for Data Drift.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5900-A3Q, 5737-J33\n",
    "# Copyright IBM Corp. 2020\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = 1.0\n",
    "\n",
    "\n",
    "# Optional Input: Keep an identifiable name. This id is used to append to various table creation DDLs.\n",
    "# A random UUID is used if this is not present.\n",
    "# NOTEBOOK_RUN_ID = \"some_identifiable_name\"\n",
    "NOTEBOOK_RUN_ID = None\n",
    "\n",
    "\n",
    "# Service Configuration Flags\n",
    "ENABLE_QUALITY = True\n",
    "ENABLE_MODEL_DRIFT = True\n",
    "ENABLE_DATA_DRIFT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Common Configuration Generation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample scoring data\n",
    "\n",
    "A sample scoring data is required to infer the schema of the complete data, so the size of the sample should be chosen accordingly. \n",
    "\n",
    "Additionally, the sample scoring data should have the following fields:\n",
    "1. Feature Columns\n",
    "2. Label/Target Column\n",
    "3. Prediction Column (with same data type as the label column)\n",
    "4. Probability Column (an array of model probabilities for all the class labels. Not required for regression models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORAGE_FORMAT** : One of [\"csv\", \"parquet\", \"orc\"]\n",
    "\n",
    "Note: Please select the format in which your training data is stored in Hive. The same format will be used to generate the various CREATE DDLs in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_FORMAT = \"csv\"\n",
    "# STORAGE_FORMAT = \"parquet\"\n",
    "# STORAGE_FORMAT = \"orc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The sample data should be of type `pyspark.sql.dataframe.DataFrame`. The cell below gives samples on:\n",
    "- how to read a CSV file from the local system into a Pyspark Dataframe.\n",
    "- how to read parquet files in a directory from the local system into a Pyspark Dataframe.\n",
    "- how to read orc files in a directory from the local system into a Pyspark Dataframe.\n",
    "\n",
    "It is important that the same storage format is chosen as the training data, otherwise there could be schema mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STORAGE_FORMAT == \"csv\":\n",
    "    # Load a csv or a directory containing csv files as PySpark DataFrame\n",
    "    # spark_df = spark.read.csv(\"/path/to/dir/containing/csv/files\", header=True, inferSchema=True)\n",
    "    pass\n",
    "\n",
    "elif STORAGE_FORMAT == \"parquet\":\n",
    "    # Load a directory containing parquet files as PySpark DataFrame\n",
    "    # spark_df = spark.read.parquet(\"/path/to/dir/containing/parquet/files\")\n",
    "    pass\n",
    "    \n",
    "elif STORAGE_FORMAT == \"orc\":\n",
    "    # Load a directory containing orc files as PySpark DataFrame\n",
    "    # spark_df = spark.read.orc(\"/path/to/dir/containing/orc/files\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # Load data from any source which matches the schema of the training data\n",
    "    pass\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model Type\n",
    "\n",
    "- Specify **binary** if the model is a binary classifier.\n",
    "- Specify **multiclass** if the model is a multi-class classifier.\n",
    "- Specify **regression** if the model is a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TYPE = \"binary\"\n",
    "MODEL_TYPE = \"multiclass\"\n",
    "# MODEL_TYPE = \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide Column Details \n",
    "\n",
    "To proceed with this notebook, the following information is required.:\n",
    "\n",
    "- **LABEL_COLUMN**: The column which contains the target field (also known as label column or the class label).\n",
    "- **PREDICTION_COLUMN**: The column containing the model output. This should be of the same data type as the label column.\n",
    "- **PROBABILITY_COLUMN**: The column (of type array) containing the model probabilities for all the possible prediction outcomes. This is not required for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"<label_column>\"\n",
    "PREDICTION_COLUMN = \"<model prediction column>\"\n",
    "PROBABILITY_COLUMN = \"<model probability column. ignored in case of regression models>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample data and key columns provided above, the notebook will deduce the feature columns and the categorical columns. They will be printed in the output of this cell. If you wish to make changes to them, you can do so in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "feature_columns = spark_df.columns.copy()\n",
    "feature_columns.remove(LABEL_COLUMN)\n",
    "feature_columns.remove(PREDICTION_COLUMN)\n",
    "\n",
    "if MODEL_TYPE != \"regression\":\n",
    "    feature_columns.remove(PROBABILITY_COLUMN)\n",
    "\n",
    "print(\"Feature Columns : {}\".format(feature_columns))\n",
    "\n",
    "categorical_columns = [f.name for f in spark_df.schema.fields if isinstance(f.dataType, (BooleanType, StringType)) and f.name in feature_columns]\n",
    "print(\"Categorical Columns : {}\".format(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_info = {\n",
    "    \"problem_type\": MODEL_TYPE,\n",
    "    \"label_column\": LABEL_COLUMN,\n",
    "    \"prediction\": PREDICTION_COLUMN,\n",
    "    \"probability\": PROBABILITY_COLUMN\n",
    "}\n",
    "\n",
    "config_info[\"feature_columns\"] = feature_columns\n",
    "config_info[\"categorical_columns\"] = categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import validate_config_info\n",
    "validate_config_info(config_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Common Configuration JSON\n",
    "\n",
    "IBM Watson OpenScale requires two additional fields - a unique identifier for each record in your feedback/payload tables (\"scoring_id\") and a timestamp field (\"scoring_timestamp\") denoting when that record entered the table. These fields are automatically added in the common configuration JSON. \n",
    "\n",
    "Please make sure that these fields are present in the respective tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.notebook_utils import generate_schemas\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "\n",
    "common_config = config_info.copy()\n",
    "common_configuration = generate_schemas(spark_df, common_config)\n",
    "\n",
    "config_json = {}\n",
    "config_json[\"common_configuration\"] = common_configuration\n",
    "config_json[\"batch_notebook_version\"] = VERSION\n",
    "\n",
    "create_download_link(config_json, \"config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Scored Training data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_scored_training_table_ddl\n",
    "\n",
    "# Database Name where Scored Training Table should be created. If None or \"\", the default database is used.\n",
    "SCORED_TRAINING_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Scored Training Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the scored training data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "create_ddl = generate_scored_training_table_ddl(config_json, database_name=SCORED_TRAINING_DATABASE_NAME,\\\n",
    "                                         table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                         path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                         table_properties=table_properties)\n",
    "print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Feedback table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_feedback_table_ddl\n",
    "\n",
    "# Database Name where Feedback Table should be created. If None or \"\", the default database is used.\n",
    "FEEDBACK_DATABASE_NAME = None\n",
    "\n",
    "# Path to the Feedback Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the feedback data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_QUALITY:\n",
    "    create_ddl = generate_feedback_table_ddl(config_json, database_name=FEEDBACK_DATABASE_NAME,\\\n",
    "                                             table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                             path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                             table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Payload table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_payload_table_ddl\n",
    "\n",
    "# Database Name where Payload Table should be created. If None or \"\", the default database is used.\n",
    "PAYLOAD_DATABASE_NAME = \"\"\n",
    "\n",
    "# Path to the Payload Data in HDFS. Leave as None if you wish to load data later.\n",
    "path_to_hdfs_directory = None\n",
    "\n",
    "# Additional Table Properties that are required for table creation.\n",
    "# Please set the table property `skip.header.line.count` as shown \n",
    "# if the payload data is stored as CSV and it contains the header row.\n",
    "# Leave as None if no additional properties are required.\n",
    "# table_properties = {\n",
    "#     \"skip.header.line.count\": 1\n",
    "# }\n",
    "table_properties = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    create_ddl = generate_payload_table_ddl(config_json, database_name=PAYLOAD_DATABASE_NAME,\\\n",
    "                                            table_suffix=NOTEBOOK_RUN_ID, stored_as=STORAGE_FORMAT,\\\n",
    "                                            path_to_hdfs_directory=path_to_hdfs_directory,\\\n",
    "                                            table_properties=table_properties)\n",
    "    print(create_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Connection Details\n",
    "\n",
    "1. If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "    \n",
    "    - **IAE_SPARK_DISPLAY_NAME**: Display Name of the Spark instance in IBM Analytics Engine\n",
    "    - **IAE_SPARK_JOBS_ENDPOINT**: Spark Jobs Endpoint for IBM Analytics Engine\n",
    "    - **IBM_CPD_VOLUME**: IBM Cloud Pak for Data storage volume name\n",
    "    - **IBM_CPD_USERNAME**: IBM Cloud Pak for Data username\n",
    "    - **IBM_CPD_APIKEY**: IBM Cloud Pak for Data API key\n",
    "\n",
    "\n",
    "2. If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "    - **SPARK_MANAGER_ENDPOINT**: Endpoint URL where the Spark Manager Application is running\n",
    "    - **SPARK_MANAGER_USERNAME**: Username to connect to Spark Manager Application\n",
    "    - **SPARK_MANAGER_PASSWORD**: Password to connect to Spark Manager Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in IAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "IAE_SPARK_DISPLAY_NAME = \"<Display Name of the Spark instance in IBM Analytics Engine>\"\n",
    "IAE_SPARK_JOBS_ENDPOINT = \"<Spark Jobs Endpoint for IBM Analytics Engine>\"\n",
    "IBM_CPD_VOLUME = \"<IBM Cloud Pak for Data storage volume name>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username>\"\n",
    "IBM_CPD_APIKEY = \"<IBM Cloud Pak for Data API key>\"\n",
    "\n",
    "# Credentials Block for Spark in IAE\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"display_name\": IAE_SPARK_DISPLAY_NAME,\n",
    "        \"endpoint\": IAE_SPARK_JOBS_ENDPOINT,\n",
    "        \"location_type\": SparkType.IAE_SPARK.value,\n",
    "        \"volume\": IBM_CPD_VOLUME\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": IBM_CPD_USERNAME,\n",
    "        \"apikey\": IBM_CPD_APIKEY\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials Block for Spark in Remote Hadoop Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.constants import SparkType\n",
    "\n",
    "SPARK_MANAGER_ENDPOINT = \"<Endpoint URL where Spark Manager Application is running>\"\n",
    "SPARK_MANAGER_USERNAME = \"<Username to connect to Spark Manager Application>\"\n",
    "SPARK_MANAGER_PASSWORD = \"<Password to connect to Spark Manager Application>\"\n",
    "\n",
    "# Credentials Block for Spark in Remote Hadoop Ecosystem\n",
    "credentials = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": SPARK_MANAGER_ENDPOINT,\n",
    "        \"location_type\": SparkType.REMOTE_SPARK.value\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": SPARK_MANAGER_USERNAME,\n",
    "        \"password\": SPARK_MANAGER_PASSWORD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Storage Inputs\n",
    "\n",
    "Enter Hive details. \n",
    " - **HIVE_METASTORE_URI**: Thrift URI for Hive Metastore to connect to\n",
    " - **TRAINING_DATABASE_NAME**: Name of the Database in Hive that has training table/view\n",
    " - **TRAINING_TABLE_NAME**: Name of the Table in HIve that has the scored training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVE_METASTORE_URI = \"<Thrift URI for Hive Metastore to connect to>\"\n",
    "TRAINING_DATABASE_NAME = \"<Name of the Database in Hive that has training table/view>\"\n",
    "TRAINING_TABLE_NAME = \"<Name of the Table in HIve that has the scored training data>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_details = {\n",
    "    \"type\": \"hive\",\n",
    "    \"connection\": {\n",
    "        \"metastore_url\": HIVE_METASTORE_URI,\n",
    "    }\n",
    "}\n",
    "\n",
    "tables = [\n",
    "    {\n",
    "        \"database\": TRAINING_DATABASE_NAME,\n",
    "        \"table\": TRAINING_TABLE_NAME,\n",
    "        \"type\": \"training\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Spark Resource Settings [Optional]\n",
    "\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `None` or `{}` if no customisation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": 2,\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": 1,\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": 2,\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": 1,\n",
    "    \n",
    "    #driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": 2,\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": 1\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Additional Spark Settings [Optional]\n",
    "\n",
    "Any other Spark property that can be set via **SparkConf**, provide them in the next cell. These properties are sent to the Spark cluster verbatim. Leave the variable `conf` to `None` or `{}` if no additional property is required.\n",
    "\n",
    "- [A list of available properties for Spark 2.4.6](https://spark.apache.org/docs/2.4.6/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "conf = {\n",
    "    \"spark.yarn.maxAppAttempts\": 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "conf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Drift Parameters [Optional]\n",
    "\n",
    "Provide the optional drift parameters in this cell. Leave the variable `drift_parameters` to `None` or `{}` if no additional parameter is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drift_parameters = {\n",
    "    \"model_drift\": {\n",
    "        # enable_drift_model_tuning - Controls whether there will be Hyper-Parameter \n",
    "        # Optimisation in the Drift Detection Model. Default: False\n",
    "        \"enable_drift_model_tuning\": True,\n",
    "        \n",
    "        # max_bins - Specify the maximum number of categories in categorical columns.\n",
    "        # Default: OpenScale will determine an approximate value. Use this only in cases\n",
    "        # where OpenScale approximation fails.\n",
    "        \"max_bins\": 10,\n",
    "    },\n",
    "    \"data_drift\": {\n",
    "        # enable_two_col_learner - Enable learning of data constraints on two column \n",
    "        # combinations. Default: True\n",
    "        \"enable_two_col_learner\": True,\n",
    "        \n",
    "        # categorical_unique_threshold - Used to discard categorical columns with a\n",
    "        # large number of unique values relative to total rows in the column.\n",
    "        # Should be between 0 and 1. Default: 0.8\n",
    "        \"categorical_unique_threshold\": 0.7,\n",
    "        \n",
    "        # max_distinct_categories - Used to discard categorical columns with a large\n",
    "        # absolute number of unique categories. Also, used for not learning\n",
    "        # categorical-categorical constraint, if potential combinations of two columns\n",
    "        # are more than this number. Default: 100000\n",
    "        \"max_distinct_categories\": 10000 \n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "drift_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Drift Configuration Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PROGRESS = True\n",
    "\n",
    "arguments = {\n",
    "    \"common_configuration\": common_configuration,\n",
    "    \"enable_data_drift\": ENABLE_DATA_DRIFT,\n",
    "    \"enable_model_drift\": ENABLE_MODEL_DRIFT,\n",
    "    \"drift_parameters\": drift_parameters,\n",
    "    \"monitoring_run_id\": NOTEBOOK_RUN_ID,\n",
    "    \"storage\": storage_details,\n",
    "    \"tables\": tables,\n",
    "    \"show_progress\": SHOW_PROGRESS\n",
    "}\n",
    "\n",
    "job_params = {\n",
    "    \"arguments\": arguments,\n",
    "    \"spark_settings\": spark_settings,\n",
    "    \"dependency_zip\": [],\n",
    "    \"conf\": conf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `SHOW_PROGRESS` is `True`, the following cell will run the Drift configuration job. It will also print the status of job in the output section. Please wait for the status to be **FINISHED**.\n",
    "\n",
    "A successful job status goes through the following values:\n",
    "1. STARTED\n",
    "2. Model Drift Configuration STARTED\n",
    "3. Data Drift Configuration STARTED\n",
    "    - Data Drift: Summary Stats Calculated\n",
    "    - Data Drift: Column Stats calculated.\n",
    "    - Data Drift: (number/total) CategoricalDistributionConstraint columns processed\n",
    "    - Data Drift: (number/total) NumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CategoricalNumericRangeConstraint columns processed\n",
    "    - Data Drift: (number/total) CatCatDistributionConstraint columns processed\n",
    "4. FINISHED\n",
    "\n",
    "If at anytime there is a failure, you will see a **FAILED** status with an exception trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.clients.engine_client import EngineClient\n",
    "from ibm_wos_utils.drift.batch.jobs.drift_configuration import DriftConfiguration\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import JobStatus\n",
    "\n",
    "client = EngineClient(credentials=credentials)\n",
    "job_response = client.engine.run_job(job_name=\"Drift_Configuration_Job\", job_class=DriftConfiguration,\n",
    "                                     job_args=job_params, background=True)\n",
    "\n",
    "# Print Job Status.\n",
    "if SHOW_PROGRESS:\n",
    "    JobStatus(client, job_response).print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `SHOW_PROGRESS` is `False`, you can run the below cell to check the job status at any point manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SHOW_PROGRESS:\n",
    "    job_id = job_response.get(\"id\")\n",
    "    print(client.engine.get_job_status(job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Drift Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from ibm_wos_utils.joblib.utils.notebook_utils import create_download_link\n",
    "    \n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    drift_archive = client.engine.get_file(job_response.get(\n",
    "            \"output_file_path\") + \"/drift_configuration\")\n",
    "\n",
    "    with NamedTemporaryFile() as tf:\n",
    "        tf.write(drift_archive)\n",
    "        tf.flush()\n",
    "        drift_archive = spark.sparkContext.sequenceFile(tf.name).collect()[0][1]\n",
    "\n",
    "    display(create_download_link(drift_archive, \"drift\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DDL for creating Drifted Transactions table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.ddl_utils import generate_drift_table_ddl\n",
    "\n",
    "# Database Name where Drifted Transactions Table should be created. If None or \"\", the default database is used.\n",
    "DRIFT_DATABASE_NAME = None\n",
    "\n",
    "if ENABLE_MODEL_DRIFT or ENABLE_DATA_DRIFT:\n",
    "    print(generate_drift_table_ddl(drift_archive, database_name=DRIFT_DATABASE_NAME, table_suffix=NOTEBOOK_RUN_ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
