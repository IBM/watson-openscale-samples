{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Watson OpenScale and monitoring models with data in remote DB2  location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook must be run in the Python 3.9 runtime environment. It requires Watson OpenScale service credentials.\n",
    "\n",
    "The notebook demonstrates how to onboard a model (which stores its runtime data in a remote DB2 database) for monitoring in IBM Watson OpenScale. Use the notebook to enable quality, drift, fairness and explainability monitoring. Before you can run the notebook, you must have the following resources:\n",
    "\n",
    "1. The configuration package (archive) containing common configuration JSON, drift archive and explain perturbations archive generated by using the [common configuration notebook](https://github.com/IBM/watson-openscale-samples/blob/main/Cloud%20Pak%20for%20Data/Batch%20Support/Configuration%20generation%20for%20OpenScale%20batch%20subscription.ipynb).\n",
    "2. Feedback, payload, drifted transactions, explanations queue and result tables details (either existing or to be created) in an IBM DB2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Provide Spark Compute Engine Details](#spark)\n",
    "2. [Provide Storage Details](#backend-storage)\n",
    "3. [Provide Table Details](#table-details)\n",
    "4. [Connect to IBM Watson OpenScale Instance](#connect-openscale)\n",
    "5. [Connect service provider in IBM Watson OpenScale Instance](#create-service-provider)\n",
    "6. [Onboard model for monitoring in IBM Watson OpenScale Instance](#create-subscription)\n",
    "7. [Enable services to monitor model](#enable-monitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>\n",
    "\n",
    "### Installing Required Libraries\n",
    "\n",
    "First import some of the packages you need to use. After you finish installing the following software packages, restart the kernel.\n",
    "\n",
    "### Import configuration archive/package\n",
    "\n",
    "Configuration archive/package created using configuration notebook will be required to onboard model for monitoring in IBM Watson OpenScale. Provide path location of archive here.\n",
    "\n",
    "Please note if you are executing this notebook in IBM Watson Studio, first upload the configuration archive/package to project and use provided code snippet to download it to local directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Note: Restart kernel after the dependencies are installed\n",
    "!pip install --upgrade ibm-watson-openscale\n",
    "!pip install ibm_wos_utils>=4.5.0\n",
    "\n",
    "# # Download \"configuration_archive.tar.gz\" from project to local directory\n",
    "# from ibm_watson_studio_lib import access_project_or_space\n",
    "# wslib = access_project_or_space()\n",
    "# wslib.download_file(\"configuration_archive.tar.gz\")\n",
    "\n",
    "archive_file_path = \"configuration_archive.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Spark Connection Details <a name=\"spark\"></a>\n",
    "\n",
    "To generate configuration for monitoring models in IBM Watson OpenScale, a spark compute engine is required. It can be either IBM Analytics Engine or your own Spark Cluster. Provide details of any one of them in this section.\n",
    "\n",
    "Please note, if you are using your own Spark cluster, checkout IBM Watson OpenScale documentation on how to setup spark manager API to enable interface for use with IBM Watson OpenScale services.\n",
    "\n",
    "### Parameters for IBM Analytics Engine\n",
    "If your job is going to run on Spark cluster as part of an IBM Analytics Engine instance on IBM Cloud Pak for Data, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| display_name | Display Name of the Spark instance in IBM Analytics Engine | |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For IBM IAE, this must be set to `cpd_iae`. | `cpd_iae` |\n",
    "| endpoint | Spark Jobs Endpoint for IBM Analytics Engine | |\n",
    "| volume | IBM Cloud Pak for Data storage volume name | |\n",
    "| username | IBM Cloud Pak for Data username | |\n",
    "| apikey | IBM Cloud Pak for Data API key | |\n",
    "\n",
    "### Parameters for Remote Spark Cluster\n",
    "If your job is going to run on Spark Cluster as part of a Remote Hadoop Ecosystem, enter the following details:\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| location_type | Identifies if compute engine is IBM IAE or Remote Spark. For Remote Spark, this must be set to `custom`. | `custom` |\n",
    "| endpoint | Endpoint URL where the Spark Manager Application is running | |\n",
    "| username | Username to connect to Spark Manager Application | |\n",
    "| password | Password to connect to Spark Manager Application | |\n",
    "\n",
    "\n",
    "### Provide Spark Resource Settings [Optional]\n",
    "Configure how much of your Spark Cluster resources can this job consume. Leave the variable `spark_settings` to `{}` if no customisation is required.\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| max_num_executors | Maximum Number of executors to launch for this session |\n",
    "| min_executors | Minimum Number of executors to launch for this session |\n",
    "| executor_cores | Number of cores to use for each executor |\n",
    "| executor_memory | Amount of memory (in GBs) to use per executor process |\n",
    "| driver_cores | Number of cores to use for the driver process |\n",
    "| driver_memory | Amount of memory (in GBs) to use for the driver process |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_connection_info = {\n",
    "    \"connection\": {\n",
    "        \"endpoint\": \"<to_be_edited>\",\n",
    "        \"location_type\": \"<to_be_edited>\",\n",
    "        \"display_name\": \"<to_be_edited>\",\n",
    "        \"volume\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": \"<to_be_edited>\",\n",
    "        \"password\": \"<to_be_edited>\",\n",
    "        \"apikey\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "\n",
    "spark_settings = {\n",
    "    # max_num_executors: Maximum Number of executors to launch for this session\n",
    "    \"max_num_executors\": \"2\",\n",
    "    \n",
    "    # min_executors: Minimum Number of executors to launch for this session\n",
    "    \"min_executors\": \"1\",\n",
    "    \n",
    "    # executor_cores: Number of cores to use for each executor\n",
    "    \"executor_cores\": \"2\",\n",
    "    \n",
    "    # executor_memory: Amount of memory (in GBs) to use per executor process\n",
    "    \"executor_memory\": \"2\",\n",
    "    \n",
    "    # driver_cores: Number of cores to use for the driver process\n",
    "    \"driver_cores\": \"2\",\n",
    "    \n",
    "    # driver_memory: Amount of memory (in GBs) to use for the driver process \n",
    "    \"driver_memory\": \"1\"\n",
    "}\n",
    "\"\"\"\n",
    "spark_settings = {}\n",
    "\n",
    "spark_connection_info[\"spark_settings\"] = spark_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Backend Storage Details <a name=\"backend-storage\"></a>\n",
    "\n",
    "IBM Watson OpenScale services monitors models by analyzing runtime data, i.e., the data model is making predictions on. To do this analysis, most of the services require access to this runtime data (also called payload data). In addition, some of the services may require access to manually labelled runtime data (also called feedback data). Hence, user needs to store such data in some backend storage and connect this storage to IBM Watson OpenScale.\n",
    "\n",
    "### Provide DB2 table details where training data is hosted\n",
    "\n",
    "| Parameter | Description | Possible Value(s) |\n",
    "| :- | :- | :- |\n",
    "| type | Describes the type of storage being used. For DB2, this must be set to `jdbc`. | `jdbc` |\n",
    "| jdbc_url | Connection string for jdbc. Example: `jdbc:db2://jdbc_host:jdbc_port/database_name` | |\n",
    "| jdbc_driver | Optional. Class name of the JDBC driver to use to connect. Example: for DB2 use `com.ibm.db2.jcc.DB2Driver` ||\n",
    "| use_ssl | Boolean value to indicate whether to use SSL while connecting | `True` or `False` |\n",
    "| certificate | SSL Certificate [Base64 encoded string] of the JDBC Connection. Ignored if `use_ssl` is `False`. |\n",
    "| location_type | Identifies the type of location for connection to use. For DB2, this must be set to `jdbc`. | `jdbc` |\n",
    "| username | Username of the JDBC Connection | |\n",
    "| password | Password of the JDBC Connection | |\n",
    "| database | Name of database hosting training data table | |\n",
    "| schema | Name of schema hosting training data table | |\n",
    "| table | Name of training data table | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datawarehouse_details = {\n",
    "    \"type\": \"jdbc\",\n",
    "    \"connection\": {\n",
    "        \"jdbc_url\": \"<to_be_edited>\",\n",
    "        \"jdbc_driver\": \"<to_be_edited>\",\n",
    "        \"use_ssl\": \"<to_be_edited>\",\n",
    "        \"certificate\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"credentials\": {\n",
    "        \"username\": \"<to_be_edited>\",\n",
    "        \"password\": \"<to_be_edited>\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide details of different tables <a name=\"table-details\"></a>\n",
    "\n",
    "IBM Watson OpenScale services require different tables to perform their analysis. Depending on which services you have enabled, provide details of the corresponding tables.\n",
    "Tables are:\n",
    "\n",
    "| Table | Description |\n",
    "| :- | :- |\n",
    "| Payload Table | Hosts the runtime data predicted by model. Required for detecting fairness and drift in runtime data. |\n",
    "| Feedback Table | Hosts the manually labelled runtime data (also called feedback data) predicted by model. Required for tracking quality of monitor by analyzing feedback data. |\n",
    "| Drifted Transactions Table | Hosts the data identified to be drifted.|\n",
    "| Explain Queue Table | Hosts the data for which explanations are required to be generated. This can be same as payload table.|\n",
    "| Explain Results Table | Hosts the explanations generated for records in explain queue table. |\n",
    "\n",
    "For each of the table, following information is required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| database | Name of the database hosting the schema. |\n",
    "| schema | Name of the schema hosting the table. |\n",
    "| table | Name of the table. |\n",
    "| auto_create | Boolean value identifying if the table already exists or has to be created via IBM Watson OpenScale. |\n",
    "| partition_column | The column to help Spark read and write data using multiple workers in your JDBC storage. This will help improve the performance of your Spark jobs. |\n",
    "| num_partition | An integer value which defines maximum number of partitions that Spark can divide the data into. In JDBC, it also means the maximum number of connections that Spark can make to the JDBC store for reading/writing data. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recommended number of partitions calculation\n",
    "# num_partitions_recommended = 12\n",
    "\n",
    "# if spark_parameters:\n",
    "#     executors = spark_parameters.get(\"max_num_executors\", 2)\n",
    "#     cores = spark_parameters.get(\"executor_cores\", 2)\n",
    "#     num_partitions_recommended = 2 * executors * cores\n",
    "    \n",
    "# print(\"{} is the recommended value for number of partitions in your data. \\\n",
    "# Please change this value as per your data.\".format(num_partitions_recommended))\n",
    "\n",
    "DATABASE_NAME=\"<to_be_edited>\"\n",
    "SCHEMA_NAME=\"<to_be_edited>\"\n",
    "\n",
    "# Payload table information\n",
    "payload_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"partition_column\": \"<to_be_edited>\",\n",
    "        \"num_partitions\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Feedback table information\n",
    "feedback_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"partition_column\": \"<to_be_edited>\",\n",
    "        \"num_partitions\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Drifted Transaction table. \n",
    "#Set this table information if drift is enabled\n",
    "drifted_transaction_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"partition_column\": \"<to_be_edited>\",\n",
    "        \"num_partitions\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Explanation Result table\n",
    "#Set this table information if Explain is enabled\n",
    "explain_result_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Explanation Queue table\n",
    "#Set this table information if Explain is enabled\n",
    "explain_queue_table = {\n",
    "    \"data\": {\n",
    "        \"auto_create\": True, #set it to False if table already exists\n",
    "        \"database\": DATABASE_NAME,\n",
    "        \"schema\": SCHEMA_NAME,\n",
    "        \"table\": \"<to_be_edited>\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"partition_column\": \"<to_be_edited>\",\n",
    "        \"num_partitions\": \"<to_be_edited>\"\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM Watson OpenScale instance <a name=\"connect-openscale\"></a>\n",
    "\n",
    "Following information is required to connect to IBM Watson OpenScale instance:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| host | Hostname of your Cloud Pak for Data cluster hosting IBM Watson OpenScale instance. |\n",
    "| username | Username to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. |\n",
    "| password | Password to connect to your IBM Watson OpenScale instance in  Cloud Pak for Data cluster. One of `username+password` or `api_key` must be provided. |\n",
    "| api_key | API Key to connect to your IBM Watson OpenScale instance in Cloud Pak for Data cluster. One of `username+password` or `api_key` must be provided. |\n",
    "| service_instance_id | Id of your IBM Watson OpenScale Instance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import APIClient\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "service_instance_id = \"<SERVICE_INSTANCE_ID>\" #Default is 00000000-0000-0000-0000-000000000000\n",
    "service_credentials = {\n",
    "    \"url\": \"<to_be_edited>\",\n",
    "    \"username\": \"<to_be_edited>\",\n",
    "    \"password\": \"<to_be_edited>\",\n",
    "#     \"apikey\":\"<to_be_edited>\"\n",
    "}\n",
    "\n",
    "authenticator = CloudPakForDataAuthenticator(\n",
    "    url=service_credentials['url'],\n",
    "    username=service_credentials['username'],\n",
    "    password=service_credentials['password'],\n",
    "#     apikey=service_credentials['apikey'],\n",
    "    disable_ssl_verification=True\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    service_url=service_credentials['url'],\n",
    "    service_instance_id=service_instance_id,\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Machine Learning Provider in IBM Watson OpenScale instance <a name=\"create-service-provider\"></a>\n",
    "\n",
    "Before configuring model for monitoring in IBM Watson OpenScale, you need to connect your machine learning provider with IBM Watson OpenScale instance. Since, we are configuring a model for monitoring which has its runtime data located remotely to IBM Watson OpenScale, we'll create a custom machine learning provider in given instance.\n",
    "\n",
    "Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| name | Name of the machine learning provider being configured. This can be any string value. |\n",
    "| description | Description for the machine learning provider being configured. |\n",
    "| service_type | Identifies type of the machine learning provider. In this case, this value must be `ServiceTypes.CUSTOM_MACHINE_LEARNING` |\n",
    "| credentials | Optional input, stores username and password to connect to machine learning provider. |\n",
    "| operational_space_id | Defines the classification of machine learning provider. Possible values are `pre-production` and `production`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Delete existing service provider with the same name as provided\n",
    "\n",
    "# service_providers = wos_client.service_providers.list().result.service_providers\n",
    "# for provider in service_providers:\n",
    "#     if provider.entity.name == SERVICE_PROVIDER_NAME:\n",
    "#         wos_client.service_providers.delete(service_provider_id=provider.metadata.id)\n",
    "#         break\n",
    "\n",
    "# Add Service Provider\n",
    "added_service_provider_result = wos_client.service_providers.add(\n",
    "        name=\"<to_be_edited>\",\n",
    "        description=\"<to_be_edited>\",\n",
    "        service_type=ServiceTypes.CUSTOM_MACHINE_LEARNING,\n",
    "        credentials={},\n",
    "        operational_space_id=\"<to_be_edited>\",\n",
    "        background_mode=False\n",
    "    ).result\n",
    "\n",
    "service_provider_id = added_service_provider_result.metadata.id\n",
    "\n",
    "wos_client.service_providers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onboard model for monitoring in IBM Watson OpenScale instance <a name=\"create-subscription\"></a>\n",
    "\n",
    "When you configure a model for monitoring in IBM Watson OpenScale instance, a corresponding subscription is created for this model. Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| subscription_name | Name of the subscription to use. This can be any string value typically identifying model being monitored. |\n",
    "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
    "| service_provider_id | Id of the machine learning provider instance created in IBM Watson OpenScale. |\n",
    "| configuration_archive | Path to configuration package archive. |\n",
    "| spark_credentials | Connection details of Spark compute engine to use for analysis by different IBM Watson OpenScale services. |\n",
    "| data_warehouse_connection | Details of the backend storage hosting tables for data, feedback data, etc. |\n",
    "| payload_table | Details of the payload table to be used with this subscription. |\n",
    "| feedback_table | Details of the feedback table to be used with this subscription. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to the configuration file. If executing in IBM Watson Studio then leave as it is\n",
    "subscription_id = client.subscriptions.create_subscription(\n",
    "    subscription_name=\"My SDK Batch Subscription-hive\",\n",
    "    datamart_id=service_instance_id,\n",
    "    service_provider_id=service_provider_id,\n",
    "    configuration_archive=archive_file_path,\n",
    "    spark_credentials=spark_connection_info,\n",
    "    data_warehouse_connection=datawarehouse_details,\n",
    "    payload_table=payload_table,\n",
    "    feedback_table=feedback_table\n",
    ")\n",
    "\n",
    "# print(\"Subscription id is {}\".format(subscription_id))\n",
    "\n",
    "# Wait for the subscription to get in active state and to create the \n",
    "# required tables in the background before moving onto enabling monitors\n",
    "\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "\n",
    "# subscription_status = None\n",
    "# while subscription_status not in (\"active\", \"error\"):\n",
    "#     subscription_status = client.subscriptions.get(subscription_id).result.entity.status.state\n",
    "#     if subscription_status not in (\"active\", \"error\"):\n",
    "#         print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)\n",
    "#         time.sleep(15)\n",
    "        \n",
    "# print(datetime.now().strftime(\"%H:%M:%S\"), subscription_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable different services to monitor model <a name=\"enable-monitors\"></a>\n",
    "\n",
    "Depending on the services enabled in configuration package and their corresponding artefacts availability, different services are enabled in given subscription. There services are called monitors.\n",
    "\n",
    "Following details are required:\n",
    "\n",
    "| Parameter | Description |\n",
    "| :- | :- |\n",
    "| datamart_id | Same as id of IBM Watson OpenScale instance. |\n",
    "| service_provider_id | Id of the machine learning provider instance created in IBM Watson OpenScale. |\n",
    "| subscription_id | Id of the subscription created for given model in IBM Watson OpenScale instance. |\n",
    "| configuration_archive | Path to configuration package archive. |\n",
    "| drifted_transaction_table | Details of the drifted transactions table to be used with this subscription. |\n",
    "| explain_queue_table | Details of the explain queue table to be used with this subscription. |\n",
    "| explain_results_table | Details of the explain results table to be used with this subscription. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_ids = client.monitor_instances.enable_monitors(\n",
    "    datamart_id=service_instance_id,\n",
    "    service_provider_id=service_provider_id,\n",
    "    subscription_id=subscription_id,\n",
    "    configuration_archive=archive_file_path,\n",
    "    drifted_transaction_table=drifted_transaction_table,\n",
    "    explain_queue_table=explain_queue_table,\n",
    "    explain_results_table=explain_result_table\n",
    ")\n",
    "\n",
    "print(instance_ids)\n",
    "\n",
    "## Track each monitor instance status\n",
    "# for key, value in instance_ids.items():\n",
    "#     monitor_instance_status = None\n",
    "\n",
    "#     while monitor_instance_status not in (\"active\", \"error\"):\n",
    "#         monitor_instance_details = client.monitor_instances.get(monitor_instance_id=value).result\n",
    "#         monitor_instance_status = monitor_instance_details.entity.status.state\n",
    "#         if monitor_instance_status not in (\"active\", \"error\"):\n",
    "#             print(datetime.now().strftime(\"%H:%M:%S\"), monitor_instance_status)\n",
    "#             time.sleep(30)\n",
    "\n",
    "#     print(key, monitor_instance_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "All the monitors have been enabled. It will take some time for monitors to get into active state. You can track the status of each monitor separately by using above code snippet.\n",
    "\n",
    "Once, all monitors are active, load data into payload or feedback table and either run on-demand evaluations or wait for scheduled evaluations to complete for each monitor. You can check more details in [Watson OpenScale Dashboard](https://url-to-your-cp4d-cluster/aiopenscale)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd777eb09628c67f3230dcb9b65a43e2b99cf8b53ccb1d6b9168eddb4470fb9f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
