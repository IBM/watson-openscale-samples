{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for analyzing payload transactions causing drift\n",
    "\n",
    "This notebook helps users of IBM Watson OpenScale to analyze payload transactions that are causing drift - both drop in accuracy and drop in data consistency. \n",
    "\n",
    "The notebook is designed to give users a jump start in their analysis of the payload transactions. It is by no means a comprehensive analysis. \n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) to be able to proceed with the analysis. \n",
    "\n",
    "PS: This notebook is designed to analyse one drift monitor run at a time for a given subscription.\n",
    "\n",
    "**Contents:**\n",
    "1. [Pre-requisites](#Pre-requisites)\n",
    "2. [Installing Dependencies](#Installing-Dependencies)\n",
    "3. [User Inputs](#User-Inputs)\n",
    "4. [Setting up Services](#Setting-up-Services)\n",
    "5. [Measurement Summary](#Measurement-Summary)\n",
    "6. [Counts from Drifted Transactions Table](#Counts-from-Drifted-Transactions-Table)\n",
    "7. [Analyse Transactions Causing Drop in Accuracy](#Analyse-Transactions-Causing-Drop-in-Accuracy)\n",
    "    * [Get all transactions causing drop in data accuracy](#Get-all-transactions-causing-drop-in-data-accuracy)\n",
    "    * [Get all transactions causing drop in accuracy in given range of drift model confidence](#Get-all-transactions-causing-drop-in-accuracy-in-given-range-of-drift-model-confidence)\n",
    "8. [Analyse Transactions Causing Drop in Accuracy and Drop in Data Consistency](#Analyse-Transactions-Causing-Drop-in-Accuracy-and-Drop-in-Data-Consistency)\n",
    "    * [Get all transactions causing drop in accuracy and drop in data consistency](#Get-all-transactions-causing-drop-in-accuracy-and--drop-in-data-consistency)\n",
    "    * [Get all transactions causing drop in accuracy and drop in data consistency in given range of drift model confidence](#Get-all-transactions-causing-drop-in-accuracy-and-drop-in-data-consistency-in-given-range-of-drift-model-confidence)\n",
    "9. [Analyse Transactions Causing Drop in Data Consistency](#Analyse-Transactions-Causing-Drop-in-Data-Consistency)\n",
    "    * [Get all transactions causing drop in data consistency](#Get-all-transactions-causing-drop-in-data-consistency)\n",
    "    * [Get all transactions violating a data constraint](#Get-all-transactions-violating-a-data-constraint)\n",
    "    * [Get all transactions where a column is causing drop in data consistency](#Get-all-transactions-where-a-column-is-causing-drop-in-data-consistency)\n",
    "    * [Explain categorical distribution constraint violations](#Explain-categorical-distribution-constraint-violations)\n",
    "    * [Explain numeric range constraint violations](#Explain-numeric-range-constraint-violations)\n",
    "    * [Explain cat-numeric range constraint violations](#Explain-cat-numeric-range-constraint-violations)\n",
    "    * [Explain cat-cat distribution constraint violations](#Explain-cat-cat-distribution-constraint-violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Jupyter Server on which this notebook is running should be able to access the HDFS cluster on which data is residing in Hive.\n",
    "2. **Connecting to Non-Kerberised Hive:** The only property required for this is HIVE_METASTORE_URI in the [User Inputs](#User-Inputs) section.\n",
    "3. **Connecting to Kerberised Hive:** \n",
    "    - Make sure you are able to obtain a Kerberos ticket using kinit for the cluster you are planning to connect to. This needs to be done before starting the jupyter server.\n",
    "    - The **core-site.xml** file under **SPARK_HOME/conf** directory in the machine where jupyter is running should have the following property:\n",
    "    ```\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>hadoop.security.authentication</name>\n",
    "            <value>kerberos</value>\n",
    "        </property>\n",
    "    </configuration>\n",
    "\n",
    "    ```\n",
    "    - There are three other properties required in the [User Inputs](#User-Inputs) section besides HIVE_METASTORE_URI. \n",
    "    - PS: Currently it is not possible to connect to a Kerberised Hive from Watson Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5737-H76\n",
    "# Copyright IBM Corp. 2020, 2022\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = \"hive-1.1.6\"\n",
    "\n",
    "# Changelog\n",
    "\n",
    "# hive-1.1.6 : Install pyspark as a pre-requisite\n",
    "# hive-1.1.5 : Upgrade ibm-wos-utils to 4.5.0\n",
    "# hive-1.1.4 : Upgrade ibm-wos-utils to 4.1.1 (scikit-learn has been upgraded to 1.0.2)\n",
    "# hive-1.1.3 : Upgrade ibm-wos-utils to 4.0.34\n",
    "# hive-1.1.2 : Upgrade ibm-wos-utils to 4.0.31\n",
    "# hive-1.1.1 : Add comment about conda install for zLinux environments; Update wos-utils version to 4.0.25\n",
    "# hive-1.1 : Update wos-utils version to 4.0.24\n",
    "# 1.0 : First public release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PYTHON = sys.executable\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts --upgrade tabulate ibm-watson-openscale pyspark | tail -n 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For IBM Watson OpenScale Cloud Pak for Data version 4.0.0 - 4.0.7, use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==0.24.2 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=0.24.2\n",
    "\n",
    "# !$PYTHON -m pip install --no-warn-conflicts \"ibm-wos-utils~=4.0.34\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For IBM Watson OpenScale Cloud Pak for Data version 4.0.8 - 4.0.x, use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==1.0.2 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=1.0.2\n",
    "\n",
    "# !$PYTHON -m pip install --no-warn-conflicts \"ibm-wos-utils~=4.1.1\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For IBM Watson OpenScale Cloud Pak for Data version 4.5.x, use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When this notebook is to be run on a zLinux cluster,\n",
    "# install scikit-learn==1.0.2 using conda before installing ibm-wos-utils\n",
    "# !conda install scikit-learn=1.0.2\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts \"ibm-wos-utils~=4.5.0\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "\n",
    "The following inputs are required:\n",
    "\n",
    "1. **IBM_CPD_ENDPOINT:** The URL representing the IBM Cloud Pak for Data service endpoint.\n",
    "2. **IBM_CPD_USERNAME:** IBM Cloud Pak for Data username used to obtain a bearer token.\n",
    "3. **IBM_CPD_PASSWORD:** IBM Cloud Pak for Data password used to obtain a bearer token.\n",
    "4. **HIVE_METASTORE_URI:** Hive Metastore URI to connect to using this notebook\n",
    "5. **KERBERISED_HIVE_YARN_RM_PRINCIPAL:** Yarn Resource Manager Principal (_required only if connecting to Kerberised Hive_)\n",
    "6. **KERBERISED_HIVE_YARN_RM_KEYTAB:** Path to the Yarn Resource Manager KeyTab file on the cluster (_required only if connecting to Kerberised Hive_)\n",
    "7. **KERBERISED_HIVE_METASTORE_PRINCIPAL:** Hive MetaStore Principal (_required only if connecting to Kerberised Hive_)\n",
    "8. **ANALYSIS_INPUT_PARAMETERS:** Analysis Input Parameters to be copied from IBM Watson OpenScale UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM Cloud Pak for Data credentials\n",
    "IBM_CPD_ENDPOINT = \"<The URL representing the IBM Cloud Pak for Data service endpoint.>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username used to obtain a bearer token.>\"\n",
    "IBM_CPD_PASSWORD = \"<IBM Cloud Pak for Data password used to obtain a bearer token.>\"\n",
    "\n",
    "# Hive Metastore URI to connect to\n",
    "HIVE_METASTORE_URI = \"<Hive Metastore URI>\"\n",
    "\n",
    "# Additional Properties Required for connecting to KERBERISED Hive\n",
    "KERBERISED_HIVE_YARN_RM_PRINCIPAL = \"<Yarn Resource Manager Principal>\"\n",
    "KERBERISED_HIVE_YARN_RM_KEYTAB = \"<Path to the Yarn Resource Manager KeyTab file on the cluster>\"\n",
    "KERBERISED_HIVE_METASTORE_PRINCIPAL = \"<Hive MetaStore Principal>\"\n",
    "\n",
    "\n",
    "# Analysis Input Parameters to be copied from UI\n",
    "# Please make sure that the quotes around the key-values \n",
    "# are correct after copying from UI\n",
    "ANALYSIS_INPUT_PARAMETERS = {\n",
    "    \"data_mart_id\": \"<data_mart_id>\",\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"monitor_instance_id\": \"<monitor_instance_id>\",\n",
    "    \"measurement_id\": \"<measurement_id>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAMART_ID = ANALYSIS_INPUT_PARAMETERS.get(\"data_mart_id\")\n",
    "SUBSCRIPTION_ID = ANALYSIS_INPUT_PARAMETERS.get(\"subscription_id\")\n",
    "MONITOR_INSTANCE_ID = ANALYSIS_INPUT_PARAMETERS.get(\"monitor_instance_id\")\n",
    "MEASUREMENT_ID = ANALYSIS_INPUT_PARAMETERS.get(\"measurement_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import APIClient\n",
    "\n",
    "from ibm_wos_utils.drift.batch.util.constants import ConstraintName\n",
    "from ibm_wos_utils.joblib.utils.analyze_notebook_utils import (\n",
    "    explain_catcat_distribution_constraint,\n",
    "    explain_categorical_distribution_constraint,\n",
    "    explain_catnum_range_constraint, explain_numeric_range_constraint,\n",
    "    get_column_query, get_drift_archive_contents,\n",
    "    get_table_details_from_subscription, show_constraints_by_column,\n",
    "    show_dataframe, show_last_n_drift_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "        .setAppName(\"Analyze Drifted Transactions\")\\\n",
    "        .set(\"spark.hadoop.hive.metastore.uris\", HIVE_METASTORE_URI)\n",
    "\n",
    "# Uncomment the following block if connecting to a Kerberised Hive\n",
    "\"\"\"\n",
    "conf = conf\\\n",
    "        .set(\"spark.hadoop.yarn.resourcemanager.principal\", KERBERISED_HIVE_YARN_RM_PRINCIPAL)\\\n",
    "        .set(\"spark.hadoop.yarn.resourcemanager.keytab\", KERBERISED_HIVE_YARN_RM_KEYTAB)\\\n",
    "        .set(\"spark.hadoop.hive.metastore.kerberos.principal\", KERBERISED_HIVE_METASTORE_PRINCIPAL)\\\n",
    "        .set(\"spark.hadoop.hive.metastore.sasl.enabled\", \"true\")\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = CloudPakForDataAuthenticator(\n",
    "        url=IBM_CPD_ENDPOINT,\n",
    "        username=IBM_CPD_USERNAME,\n",
    "        password=IBM_CPD_PASSWORD,\n",
    "        disable_ssl_verification=True\n",
    "    )\n",
    "wos_client = APIClient(authenticator=authenticator, service_url=IBM_CPD_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not DATAMART_ID or not SUBSCRIPTION_ID:\n",
    "    raise Exception(\"DATAMART_ID and SUBSCRIPTION_ID are required to proceed.\")\n",
    "\n",
    "subscription = wos_client.subscriptions.get(subscription_id=SUBSCRIPTION_ID).result\n",
    "monitor_instance = wos_client.monitor_instances.list(data_mart_id=DATAMART_ID, target_target_id=SUBSCRIPTION_ID, monitor_definition_id=\"drift\").result.monitor_instances[0]\n",
    "model_drift_enabled = monitor_instance.entity.parameters.get(\"model_drift_enabled\", False)\n",
    "data_drift_enabled = monitor_instance.entity.parameters.get(\"data_drift_enabled\", False)\n",
    "\n",
    "if not MONITOR_INSTANCE_ID:\n",
    "    MONITOR_INSTANCE_ID = monitor_instance.metadata.id\n",
    "    \n",
    "drift_archive = wos_client.monitor_instances.download_drift_model(monitor_instance_id=MONITOR_INSTANCE_ID).result.content\n",
    "schema, ddm_properties, constraints_set = get_drift_archive_contents(drift_archive, model_drift_enabled, data_drift_enabled)\n",
    "payload_database_name, _, payload_table_name = get_table_details_from_subscription(subscription, \"payload\")\n",
    "drift_database_name, _, drift_table_name = get_table_details_from_subscription(subscription, \"drift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies heavily on filtering transactions in the Drifted Transactions table based on three columns: `run_id`, `is_model_drift` and `is_data_drift`. \n",
    "\n",
    "It is, therefore, recommended that you create and build an index for these columns, if not done already as part of the common configuration notebook. You can use the following DDL to create and build the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_string = \"CREATE INDEX {1}_index ON TABLE {0}.{1} (run_id, is_model_drift, is_data_drift) AS 'BITMAP' WITH DEFERRED REBUILD;\\n\"\n",
    "ddl_string += \"ALTER INDEX {1}_index ON {0}.{1} REBUILD;\"\n",
    "ddl_string.format(drift_database_name, drift_table_name)\n",
    "print(ddl_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MEASUREMENT_ID:\n",
    "    print(\"Please pick a measurement to analyze from the following list:\")\n",
    "    \n",
    "show_last_n_drift_measurements(10, wos_client, SUBSCRIPTION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not selected MEASUREMENT_ID so far, please enter a measurement ID\n",
    "# from the above cell's output to analyze.\n",
    "\n",
    "# MEASUREMENT_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MEASUREMENT_ID:\n",
    "    raise Exception(\"MEASUREMENT_ID is required to proceed.\")\n",
    "\n",
    "measurement = wos_client.monitor_instances.measurements.get(measurement_id=MEASUREMENT_ID, monitor_instance_id=MONITOR_INSTANCE_ID).result\n",
    "measurement_data = measurement.entity.sources[0].data\n",
    "MONITOR_RUN_ID = measurement.entity.run_id\n",
    "MONITOR_RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy and drop in data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IBM Watson OpenScale analyzed {} transactions between {} and {} for drift. Here's a summary.\".format(measurement_data[\"transactions_count\"], measurement_data[\"start\"], measurement_data[\"end\"]))\n",
    "\n",
    "if model_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing drop in accuracy.\".format(measurement_data[\"drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))\n",
    "\n",
    "if data_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing drop in data consistency.\".format(measurement_data[\"data_drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))\n",
    "    \n",
    "if model_drift_enabled and data_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing both drop in accuracy and drop in data consistency.\".format(measurement_data[\"model_data_drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy - percent bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_drift_enabled:\n",
    "    rows_df = pd.DataFrame(measurement_data[\"drifted_transactions\"][\"drift_model_confidence_count\"])\n",
    "    rows_df = rows_df[[\"lower_limit\", \"upper_limit\", \"count\"]]\n",
    "    rows_df.columns = [\"Drift Model Confidence - Lower Limit\", \"Drift Model Confidence - Upper Limit\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in data consistency - feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_drift_enabled:\n",
    "    rows_df = pd.Series(measurement_data[\"data_drifted_transactions\"][\"features_count\"])\\\n",
    "                .sort_values(ascending=False).to_frame()\n",
    "    rows_df.reset_index(inplace=True)\n",
    "    rows_df.columns = [\"Feature Column\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy - constraints list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_drift_enabled:\n",
    "    rows_df = pd.Series(measurement_data[\"data_drifted_transactions\"][\"constraints_count\"])\\\n",
    "                .sort_values(ascending=False).to_frame()\n",
    "    rows_df.reset_index(inplace=True)\n",
    "    rows_df.columns = [\"Constraint Name\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts from Drifted Transactions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_table_df = spark.sql(\"select * from {}.{} where run_id = '{}'\".format(drift_database_name, drift_table_name, MONITOR_RUN_ID))\n",
    "drift_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.hive_utils import get_table_as_dataframe\n",
    "\n",
    "payload_table_df = get_table_as_dataframe(spark=spark,\n",
    "                                         database_name=payload_database_name,\n",
    "                                         table_name=payload_table_name,\n",
    "                                         columns_to_map=subscription.entity.asset_properties.feature_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Total number of drifted transactions: {}\".format(drift_table_df.count()))\n",
    "print(\"Total number of model drift transactions: {}\".format(drift_table_df.where(\"is_model_drift\").count()))\n",
    "print(\"Total number of data drift transactions: {}\".format(drift_table_df.where(\"is_data_drift\").count()))\n",
    "print(\"Total number of model + data drift transactions: {}\".format(drift_table_df.where(\"is_model_drift\").where(\"is_data_drift\").count()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Transactions Causing Drop in Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in data accuracy\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy.\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy in given range of drift model confidence\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drift Model Confidence Lower Limit\n",
    "dm_conf_lower = 0.5\n",
    "# Drift Model Confidence Upper Limit\n",
    "dm_conf_upper = 1.0\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(drift_table_df.drift_model_confidence.between(dm_conf_lower,dm_conf_upper))\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy where drift model confidence is between {} and {}\".format(count, dm_conf_lower, dm_conf_upper))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Transactions Causing Drop in Accuracy and Drop in Data Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy and  drop in data consistency\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing both drop in accuracy and drop in data consistency\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy and drop in data consistency in given range of drift model confidence\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drift Model Confidence Lower Limit\n",
    "dm_conf_lower = 0.5\n",
    "# Drift Model Confidence Upper Limit\n",
    "dm_conf_upper = 1.0\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .where(drift_table_df.drift_model_confidence.between(dm_conf_lower,dm_conf_upper))\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy and drop in data consistency where drift model confidence is between {} and {}\".format(count, dm_conf_lower, dm_conf_upper))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Transactions Causing Drop in Data Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in data consistency\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .select([\"scoring_id\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in data consistency\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions violating a data constraint\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_name = ConstraintName.CATEGORICAL_DISTRIBUTION_CONSTRAINT\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "        .where(\"is_data_drift\")\\\n",
    "        .where(F.col(constraint_name.value).like(\"%1%\"))\\\n",
    "        .select([\"scoring_id\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are violating {}.\".format(count, constraint_name.value))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "    \n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions where a column is causing drop in data consistency\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_query = get_column_query(constraints_set, schema, column=\"<column_name>\")\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .where(filter_query)\\\n",
    "    .select([\"scoring_id\"])\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are satisfying the given query.\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query all the learnt constraints based on a column name\n",
    "\n",
    "Use the `show_constraints_by_column` method to find all the constraints learnt for a particular column at training time. The constraint ids shown in the cell output can be used to explain the corresponding constraint in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_constraints_by_column(constraints_set, \"<column_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain categorical distribution constraint violations\n",
    "\n",
    "Explains categorical distribution constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_categorical_distribution_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain numeric range constraint violations\n",
    "\n",
    "Explains numeric range constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_numeric_range_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain cat-numeric range constraint violations\n",
    "\n",
    "Explains cat-numeric range constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_catnum_range_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain cat-cat distribution constraint violations\n",
    "\n",
    "Explains cat-cat distribution constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "The `drifted_transactions_df` can be exported to a format of your choice for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_catcat_distribution_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "Developed by [Prem Piyush Goyal](mailto:prempiyush@in.ibm.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
