{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for analyzing payload transactions causing drift\n",
    "\n",
    "Use this notebook to analyze payload transactions that are causing drift, both a drop in accuracy and a drop in data consistency. The notebook is designed to get you started analyzing payload transactions and is not meant to be a comprehensive analysis. \n",
    "\n",
    "The user needs to provide the necessary inputs (where marked) to be able to proceed with the analysis. \n",
    "\n",
    "**Note**: This notebook is designed to analyze one drift monitor run at a time for a given subscription. For multiple models, this notebook needs to be run for each model separately.\n",
    "\n",
    "**Contents:**\n",
    "1. [Pre-requisites](#Pre-requisites)\n",
    "2. [Installing Dependencies](#Installing-Dependencies)\n",
    "3. [User Inputs](#User-Inputs)\n",
    "4. [Setting up Services](#Setting-up-Services)\n",
    "5. [Measurement Summary](#Measurement-Summary)\n",
    "6. [Counts from Drifted Transactions Table](#Counts-from-Drifted-Transactions-Table)\n",
    "7. [Analyse Transactions Causing Drop in Accuracy](#Analyse-Transactions-Causing-Drop-in-Accuracy)\n",
    "    * [Get all transactions causing drop in data accuracy](#Get-all-transactions-causing-drop-in-data-accuracy)\n",
    "    * [Get all transactions causing drop in accuracy in given range of drift model confidence](#Get-all-transactions-causing-drop-in-accuracy-in-given-range-of-drift-model-confidence)\n",
    "8. [Analyse Transactions Causing Drop in Accuracy and Drop in Data Consistency](#Analyse-Transactions-Causing-Drop-in-Accuracy-and-Drop-in-Data-Consistency)\n",
    "    * [Get all transactions causing drop in accuracy and drop in data consistency](#Get-all-transactions-causing-drop-in-accuracy-and--drop-in-data-consistency)\n",
    "    * [Get all transactions causing drop in accuracy and drop in data consistency in given range of drift model confidence](#Get-all-transactions-causing-drop-in-accuracy-and-drop-in-data-consistency-in-given-range-of-drift-model-confidence)\n",
    "9. [Analyse Transactions Causing Drop in Data Consistency](#Analyse-Transactions-Causing-Drop-in-Data-Consistency)\n",
    "    * [Get all transactions causing drop in data consistency](#Get-all-transactions-causing-drop-in-data-consistency)\n",
    "    * [Get all transactions violating a data constraint](#Get-all-transactions-violating-a-data-constraint)\n",
    "    * [Get all transactions where a column is causing drop in data consistency](#Get-all-transactions-where-a-column-is-causing-drop-in-data-consistency)\n",
    "    * [Explain categorical distribution constraint violations](#Explain-categorical-distribution-constraint-violations)\n",
    "    * [Explain numeric range constraint violations](#Explain-numeric-range-constraint-violations)\n",
    "    * [Explain cat-numeric range constraint violations](#Explain-cat-numeric-range-constraint-violations)\n",
    "    * [Explain cat-cat distribution constraint violations](#Explain-cat-cat-distribution-constraint-violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The platform, such as Watson Studio or another Jupyter Server on which this notebook is running must be able to access the HDFS cluster on which data is residing in the Apache Hive data warehouse.\n",
    "2. **Connecting to Non-Kerberized Hive:** The only property required to connect to a non-Kerberized Hive data warehouse is the `HIVE_METASTORE_URI` parameter in the [User Inputs](#User-Inputs) section.\n",
    "3. **Connecting to Kerberized Hive:** \n",
    "    - Make sure you are able to obtain a Kerberos ticket by using the `kinit` method for the cluster you are planning to connect to. This needs to be done before starting the jupyter server.\n",
    "    - The **core-site.xml** file in the **SPARK_HOME/conf** directory on the workstation where jupyter is running should have the following property:\n",
    "    ```\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>hadoop.security.authentication</name>\n",
    "            <value>kerberos</value>\n",
    "        </property>\n",
    "    </configuration>\n",
    "\n",
    "    ```\n",
    "    - There are three other properties required in the [User Inputs](#User-Inputs) section besides HIVE_METASTORE_URI. \n",
    "    - **Note**: Currently it is not possible to connect to a Kerberized Apache Hive data warehouse from Watson Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "Let's start by installing some of the required software libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5900-A3Q, 5737-J33\n",
    "# Copyright IBM Corp. 2020\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade\n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "VERSION = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PYTHON = sys.executable\n",
    "\n",
    "!$PYTHON -m pip install --no-warn-conflicts --upgrade tabulate ibm-watson-openscale ibm-wos-utils~=2.1.1 | tail -n 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "\n",
    "The following inputs are required:\n",
    "\n",
    "1. **IBM_CPD_ENDPOINT:** The URL representing the IBM Cloud Pak for Data service endpoint.\n",
    "2. **IBM_CPD_USERNAME:** IBM Cloud Pak for Data username used to obtain a bearer token.\n",
    "3. **IBM_CPD_PASSWORD:** IBM Cloud Pak for Data password used to obtain a bearer token.\n",
    "4. **HIVE_METASTORE_URI:** Hive Metastore URI to connect to using this notebook\n",
    "5. **KERBERISED_HIVE_YARN_RM_PRINCIPAL:** Yarn Resource Manager Principal (_required only if connecting to Kerberised Hive_)\n",
    "6. **KERBERISED_HIVE_YARN_RM_KEYTAB:** Path to the Yarn Resource Manager KeyTab file on the cluster (_required only if connecting to Kerberised Hive_)\n",
    "7. **KERBERISED_HIVE_METASTORE_PRINCIPAL:** Hive MetaStore Principal (_required only if connecting to Kerberised Hive_)\n",
    "8. **ANALYSIS_INPUT_PARAMETERS:** Analysis Input Parameters to be copied from IBM Watson OpenScale UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM Cloud Pak for Data credentials\n",
    "IBM_CPD_ENDPOINT = \"<The URL representing the IBM Cloud Pak for Data service endpoint.>\"\n",
    "IBM_CPD_USERNAME = \"<IBM Cloud Pak for Data username used to obtain a bearer token.>\"\n",
    "IBM_CPD_PASSWORD = \"<IBM Cloud Pak for Data password used to obtain a bearer token.>\"\n",
    "\n",
    "# Hive Metastore URI to connect to\n",
    "HIVE_METASTORE_URI = \"<Hive Metastore URI>\"\n",
    "\n",
    "# Additional Properties Required for connecting to KERBERISED Hive\n",
    "KERBERISED_HIVE_YARN_RM_PRINCIPAL = \"<Yarn Resource Manager Principal>\"\n",
    "KERBERISED_HIVE_YARN_RM_KEYTAB = \"<Path to the Yarn Resource Manager KeyTab file on the cluster>\"\n",
    "KERBERISED_HIVE_METASTORE_PRINCIPAL = \"<Hive MetaStore Principal>\"\n",
    "\n",
    "\n",
    "# Analysis Input Parameters to be copied from UI\n",
    "# Please make sure that the quotes around the key-values \n",
    "# are correct after copying from UI\n",
    "ANALYSIS_INPUT_PARAMETERS = {\n",
    "    \"data_mart_id\": \"<data_mart_id>\",\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"monitor_instance_id\": \"<monitor_instance_id>\",\n",
    "    \"measurement_id\": \"<measurement_id>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAMART_ID = ANALYSIS_INPUT_PARAMETERS.get(\"data_mart_id\")\n",
    "SUBSCRIPTION_ID = ANALYSIS_INPUT_PARAMETERS.get(\"subscription_id\")\n",
    "MONITOR_INSTANCE_ID = ANALYSIS_INPUT_PARAMETERS.get(\"monitor_instance_id\")\n",
    "MEASUREMENT_ID = ANALYSIS_INPUT_PARAMETERS.get(\"measurement_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import APIClient\n",
    "\n",
    "from ibm_wos_utils.drift.batch.util.constants import ConstraintName\n",
    "from ibm_wos_utils.joblib.utils.analyze_notebook_utils import (\n",
    "    explain_catcat_distribution_constraint,\n",
    "    explain_categorical_distribution_constraint,\n",
    "    explain_catnum_range_constraint, explain_numeric_range_constraint,\n",
    "    get_column_query, get_drift_archive_contents,\n",
    "    get_table_details_from_subscription, show_constraints_by_column,\n",
    "    show_dataframe, show_last_n_drift_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "        .setAppName(\"Analyze Drifted Transactions\")\\\n",
    "        .set(\"spark.hadoop.hive.metastore.uris\", HIVE_METASTORE_URI)\n",
    "\n",
    "# Uncomment the following block if connecting to a Kerberised Hive\n",
    "\"\"\"\n",
    "conf = conf\\\n",
    "        .set(\"spark.hadoop.yarn.resourcemanager.principal\", KERBERISED_HIVE_YARN_RM_PRINCIPAL)\\\n",
    "        .set(\"spark.hadoop.yarn.resourcemanager.keytab\", KERBERISED_HIVE_YARN_RM_KEYTAB)\\\n",
    "        .set(\"spark.hadoop.hive.metastore.kerberos.principal\", KERBERISED_HIVE_METASTORE_PRINCIPAL)\\\n",
    "        .set(\"spark.hadoop.hive.metastore.sasl.enabled\", \"true\")\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = CloudPakForDataAuthenticator(\n",
    "        url=IBM_CPD_ENDPOINT,\n",
    "        username=IBM_CPD_USERNAME,\n",
    "        password=IBM_CPD_PASSWORD,\n",
    "        disable_ssl_verification=True\n",
    "    )\n",
    "wos_client = APIClient(authenticator=authenticator, service_url=IBM_CPD_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not DATAMART_ID or not SUBSCRIPTION_ID:\n",
    "    raise Exception(\"DATAMART_ID and SUBSCRIPTION_ID are required to proceed.\")\n",
    "\n",
    "subscription = wos_client.subscriptions.get(subscription_id=SUBSCRIPTION_ID).result\n",
    "monitor_instance = wos_client.monitor_instances.list(data_mart_id=DATAMART_ID, target_target_id=SUBSCRIPTION_ID, monitor_definition_id=\"drift\").result.monitor_instances[0]\n",
    "model_drift_enabled = monitor_instance.entity.parameters.get(\"model_drift_enabled\", False)\n",
    "data_drift_enabled = monitor_instance.entity.parameters.get(\"data_drift_enabled\", False)\n",
    "\n",
    "if not MONITOR_INSTANCE_ID:\n",
    "    MONITOR_INSTANCE_ID = monitor_instance.metadata.id\n",
    "    \n",
    "drift_archive = wos_client.monitor_instances.download_drift_model(monitor_instance_id=MONITOR_INSTANCE_ID).result.content\n",
    "schema, ddm_properties, constraints_set = get_drift_archive_contents(drift_archive, model_drift_enabled, data_drift_enabled)\n",
    "payload_database_name, _, payload_table_name = get_table_details_from_subscription(subscription, \"payload\")\n",
    "drift_database_name, _, drift_table_name = get_table_details_from_subscription(subscription, \"drift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MEASUREMENT_ID:\n",
    "    print(\"Please pick a measurement to analyze from the following list:\")\n",
    "    \n",
    "show_last_n_drift_measurements(10, wos_client, SUBSCRIPTION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not selected MEASUREMENT_ID so far, please enter a measurement ID\n",
    "# from the above cell's output to analyze.\n",
    "\n",
    "# MEASUREMENT_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MEASUREMENT_ID:\n",
    "    raise Exception(\"MEASUREMENT_ID is required to proceed.\")\n",
    "\n",
    "measurement = wos_client.monitor_instances.measurements.get(measurement_id=MEASUREMENT_ID, monitor_instance_id=MONITOR_INSTANCE_ID).result\n",
    "measurement_data = measurement.entity.sources[0].data\n",
    "MONITOR_RUN_ID = measurement.entity.run_id\n",
    "MONITOR_RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy and drop in data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IBM Watson OpenScale analyzed {} transactions between {} and {} for drift. Here's a summary.\".format(measurement_data[\"transactions_count\"], measurement_data[\"start\"], measurement_data[\"end\"]))\n",
    "\n",
    "if model_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing drop in accuracy.\".format(measurement_data[\"drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))\n",
    "\n",
    "if data_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing drop in data consistency.\".format(measurement_data[\"data_drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))\n",
    "    \n",
    "if model_drift_enabled and data_drift_enabled:\n",
    "    print(\"  - Total {} transactions out of {} transactions are causing both drop in accuracy and drop in data consistency.\".format(measurement_data[\"model_data_drifted_transactions\"][\"count\"], measurement_data[\"transactions_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy - percent bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if model_drift_enabled:\n",
    "    rows_df = pd.DataFrame(measurement_data[\"drifted_transactions\"][\"drift_model_confidence_count\"])\n",
    "    rows_df = rows_df[[\"lower_limit\", \"upper_limit\", \"count\"]]\n",
    "    rows_df.columns = [\"Drift Model Confidence - Lower Limit\", \"Drift Model Confidence - Upper Limit\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in data consistency - feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if data_drift_enabled:\n",
    "    rows_df = pd.Series(measurement_data[\"data_drifted_transactions\"][\"features_count\"])\\\n",
    "                .sort_values(ascending=False).to_frame()\n",
    "    rows_df.reset_index(inplace=True)\n",
    "    rows_df.columns = [\"Feature Column\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of transactions causing drop in accuracy - constraints list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_drift_enabled:\n",
    "    rows_df = pd.Series(measurement_data[\"data_drifted_transactions\"][\"constraints_count\"])\\\n",
    "                .sort_values(ascending=False).to_frame()\n",
    "    rows_df.reset_index(inplace=True)\n",
    "    rows_df.columns = [\"Constraint Name\", \"Violated Transactions Count\"]\n",
    "    display(rows_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts from the drifted transactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_table_df = spark.sql(\"select * from {}.{} where run_id = '{}'\".format(drift_database_name, drift_table_name, MONITOR_RUN_ID))\n",
    "drift_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_wos_utils.joblib.utils.hive_utils import get_table_as_dataframe\n",
    "\n",
    "payload_table_df = get_table_as_dataframe(spark=spark,\n",
    "                                         database_name=payload_database_name,\n",
    "                                         table_name=payload_table_name,\n",
    "                                         columns_to_map=subscription.entity.asset_properties.feature_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Total number of drifted transactions: {}\".format(drift_table_df.count()))\n",
    "print(\"Total number of model drift transactions: {}\".format(drift_table_df.where(\"is_model_drift\").count()))\n",
    "print(\"Total number of data drift transactions: {}\".format(drift_table_df.where(\"is_data_drift\").count()))\n",
    "print(\"Total number of model + data drift transactions: {}\".format(drift_table_df.where(\"is_model_drift\").where(\"is_data_drift\").count()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze transactions that cause a drop in accuracy\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<dl>\n",
    "    <dt>Drop in accuracy</dt>\n",
    "<dd>Estimates the drop in accuracy of the model at runtime. Model accuracy drops if there is an increase in transactions that are similar to those that the model did not evaluate correctly in the training data. This type of drift is calculated for structured binary and multi-class classification models only.</dd>\n",
    "</dl>\n",
    "\n",
    "A drop in either model accuracy or data consistency lead to a negative impact on the business outcomes that are associated with the model and must be addressed by retraining the model.\n",
    "\n",
    "**Tip**: To further analyze transactions, export the `drifted_transactions_df` data frame to one of the supported formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions that cause a drop in data accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy.\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions that cause a drop in accuracy in given range of drift model confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drift Model Confidence Lower Limit\n",
    "dm_conf_lower = 0.5\n",
    "# Drift Model Confidence Upper Limit\n",
    "dm_conf_upper = 1.0\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(drift_table_df.drift_model_confidence.between(dm_conf_lower,dm_conf_upper))\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy where drift model confidence is between {} and {}\".format(count, dm_conf_lower, dm_conf_upper))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse transactions that cause a drop in accuracy and a drop in data consistency\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<dl>\n",
    "    <dt>Drop in accuracy</dt>\n",
    "<dd>Estimates the drop in accuracy of the model at runtime. Model accuracy drops if there is an increase in transactions that are similar to those that the model did not evaluate correctly in the training data. This type of drift is calculated for structured binary and multi-class classification models only.</dd>\n",
    "\n",
    "<dt>Drop in data consistency</dt>\n",
    "<dd>Estimates the drop in consistency of the data at runtime as compared to the characteristics of the data at training time.</dd>\n",
    "</dl>\n",
    "\n",
    "A drop in either model accuracy or data consistency lead to a negative impact on the business outcomes that are associated with the model and must be addressed by retraining the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions that cause a drop in accuracy and a drop in data consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing both drop in accuracy and drop in data consistency\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions causing drop in accuracy and drop in data consistency in given range of drift model confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drift Model Confidence Lower Limit\n",
    "dm_conf_lower = 0.5\n",
    "# Drift Model Confidence Upper Limit\n",
    "dm_conf_upper = 1.0\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_model_drift\")\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .where(drift_table_df.drift_model_confidence.between(dm_conf_lower,dm_conf_upper))\\\n",
    "    .select([\"scoring_id\",\"drift_model_confidence\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in accuracy and drop in data consistency where drift model confidence is between {} and {}\".format(count, dm_conf_lower, dm_conf_upper))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions in the order of drift_model_confidence\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "        .sort([\"drift_model_confidence\"], ascending=False)\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows, priority_columns=[\"drift_model_confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse transactions that cause a drop in data consistency\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<dl>\n",
    "<dt>Drop in data consistency</dt>\n",
    "<dd>Estimates the drop in consistency of the data at runtime as compared to the characteristics of the data at training time.</dd>\n",
    "</dl>\n",
    "\n",
    "A drop in either model accuracy or data consistency lead to a negative impact on the business outcomes that are associated with the model and must be addressed by retraining the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions that cause a drop in data consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .select([\"scoring_id\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are causing drop in data consistency\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions that violate a data constraint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_name = ConstraintName.CATEGORICAL_DISTRIBUTION_CONSTRAINT\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "        .where(\"is_data_drift\")\\\n",
    "        .where(F.col(constraint_name.value).like(\"%1%\"))\\\n",
    "        .select([\"scoring_id\"])\n",
    "\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are violating {}.\".format(count, constraint_name.value))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "    \n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all transactions where a column causes a drop in data consistency\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_query = get_column_query(constraints_set, schema, column=\"<column_name>\")\n",
    "\n",
    "drifted_transactions_df = drift_table_df\\\n",
    "    .where(\"is_data_drift\")\\\n",
    "    .where(filter_query)\\\n",
    "    .select([\"scoring_id\"])\n",
    "count = drifted_transactions_df.count()\n",
    "\n",
    "print(\"Total {} transactions are satisfying the given query.\".format(count))\n",
    "\n",
    "if count:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    drifted_transactions_df = payload_table_df\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"leftsemi\")\\\n",
    "        .join(drifted_transactions_df, [\"scoring_id\"], \"left\")\\\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query all the learnt constraints based on a column name\n",
    "\n",
    "Use the `show_constraints_by_column` method to find all the constraints learnt for a particular column at training time. The constraint ids shown in the cell output can be used to explain the corresponding constraint in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_constraints_by_column(constraints_set, \"<column_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain categorical distribution constraint violations\n",
    "\n",
    "Explains categorical distribution constraint violations given a constraint id. To retrieve the constraint ID, run [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_categorical_distribution_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain numeric range constraint violations\n",
    "\n",
    "Explains numeric range constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_numeric_range_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain cat-numeric range constraint violations\n",
    "\n",
    "Explains cat-numeric range constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_catnum_range_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain cat-cat distribution constraint violations\n",
    "\n",
    "Explains cat-cat distribution constraint violations given a constraint id. The constraint id can be gotten by running [this cell](#Query-all-the-learnt-constraints-based-on-a-column-name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "constraint_id = \"<constraint_id>\"\n",
    "\n",
    "drifted_transactions_df = explain_catcat_distribution_constraint(drifted_transactions_df=drift_table_df,\n",
    "                              payload_table_df=payload_table_df,\n",
    "                              constraints_set=constraints_set,\n",
    "                              schema=schema,\n",
    "                              constraint_id=constraint_id)\n",
    "\n",
    "\n",
    "if drifted_transactions_df:\n",
    "    num_rows = 10\n",
    "    print(\"Showing {} such transactions.\".format(num_rows))\n",
    "\n",
    "    show_dataframe(drifted_transactions_df, num_rows=num_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
