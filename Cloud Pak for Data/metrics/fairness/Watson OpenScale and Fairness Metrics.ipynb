{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9022bf7",
   "metadata": {},
   "source": [
    "# Watson OpenScale Fairness Metrics and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1fe6a",
   "metadata": {},
   "source": [
    "# 1. Introduction <a name=\"introduction\"></a>\n",
    "The notebook will train a German Credit Risk model, compute Fairness Metrics **Statistical Parity Difference** and **Smoothed Empirical Differential** on the model prediction and then show how **Fair Score Transformer** can be used to transform the model output for fair prediction.<br/>\n",
    "\n",
    "This document includes below sections, you will need `edit` and `restart` notebook kernel in **Setup** section.\n",
    "\n",
    "- [1.Introduction](#introduction)\n",
    "- [2.Setup](#setup)\n",
    "- [3.Model building and evaluation](#model)\n",
    "- [4.OpenScale configuration](#openscale)\n",
    "- [5.Compute Statistical Parity Difference with Original Scores](#spd)\n",
    "- [6.Compute Smoothed Empirical Differential](#sed)\n",
    "- [7.Fair Score Transformer](#fst)\n",
    "- [8.Compute Statistical Parity Difference with Transformed Scores](#spd2)\n",
    "\n",
    "**Note:** This notebook should be run using with **Python 3.10.x** runtime. It requires service credentials for the following services:\n",
    "  * Watson OpenScale <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee20870",
   "metadata": {},
   "source": [
    "## 2. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cd0e6",
   "metadata": {},
   "source": [
    "### 2.1 Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642752ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fe610",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1\n",
    "!pip install --upgrade ibm-metrics-plugin --no-cache | tail -n 1\n",
    "!pip install --upgrade pyspark==3.3.1 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea48c8",
   "metadata": {},
   "source": [
    "#### Action: restart the kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b5c9f",
   "metadata": {},
   "source": [
    "### 2.2 Configure credentials\n",
    "\n",
    "Provide your IBM Watson OpenScale credentials in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "WOS_CREDENTIALS = {\n",
    "    \"url\": \"<cluster-url>\",\n",
    "    \"username\": \"<username>\",\n",
    "    \"password\": \"<password>\",\n",
    "    \"instance_id\": \"<openscale instance id>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88aeac",
   "metadata": {},
   "source": [
    "### 2.3 Run the notebook\n",
    "\n",
    "&ensp;&ensp;&ensp;At this point, the notebook is ready to run. You can either run the cells one at a time, or click the **Kernel** option above and select **Restart and Run All** to run all the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d603e26",
   "metadata": {},
   "source": [
    "## 3. Model building and evaluation <a name=\"model\"></a>\n",
    "&ensp;&ensp;&ensp;In this section you will learn how to train sklearn model, run prediction and evaluate its output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586be7f",
   "metadata": {},
   "source": [
    "### 3.1 Load the training data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6248920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm german_credit_data_biased_training.csv\n",
    "!wget https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/Cloud%20Pak%20for%20Data/WML/assets/data/credit_risk/german_credit_data_biased_training.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.read_csv(\"german_credit_data_biased_training.csv\", sep=\",\", header=0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465e50d",
   "metadata": {},
   "source": [
    "### 3.2 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, brier_score_loss\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b06713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x == \"Risk\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0541723",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"Risk\"] = data_df[\"Risk\"].apply(lambda x: convert(x))\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382075b2",
   "metadata": {},
   "source": [
    "### 3.3 Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17253f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70c272",
   "metadata": {},
   "source": [
    "### 3.4 Create a model\n",
    "&ensp;&ensp;&ensp;Preparing the pipeline. In this step you will encode target column labels into numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_idx = np.s_[0:-1]\n",
    "#all_records_idx = np.s_[:]\n",
    "first_record_idx = np.s_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_fields = [type(fld) is str for fld in train_data.iloc[first_record_idx, features_idx]]\n",
    "ct = ColumnTransformer([(\"ohe\", OneHotEncoder(), list(np.array(train_data.columns)[features_idx][string_fields]))])\n",
    "clf_linear = SGDClassifier(loss='log', penalty='l2', max_iter=1000, tol=1e-5)\n",
    "pipeline_linear = Pipeline([('ct', ct), ('clf_linear', clf_linear)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc10078",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a095655",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model = pipeline_linear.fit(train_data.drop('Risk', axis=1), train_data.Risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f497b05",
   "metadata": {},
   "source": [
    "### 3.5 Evaluate the model\n",
    "&ensp;&ensp;&ensp;Run the model to get predict class labels and probability estimates for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = risk_model.predict(test_data.drop('Risk', axis=1))\n",
    "y_probs = risk_model.predict_proba(test_data.drop('Risk', axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedb1e2",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Compute accuracy and loss with model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc = accuracy_score(test_data['Risk'], y_preds)\n",
    "print(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c806db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_brier = brier_score_loss(test_data['Risk'], y_probs)\n",
    "print(lr_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc523e5",
   "metadata": {},
   "source": [
    "## 4. OpenScale configuration <a name=\"openscale\"></a>\n",
    "&ensp;&ensp;&ensp;The notebook will now import the necessary libraries and set up a Python OpenScale client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_openscale import APIClient as OpenScaleAPIClient\n",
    "from ibm_cloud_sdk_core.authenticators import CloudPakForDataAuthenticator\n",
    "\n",
    "authenticator = CloudPakForDataAuthenticator(\n",
    "    url=WOS_CREDENTIALS[\"url\"],\n",
    "    username=WOS_CREDENTIALS[\"username\"],\n",
    "    password=WOS_CREDENTIALS[\"password\"],\n",
    "    disable_ssl_verification=True\n",
    ")\n",
    "\n",
    "client = OpenScaleAPIClient(\n",
    "    service_url=WOS_CREDENTIALS['url'],\n",
    "    service_instance_id=WOS_CREDENTIALS[\"instance_id\"],\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77101c",
   "metadata": {},
   "source": [
    "## 5. Compute Statistical Parity Difference with Original Scores <a name=\"spd\"></a>\n",
    "\n",
    "**Statistical Parity Difference** is a fairness metric that can be used to describe the fairness for the model predictions.\n",
    "It is the difference between the ratio of favourable outcomes in unprivileged and privileged groups. It can\n",
    "be computed from either the input dataset or the dataset output from a classifier (predicted dataset). A value\n",
    "of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group, and a value greater than 0 implies higher benefit for the unprivileged group.<br>\n",
    "$$ð‘ƒ(ð‘Œ=1|ð·=unprivileged)âˆ’ð‘ƒ(ð‘Œ=1|ð·=privileged)$$\n",
    "\n",
    "Take the German credit risk datasets as example, if user set\n",
    "+ privileged group as Sex=\"male\" \n",
    "+ unprivileged group as Sex=\"female\"\n",
    "\n",
    "and set\n",
    "+ favourable label as Risk=\"No Risk\"\n",
    "+ unfavourable label as Risk=\"Risk\"\n",
    "\n",
    "then, the SPD result \n",
    "+ spd > 0 means the unpriviliage group Sex=\"female\" has higher rate to be marked as favourable label \"No Risk\" than priviliage group Sex=\"male\".\n",
    "+ spd = 0 means the unpriviliage group Sex=\"female\" has same rate to be marked as favourable label \"No Risk\" with priviliage group Sex=\"male\".\n",
    "+ spd < 0 means the unpriviliage group Sex=\"female\" has lower rate to be marked as favourable label \"No Risk\" than priviliage group Sex=\"male\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de1f33",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Add a new column `pred` with value of the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"pred\"] = y_preds\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a184a",
   "metadata": {},
   "source": [
    "### 5.1 Prepare input to compute Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4892d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99889ec",
   "metadata": {},
   "source": [
    "Setup configuration to compute *Statistical Parity Difference*,<br/>\n",
    "\n",
    "Configure label and problem type in the overall section.\n",
    "- **problem_type(str)**: `binary` and `multi-classification` is supported.\n",
    "- **label_column(str)**: Column name of label in the data frame\n",
    "\n",
    "Inside `fairness` as below, there are three sections which is required to configure.\n",
    "- **metrics_configuration(dict)**: Configure *Statistical Parity Difference* as one of the metrics with name `FairnessMetricType.SPD.value`, and it requires a `features` property to describe which features the metric will be computed upon. *Statistical Parity Difference* is supported to run with individual features (eg. `[[\"a\"],[\"b\"]]`), but not suppored to run with intersectional features (eg. `[[\"a\", \"b\"]]`).\n",
    "\n",
    "- **protected_attributes(list)**: Describe privileged group defintion for features upon which this metric will be computed. Configure each feature with below information:\n",
    "  - feature(str): Name of the feature, which should be same as configured in `features` of `metrics_configuration` section.\n",
    "  - reference_group(list): List of feature values which make a sample privileged. \n",
    "\n",
    "- **favourable_label(list)**: A list of favourable labels or outcomes of the model.\n",
    "\n",
    "\n",
    "**Note** that `label_column` used here is the new added `pred` column.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_metrics_plugin.common.utils.constants import FairnessMetricType\n",
    "\n",
    "spd_config = {}\n",
    "spd_config['configuration'] = {\n",
    "            \"problem_type\": \"binary\",\n",
    "            \"label_column\": \"pred\",\n",
    "            \"fairness\": {\n",
    "                         \"metrics_configuration\": {\n",
    "                                    FairnessMetricType.SPD.value: {\n",
    "                                        \"features\": [[\"Sex\"]]\n",
    "                                    }\n",
    "                        },\n",
    "                        \"protected_attributes\": [\n",
    "                            {\n",
    "                                \"feature\": \"Sex\",\n",
    "                                \"reference_group\": [\"male\"]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"favourable_label\": [\"1\"]          \n",
    "            }  \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa441a",
   "metadata": {},
   "source": [
    "### 5.2 Compute Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681cf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = client.ai_metrics.compute_metrics(spark=spark, configuration=spd_config, data_frame=sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84514f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb77ecf",
   "metadata": {},
   "source": [
    "## 6. Compute Smoothed Empirical Differential Fairness <a name=\"sed\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcde75",
   "metadata": {},
   "source": [
    "**Smoothed Empirical Differential(SED)** is a fairness metric that can be used to describe the fairness for the model predictions. It is used to quantify the differential in the probability of favorable/unfavorable outcomes between intersecting groups divided by features. All intersecting groups are equal, there is no unprivileged or privileged groups. \n",
    "\n",
    "SED value is the minimum ratio of Dirichlet smoothed probability of favorable and unfavorable outcomes between different intersecting groups in the dataset. Its value is between 0 and 1, excluding 0 and 1. The bigger, the better.\n",
    "\n",
    "Take the German credit risk datasets as example, assume:\n",
    "\n",
    "+ the favorable outcomes of label column is \"No Risk\",\n",
    "+ the unfavorable outcomes of label column is \"Risk\".\n",
    "\n",
    "if user divide dataset by *feature \"Sex\"*ï¼Œthere will be two intersecting groups:\n",
    "+ intersecting group Sex=\"male\" \n",
    "+ intersecting group Sex=\"female\"\n",
    "\n",
    "and assume:\n",
    "\n",
    "+ the Dirichlet smoothed probability of favorable outcomes \"No Risk\" in intersecting group \"Sex\"=\"male\" is 0.2\n",
    "+ the Dirichlet smoothed probability of unfavorable outcomes \"Risk\" in intersecting group \"Sex\"=\"male\" is 0.8\n",
    "+ the Dirichlet smoothed probability of favorable outcomes \"No Risk\" in intersecting group \"Sex\"=\"female\" is 0.4\n",
    "+ the Dirichlet smoothed probability of unfavorable outcomes \"Risk\" in intersecting group \"Sex\"=\"female\" is 0.6\n",
    "\n",
    "then, calculate the label differential between intersecting groups (*Note that it always chooses the smaller one as the numerator or the bigger one as the denominator*): \n",
    "\n",
    "+ the favorable outcomes' differential between intersecting group \"Sex\"=\"male\" and \"Sex\"=\"female\" will be 0.2/0.4=0.5\n",
    "+ the unfavorable outcomes' differential between intersecting group \"Sex\"=\"male\" and \"Sex\"=\"female\" will be 0.6/0.8=0.75\n",
    "\n",
    "then, calculate the differential between intersecting groups:\n",
    "+ the differential between intersecting group \"Sex\"=\"male\" and \"Sex\"=\"female\" will be min(0.5, 0.75)=0.5\n",
    "\n",
    "Since there are only two intersecting groups, so,\n",
    "\n",
    "+ the final differentials of dataset will be 0.5.\n",
    "\n",
    "*References: James R. Foulds, Rashidul Islam, Kamrun Naher Keya, Shimei Pan, \"An Intersectional Definition of Fairness\", Department of Information Systems, University of Maryland, Baltimore County, USA*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c77968",
   "metadata": {},
   "source": [
    "### 6.1 Smoothed Empirical Differential Configuration\n",
    "\n",
    "Configure label and problem type in the overall section.\n",
    "- **problem_type(str)**: `binary` and `multi-classification` is supported.\n",
    "- **label_column(str)**: Column name of label in the data frame.\n",
    "\n",
    "Inside `fairness` as below, there are three sections which is required to configure.\n",
    "- **metrics_configuration(dict)**: Configure *Smoothed Empirical Differential* as one of the metrics with name `FairnessMetricType.SED.value`, and it requires a `features` property to describes which features the metric will be computed upon. *Smoothed Empirical Differential* is supported to run with individual features (eg. `[[\"a\"],[\"b\"]]`) and with intersectional features (eg. `[[\"a\", \"b\"]]`).\n",
    "\n",
    "- **protected_attributes(list)**: Describe protected features upon which this metric will be computed. Configure each feature with such information:\n",
    "  - feature(str): Name of the feature, which should be same as configured in `features` of `metrics_configuration` section.\n",
    "\n",
    "- **favourable_label(list)**: A list of favourable labels or outcomes of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sed_config = {}\n",
    "sed_config['configuration'] = {\n",
    "            \"problem_type\": \"binary\",\n",
    "            \"label_column\": \"pred\",\n",
    "            \"fairness\": {\n",
    "                         \"metrics_configuration\": {\n",
    "                                    FairnessMetricType.SED.value: {\n",
    "                                        \"features\": [[\"Sex\"]],\n",
    "                                    }\n",
    "                        },\n",
    "                        \"protected_attributes\": [\n",
    "                            {\n",
    "                                \"feature\": \"Sex\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"favourable_label\": [\"1\"]\n",
    "            }  \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69018db",
   "metadata": {},
   "source": [
    "### 6.2 Compute Smoothed Empirical Differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = client.ai_metrics.compute_metrics(spark=spark, configuration=sed_config, data_frame=sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784ae0f",
   "metadata": {},
   "source": [
    "## 7. Fair Score Transformer <a name=\"fst\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f059d12",
   "metadata": {},
   "source": [
    "**Fair Score Transformer** can be used as post-processing technique that transforms probability estimates ( or scores) of `probabilistic binary classication` model with respect to fairness goals like statistical parity or equalized odds. To use **Fair Score Transformer** in OpenScale, you need first train a **Fair Score Transformer** and then use it to transform scores.\n",
    "\n",
    "*References: D. Wei, K. Ramamurthy, and F. Calmon, \"Optimized Score Transformation for Fair Classification\", International Conference on Artificial Intelligence and Statistics, 2020.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e0105",
   "metadata": {},
   "source": [
    "### 7.1 Train Fair Score Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392b2b5",
   "metadata": {},
   "source": [
    "To train Fair Score Transformer, at least two columns is required in the dataframe, one is the probability estimates from the trained classification model and another is the corresponding protected attributes.\n",
    "\n",
    "**Note**\n",
    "The `label` column is not required to train the transformer but required to compute accuray with the trained transformer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff65c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability estimates\n",
    "r = y_probs.tolist()\n",
    "#protected attributes\n",
    "A = test_data[\"Sex\"].tolist()\n",
    "#label values\n",
    "y = test_data[\"Risk\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd254b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_name = \"r\"\n",
    "A_name = \"A\"\n",
    "y_name = \"y\"\n",
    "data = pd.DataFrame({\n",
    "    r_name: r,\n",
    "    A_name: A,\n",
    "    y_name: y\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeab993",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(data)\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac71ce7",
   "metadata": {},
   "source": [
    "### 7.2 Fair Score Transformer Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b1fc0",
   "metadata": {},
   "source": [
    "Setup configuration to fit **Fair Score Transformer**. Inside `metrics_configuration` as below, specify the name of the transformer with `FairnessMetricType.FST.value`. To configure it, you need to provide `params` and `features` information as below. This notebook will transform scores with respect to the **Statistical Parity Difference** fairness goal (set `criteria` as `MSP`).\n",
    "\n",
    "- **params**: Parameters of Fair Score Transformer\n",
    "  - epsilon (float): Bound on mean statistical parity or mean equalized odds.\n",
    "  - criteria (str): Optimize for mean statistical parity (\"MSP\") or mean equalized odds (\"MEO\").\n",
    "  - Aprobabilistic (bool): Indicator of whether actual protected attribute values (False) or probabilistic estimates (True) are provided. Default False.\n",
    "  - iterMax (float): Maximum number of ADMM iterations. Default 1e3.\n",
    "- **features**: Columns definition in the dataframe\n",
    "  - probabilities: Column name of probability estimates.\n",
    "  - protected: Column name of protected attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\"probabilities\": r_name, \"protected\": A_name}\n",
    "configuration = dict()\n",
    "configuration[\"configuration\"] = {\n",
    "    \"fairness\": {\n",
    "        \"metrics_configuration\": {\n",
    "            FairnessMetricType.FST.value: {\n",
    "                \"params\": {\n",
    "                    \"epsilon\": 0.01,\n",
    "                    \"criteria\": \"MSP\",\n",
    "                    \"Aprobabilistic\": False,\n",
    "                    \"iterMax\": 1e3\n",
    "                },\n",
    "                \"features\": columns\n",
    "            }\n",
    "        }     \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d135af0",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Fit fair score transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c150cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst = client.ai_metrics.fit_transformer(spark=spark, configuration=configuration, data_frame=sparkDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01a000",
   "metadata": {},
   "source": [
    "### 7.3 Transform scores with Fair Score Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d2efd",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Trained transformer can be used to compute new probability estimates and it requires the exactly same columns as fitting phase.<br/> \n",
    "\n",
    "&ensp;&ensp;&ensp;**Note:** No matter what column name is used for the existing probability estimates, the new probability estimates column will be named as **r_transformed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349eedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df = fst.predict_proba(spark, sparkDF, columns, keep_cols=[y_name])\n",
    "probs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584f5d6",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Trained transformer can also be used to compute new class labels based on transformed probability estimates, and it requires the exactly same columns as fitting phase. \n",
    "\n",
    "&ensp;&ensp;&ensp;**Note:** No matter what column name is used for the `label` column, the new class labels column will be named as **r_transformed_thresh**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = fst.predict(spark, sparkDF, columns, keep_cols=[y_name])\n",
    "preds_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b886c",
   "metadata": {},
   "source": [
    "### 7.4 Evaluate with transformed scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f806b2",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;To compute accuray based on transformed probability estimates with the trained transformer, you need to specify the `label` column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = columns.copy()\n",
    "score_columns[\"label\"] = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b78191",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = fst.score(spark, sparkDF, score_columns)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232df27d",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Or you can get the predict class labels from transformer and compute accuracy directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62aa46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_preds = preds_df.select(\"r_transformed_thresh\").toPandas().values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a544d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc = accuracy_score(test_data['Risk'], y_preds)\n",
    "fst_lr_acc = accuracy_score(test_data['Risk'], fst_preds)\n",
    "print(\"Original model accuray: {}\".format(lr_acc))\n",
    "print(\"Accuracy with transformed predicts: {}\".format(fst_lr_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817ba82",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;You can compute loss with transformed probability estimates too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_probs = probs_df.select(\"r_transformed\").toPandas().values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b41ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_lr_brier = brier_score_loss(test_data['Risk'], fst_probs)\n",
    "lr_brier = brier_score_loss(test_data['Risk'], y_probs)\n",
    "print(\"Original model loss: {}\".format(lr_acc))\n",
    "print(\"Loss with transformed scores: {}\".format(fst_lr_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f54db1",
   "metadata": {},
   "source": [
    "## 8. Compute Statistical Parity Difference  with Transformed Scores <a name=\"spd2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed6d34",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Compute **Statistical Parity Difference** based on the transformed class labels. <br/>\n",
    "&ensp;&ensp;&ensp;Add a new column `pred_transformed` with value of the transformed class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"pred_transformed\"] = fst_preds\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf2b91",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Prepare input to compuate **Statistical Parity Difference**. <br/>\n",
    "&ensp;&ensp;&ensp;All will be the same as before except the `label_column` will be swithed to use the transformed class labels column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_config['configuration']['label_column'] = \"pred_transformed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddb14f",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;Compute **Statistical Parity Difference** again and it should be improved compared with data before transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = client.ai_metrics.compute_metrics(spark=spark, configuration=spd_config, data_frame=sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da63ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
