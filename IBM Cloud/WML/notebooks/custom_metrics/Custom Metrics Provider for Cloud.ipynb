{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "130546d4-d72c-46f1-a506-246ace42ad56"
   },
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6d32920-c6f8-4df6-91a7-925640a9920a"
   },
   "source": [
    "# Working with a custom metrics provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab7073b7-baf8-48ba-b9fb-e39e497596aa"
   },
   "source": [
    "This notebook demonstrates how to configure the custom monitor and custom metrics deployment by using IBM Watson OpenScale. This notebook should be run in a Watson Studio project, using **IBM Runtime 22.1 on Python 3.9 XS** runtime environment. **If you are viewing this in Watson Studio and do not see the required runtime env in the upper right corner of your screen, please update the runtime now.**. It requires service credentials for the following services:\n",
    "  * Watson OpenScale\n",
    "  * Watson Machine Learning\n",
    "\n",
    "  \n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "  1. [Set up your environment](#setup)\n",
    "  1. [Create the custom metrics provider - python function.](#provider)\n",
    "  1. [Register the custom metrics provider and create a deployment](#deployment)\n",
    "  1. [Configure Watson OpenScale](#config)\n",
    "  1. [Create the integrated system for the custom metrics provider](#custom)\n",
    "  1. [Set up the custom monitor definition and instance](#monitor)\n",
    "  1. [Run the custom monitor](#run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c3c7fcc-8701-45f3-ab34-af1b093b590c"
   },
   "source": [
    "## 1. Set up your environment <a name=\"setup\"></a>\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "8b4212ee-4116-4a47-a586-f0997df1ec53"
   },
   "source": [
    "### Install the  `ibm-watson-machine-learning` and `ibm-watson-openscale` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eb75b61-93b5-4139-96d5-fae98c8c9173",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson-machine-learning   | tail -n 1\n",
    "!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd55a125-14e4-440c-a2ee-c58813d75f9e"
   },
   "source": [
    "### Action: restart the kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2abe81f0-cd47-4fe0-9b5c-505712c17752",
    "scrolled": true
   },
   "source": [
    "### Credentials for IBM Cloud\n",
    "To authenticate, in the following code boxes, replace the sample data with your own credentials. Get the information from your system administrator or through the IBM Cloud dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37f4abdea5d34ffd87dc6716b8ff989c"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Paste your credentials into the following section and then run this cell.\n",
    "############################################################################################\n",
    "CLOUD_API_KEY = \"<Your Cloud IAM API Key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAM_URL=\"https://iam.ng.bluemix.net/oidc/token\"\n",
    "OPENSCALE_API_URL = \"https://api.aiopenscale.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f499be0-0ed7-46c9-9bf4-753c2529525f"
   },
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": CLOUD_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1df5c3f-4532-405b-a14d-6018bbca41e2"
   },
   "source": [
    "\n",
    "### Enter your Watson OpenScale GUID\n",
    "\n",
    "For most systems, the default GUID is already entered for you. You would only need to update this particular entry if the GUID was changed from the default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a569d1f-2989-4f22-892f-3983f398ffce"
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Paste your IBM Watson OpenScale DataMart Id in the following field and then run this cell.\n",
    "####################################################################\n",
    "WOS_GUID=\"<IBM Watson OpenScale DataMart Id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cce9ad8f9bf4fb095680d91ad38b2a0"
   },
   "source": [
    "### Define the Watson OpenScale subscription to which the custom metrics have to be sent.\n",
    "\n",
    "Create a subscription from the Watson Openscale UI or SDK to configure custom metrics. You can configure custom metrics for the subscriptions that have predefined monitors, such as fairness, quality, or drift or without predefined monitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48519343224a4ca582ee954c3a0e6605"
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Paste your Subscription in the following field and then run this cell.\n",
    "####################################################################\n",
    "\n",
    "subscription_id = \"<Subscription Id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67584279ed094267bbcd98224d6d0b2f"
   },
   "source": [
    "### Python function details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cecf1dd131db4b378dd4217f998ef3f0"
   },
   "outputs": [],
   "source": [
    "PYTHON_FUNCTION_NAME = \"Custom Metrics Provider Function\"\n",
    "DEPLOYMENT_NAME = \"Custom Metrics Provider Deployment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee9bcd7ca4e9412881c93ecab30afdb5"
   },
   "source": [
    "### OpenScale Custom Metrics Provider name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7209da78bf914a0c942d2cbff79d72cb"
   },
   "outputs": [],
   "source": [
    "CUSTOM_METRICS_PROVIDER_NAME = \"Custom Metrics Provider\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eebc2a4d71204bf1ac53ab1f5e92318c"
   },
   "source": [
    "### OpenSale Custom Monitor name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eea74c49408144aaa531949b018cdc5d"
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# UPDATE your custom monitor name in the following field and then run this cell.\n",
    "####################################################################\n",
    "CUSTOM_MONITOR_NAME = \"Sample Model Performance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51dab6f2e41045f38ead783922814c0a"
   },
   "source": [
    "## 2. Create the custom metrics provider - Python function <a name=\"provider\"></a>\n",
    "\n",
    "The Python function receives the required variables, such as the `datamart_id`, `monitor_instance_id`, `monitor_id`, `monitor_instance_parameters` and `subscription_id` from the Watson OpenScale service when it is invoked by the custom monitor. \n",
    "\n",
    "In the Python function, add your own logic to compute the custom metrics in the `get_metrics` method, publish the metrics to the Watson Openscale service and update the status of the run to the `finished` state in the custom monitor instance run.\n",
    "\n",
    "Update the `WOS_CREDENTIALS` in the Python function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7ec6979ce114f528246ba99e8619090"
   },
   "outputs": [],
   "source": [
    "#wml_python_function\n",
    "parms = {\n",
    "        \"url\": OPENSCALE_API_URL,\n",
    "        \"iam_url\": IAM_URL,\n",
    "        \"apikey\": CLOUD_API_KEY\n",
    "    }\n",
    "def custom_metrics_provider(parms = parms):\n",
    "    \n",
    "    import json\n",
    "    import requests\n",
    "    import base64\n",
    "    from requests.auth import HTTPBasicAuth\n",
    "    import time\n",
    "    import uuid\n",
    "    import datetime\n",
    "    \n",
    "    headers = {}\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "    headers[\"Accept\"] = \"application/json\"\n",
    "\n",
    "    def get_access_token():\n",
    "        headers={}\n",
    "        headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n",
    "        headers[\"Accept\"] = \"application/json\"\n",
    "        auth = HTTPBasicAuth(\"bx\", \"bx\")\n",
    "        data = {\n",
    "            \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n",
    "            \"apikey\": parms[\"apikey\"]\n",
    "        }\n",
    "        response = requests.post(parms[\"iam_url\"], data=data, headers=headers, auth=auth)\n",
    "        json_data = response.json()\n",
    "        access_token = json_data['access_token']\n",
    "        return access_token    \n",
    "    \n",
    "    \n",
    "    def get_feedback_data(access_token, data_mart_id, feedback_dataset_id):\n",
    "        json_data = None\n",
    "        if feedback_dataset_id is not None:\n",
    "            headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n",
    "            DATASETS_STORE_RECORDS_URL = parms[\"url\"] + \"/openscale/{0}/v2/data_sets/{1}/records?limit={2}&format=list\".format(data_mart_id, feedback_dataset_id, 100)\n",
    "            response = requests.get(DATASETS_STORE_RECORDS_URL, headers=headers, verify=False)\n",
    "            json_data = response.json()\n",
    "            return json_data\n",
    "    \n",
    "    #Update the run status to Finished in the Monitor Run\n",
    "    def update_monitor_run_status(base_url, access_token, custom_monitor_instance_id, run_id, status, error_msg = None):\n",
    "        monitor_run_url = base_url + '/v2/monitor_instances/' + custom_monitor_instance_id + '/runs/'+run_id\n",
    "        completed_timestamp = datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        patch_payload  = []\n",
    "        base_path = \"/status\"\n",
    "        \n",
    "        patch_payload.append(get_patch_request_field(base_path, \"state\", status))\n",
    "        patch_payload.append(get_patch_request_field(base_path, \"completed_at\", completed_timestamp))\n",
    "        if error_msg != None:\n",
    "            error_json = get_error_json(error_msg)\n",
    "            patch_payload.append(get_patch_request_field(base_path, \"failure\", error_json))\n",
    "        \n",
    "        headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n",
    "        response = requests.patch(monitor_run_url, headers=headers, json = patch_payload, verify=False)\n",
    "        monitor_run_response = response.json()\n",
    "        return response.status_code, monitor_run_response\n",
    "    \n",
    "    def get_error_json(error_message):\n",
    "        trace = str(uuid.uuid4())\n",
    "        error_json = {\n",
    "            'trace': trace,\n",
    "            'errors': [{\n",
    "                'code': \"custom_metrics_error_code\",\n",
    "                'message': str(error_message)\n",
    "            }]\n",
    "        }\n",
    "        return error_json\n",
    "    \n",
    "    def get_patch_request_field(base_path, field_name, field_value, op_name=\"replace\"):\n",
    "        field_json = {\n",
    "            \"op\": op_name,\n",
    "            \"path\": \"{0}/{1}\".format(base_path, field_name),\n",
    "            \"value\": field_value\n",
    "        }\n",
    "        return field_json\n",
    "    \n",
    "    #Add your code to compute the custom metrics. \n",
    "    def get_metrics(access_token, data_mart_id, subscription_id, feedback_dataset_id):\n",
    "        #Add the logic here to compute the metrics. Use the below metric names while creating the custom monitor definition\n",
    "        json_data = get_feedback_data(access_token, data_mart_id, feedback_dataset_id)\n",
    "        gender_less40_fav_prediction_ratio = 0\n",
    "        if json_data is not None:\n",
    "            fields = json_data['records'][0]['fields']\n",
    "            values = json_data['records'][0]['values']\n",
    "            import pandas as pd\n",
    "            feedback_data = pd.DataFrame(values, columns = fields)\n",
    "            female_less40_fav_prediction = len(feedback_data.query('Sex == \\'female\\' & Age <= 40 & Risk == \\'No Risk\\''))\n",
    "            male_less40_fav_prediction = len(feedback_data.query('Sex == \\'male\\' & Age <= 40 & Risk == \\'No Risk\\''))\n",
    "            gender_less40_fav_prediction_ratio = female_less40_fav_prediction / male_less40_fav_prediction\n",
    "        \n",
    "        #Remove the tag(\"region\": \"us-south\") in below metrics while publishing the metric values to Openscale Datamart \n",
    "        #if the custom monitor definition is not created with tags\n",
    "        metrics = {\"specificity\": 1.2, \"sensitivity\": 0.85, \"gender_less40_fav_prediction_ratio\": gender_less40_fav_prediction_ratio, \"region\": \"us-south\"}\n",
    "        #metrics = {\"specificity\": 1.2, \"sensitivity\": 0.85, \"gender_less40_fav_prediction_ratio\": gender_less40_fav_prediction_ratio}\n",
    "                \n",
    "        return metrics\n",
    "        \n",
    "        \n",
    "    # Publishes the Custom Metrics to OpenScale\n",
    "    def publish_metrics(base_url, access_token, data_mart_id, subscription_id, custom_monitor_id, custom_monitor_instance_id, custom_monitoring_run_id, feedback_dataset_id, timestamp):\n",
    "        # Generate an monitoring run id, where the publishing happens against this run id\n",
    "        custom_metrics = get_metrics(access_token, data_mart_id, subscription_id, feedback_dataset_id)\n",
    "        measurements_payload = [\n",
    "                  {\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"run_id\": custom_monitoring_run_id,\n",
    "                    \"metrics\": [custom_metrics]\n",
    "                  }\n",
    "                ]\n",
    "        headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n",
    "        measurements_url = base_url + '/v2/monitor_instances/' + custom_monitor_instance_id + '/measurements'\n",
    "        response = requests.post(measurements_url, headers=headers, json = measurements_payload, verify=False)\n",
    "        published_measurement = response.json()\n",
    "     \n",
    "        return response.status_code, published_measurement\n",
    "        \n",
    "    \n",
    "    def publish( input_data ):\n",
    "\n",
    "        timestamp = datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        \n",
    "        payload = input_data.get(\"input_data\")[0].get(\"values\")\n",
    "        data_mart_id = payload['data_mart_id']\n",
    "        subscription_id = payload['subscription_id']\n",
    "        custom_monitor_id = payload['custom_monitor_id']\n",
    "        custom_monitor_instance_id = payload['custom_monitor_instance_id']\n",
    "        custom_monitor_instance_params  = payload['custom_monitor_instance_params']\n",
    "        custom_monitor_run_id = payload['custom_monitor_run_id']\n",
    "        payload_dataset_id = payload.get('payload_dataset_id')\n",
    "        feedback_dataset_id = payload.get('feedback_dataset_id')\n",
    "\n",
    "        base_url = parms['url'] + '/openscale' + '/' + data_mart_id\n",
    "        access_token = get_access_token()\n",
    "        \n",
    "        published_measurements = []\n",
    "        error_msgs = []\n",
    "        run_status = \"finished\"\n",
    "        error_msg = None\n",
    "        \n",
    "        try:\n",
    "            status_code, published_measurement = publish_metrics(base_url, access_token, data_mart_id, subscription_id, custom_monitor_id, custom_monitor_instance_id, custom_monitor_run_id, feedback_dataset_id, timestamp)\n",
    "            if int(status_code) in [200, 201, 202]:\n",
    "                published_measurements.append(published_measurement)\n",
    "            else:\n",
    "                run_status = \"error\"\n",
    "                error_msg = published_measurement\n",
    "                error_msgs.append(error_msg)\n",
    "                    \n",
    "            status_code, response = update_monitor_run_status(base_url, access_token, custom_monitor_instance_id, custom_monitor_run_id, run_status, error_msg)\n",
    "            if not int(status_code) in [200, 201, 202]:\n",
    "                error_msgs.append(response)\n",
    "                \n",
    "        except Exception as ex:\n",
    "            error_msgs.append(str(ex))\n",
    "        if len(error_msgs) == 0:\n",
    "            response_payload = {\n",
    "                \"predictions\" : [{ \n",
    "                    \"values\" : published_measurements\n",
    "                }]\n",
    "\n",
    "            }\n",
    "        else:\n",
    "            response_payload = {\n",
    "                \"predictions\":[],\n",
    "                \"errors\": error_msgs\n",
    "            }\n",
    "        \n",
    "        return response_payload\n",
    "        \n",
    "    return publish\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47683fe263944fafb25f856214b1ec70"
   },
   "source": [
    "## 3. Register the custom metrics provider and create a deployment. <a name=\"deployment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8948f9b03e849d9aba1f74d7e7830c3"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_client = APIClient(WML_CREDENTIALS)\n",
    "wml_client.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15601ba7120748b9902f57ca107067b6"
   },
   "outputs": [],
   "source": [
    "wml_client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74c4db6b630d424a895daadee635b4c4"
   },
   "outputs": [],
   "source": [
    "space_id = \"<Update your space id>\" #update your space id\n",
    "wml_client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06bb0042-e9b4-432a-9ada-8deb8c7f64dc"
   },
   "source": [
    "### Remove existing function and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24d235e3-a887-4d8e-a3b6-7c2b5c6f32fc"
   },
   "outputs": [],
   "source": [
    "deployments_list = wml_client.deployments.get_details()\n",
    "for deployment in deployments_list[\"resources\"]:\n",
    "    model_id = deployment[\"entity\"][\"asset\"][\"id\"]\n",
    "    deployment_id = deployment[\"metadata\"][\"id\"]\n",
    "    if deployment[\"metadata\"][\"name\"] == DEPLOYMENT_NAME:\n",
    "        print(\"Deleting deployment id\", deployment_id)\n",
    "        wml_client.deployments.delete(deployment_id)\n",
    "        print(\"Deleting model id\", model_id)\n",
    "        wml_client.repository.delete(model_id)\n",
    "\n",
    "wml_client.repository.list_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36a1b7cb5d3f48049fd42edcb471d157"
   },
   "source": [
    "### Create the function meta properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd7e0a1e303c41b9b90815e8c2b33894"
   },
   "outputs": [],
   "source": [
    "software_spec_id =  wml_client.software_specifications.get_id_by_name('runtime-22.1-py3.9')\n",
    "print(software_spec_id)\n",
    "function_meta_props = {\n",
    "     wml_client.repository.FunctionMetaNames.NAME: PYTHON_FUNCTION_NAME,\n",
    "     wml_client.repository.FunctionMetaNames.SOFTWARE_SPEC_ID: software_spec_id\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7fbbccd0f2344d097433ebdaf04469c"
   },
   "source": [
    "### Store the Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d2aef0d0e1e47048cbf4abd94aac631"
   },
   "outputs": [],
   "source": [
    "function_artifact = wml_client.repository.store_function(meta_props=function_meta_props, function=custom_metrics_provider)\n",
    "function_uid = wml_client.repository.get_function_id(function_artifact)\n",
    "print(\"Function UID = \" + function_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82a7d7df8fd344158108d57be229c5fa"
   },
   "outputs": [],
   "source": [
    "function_details = wml_client.repository.get_details(function_uid)\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(function_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b89b9d948f242ffa77bde26a8e9cb81"
   },
   "source": [
    "### Deploy the Python function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1c33df40ea94f4ca7f8de257818b417"
   },
   "outputs": [],
   "source": [
    "hardware_spec_id = wml_client.hardware_specifications.get_id_by_name('M')\n",
    "hardware_spec_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6774445edd764e708ba6b4612edaed71"
   },
   "source": [
    "### Create deployment metadata for the Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "888a3c84fc06461f8ebc2120dfd58b22"
   },
   "outputs": [],
   "source": [
    "deploy_meta = {\n",
    " wml_client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n",
    " wml_client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    " wml_client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { \"id\": hardware_spec_id}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3273246ba52a49a1a7e7e98964d2cb63"
   },
   "source": [
    "### Create a deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b5a9733edfb484e9a8533d22581bd8d"
   },
   "outputs": [],
   "source": [
    "deployment_details = wml_client.deployments.create(function_uid, meta_props=deploy_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e97ac943-3e1f-40b1-80ab-4c0ba2f823b0"
   },
   "source": [
    "### Get the scoring URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb56417d0e6e444484c25da9e6048aa5"
   },
   "outputs": [],
   "source": [
    "created_at = deployment_details['metadata']['created_at']\n",
    "find_string_pos = created_at.find(\"T\")\n",
    "if find_string_pos != -1:\n",
    "    current_date = created_at[0:find_string_pos]\n",
    "scoring_url = wml_client.deployments.get_scoring_href(deployment_details)\n",
    "scoring_url = scoring_url + \"?version=\"+current_date\n",
    "print(scoring_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73c85183a81f4997837587b2c64753c6"
   },
   "source": [
    "## 4. Configure OpenScale <a name=\"config\"></a>\n",
    "\n",
    "Import the required libraries and set up the Watson OpenScale Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "addc28e79d494e68a5cb6a942d4f8bac"
   },
   "outputs": [],
   "source": [
    "from ibm_watson_openscale import APIClient\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorMeasurementRequest\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorMetricRequest\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import MetricThreshold\n",
    "from ibm_watson_openscale.supporting_classes.enums import MetricThresholdTypes\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorTagRequest\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import Target\n",
    "from ibm_watson_openscale.supporting_classes.enums import TargetTypes\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import IntegratedSystems, ApplicabilitySelection\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorInstanceSchedule, ScheduleStartTime, MonitorRuntime\n",
    "\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bb826627adf4a2ba2e2bc20d5ab25a2"
   },
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator,BearerTokenAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "\n",
    "authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\n",
    "#authenticator = BearerTokenAuthenticator(bearer_token=IAM_TOKEN) ## uncomment this line if using IAM token to authenticate\n",
    "wos_client = APIClient(service_url=OPENSCALE_API_URL,authenticator=authenticator)\n",
    "wos_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the integrated system for the custom metrics provider <a name=\"custom\"></a>\n",
    "\n",
    "\n",
    "Update the custom metrics deployment URL, which is created during the Python function creation in the integrated system. Watson OpenScale invokes the deployment URL at runtime to compute the custom metrics. \n",
    "\n",
    "You must define the authentication type based on the communication with custom metrics deployment. Watson OpenScale supports 2 types of authentication: basic and bearer. If custom metrics deployment accepts the `basic` authentication type, then provide `auth_type=basic` otherwise use `auth_type=bearer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_type = \"bearer\" #Supported values are basic and bearer\n",
    "\n",
    "if auth_type == \"basic\":\n",
    "    CUSTOM_METRICS_PROVIDER_CREDENTIALS = {\n",
    "        \"auth_type\":\"basic\",\n",
    "        \"username\":  \"*****\",# update the username here \n",
    "        \"password\": \"*****\"# Update the password here\n",
    "   }\n",
    "    \n",
    "if auth_type == \"bearer\":\n",
    "    CUSTOM_METRICS_PROVIDER_CREDENTIALS = {\n",
    "        \"auth_type\":\"bearer\",\n",
    "        \"token_info\": {\n",
    "           \"url\":  IAM_URL,\n",
    "           \"headers\": { \"Content-type\": \"application/x-www-form-urlencoded\" }, # update the headers here \n",
    "           \"payload\": \"grant_type=urn:ibm:params:oauth:grant-type:apikey&response_type=cloud_iam&apikey=\"+CLOUD_API_KEY, # update the payload here \n",
    "           \"method\": \"POST\" # # update the http method here \n",
    "        }        \n",
    "    }\n",
    "    \n",
    "  #if custom metrics deployment is on other cpd cluster or some other cloud then please uncomment and update \n",
    "  #the below \"TOKEN_INFO\" properties to generate the token to communicate to the custom metrics deployment url\n",
    "  #Here are the sample values given in the token_info\n",
    "    #TOKEN_INFO = {\n",
    "    #    \"url\":  \"https://iam.ng.bluemix.net/oidc/token\", # update the token generation here \n",
    "    #    \"headers\": { \"Content-type\": \"application/x-www-form-urlencoded\" }, # update the headers here \n",
    "    #    \"payload\": \"grant_type=urn:ibm:params:oauth:grant-type:apikey&response_type=cloud_iam&apikey=<api_key>\", # update the payload here \n",
    "    #    \"method\": \"POST\" # # update the http method here \n",
    "    #}\n",
    "    #CUSTOM_METRICS_PROVIDER_CREDENTIALS[\"token_info\"] = TOKEN_INFO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove existing integrated system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing custom metrics provider integrated systems if present\n",
    "integrated_systems = IntegratedSystems(wos_client).list().result.integrated_systems\n",
    "for system in integrated_systems:\n",
    "    if system.entity.type == 'custom_metrics_provider' and system.entity.name == CUSTOM_METRICS_PROVIDER_NAME:\n",
    "        print(\"Deleting integrated system {}\".format(system.entity.name))\n",
    "        IntegratedSystems(wos_client).delete(integrated_system_id=system.metadata.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metrics_integrated_system = wos_client.integrated_systems.add(\n",
    "    name=CUSTOM_METRICS_PROVIDER_NAME,\n",
    "    description=CUSTOM_METRICS_PROVIDER_NAME,\n",
    "    type=\"custom_metrics_provider\",\n",
    "    credentials= CUSTOM_METRICS_PROVIDER_CREDENTIALS,\n",
    "    connection={\n",
    "        \"display_name\": CUSTOM_METRICS_PROVIDER_NAME,\n",
    "        \"endpoint\": scoring_url\n",
    "    }\n",
    ").result\n",
    "\n",
    "integrated_system_id = custom_metrics_integrated_system.metadata.id\n",
    "print(custom_metrics_integrated_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bb4bd32-b0b5-4ddc-835f-c40cab1c779a"
   },
   "source": [
    "## 6. Set up the custom monitor definition and instance <a name=\"monitor\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59b9827e31ab45edbad79789213703d1"
   },
   "source": [
    "### Check for the existence of the custom monitor definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14e3f1e2e2f24bfd86f86feee24619e0"
   },
   "outputs": [],
   "source": [
    "def get_custom_monitor_definition():\n",
    "    monitor_definitions = wos_client.monitor_definitions.list().result.monitor_definitions\n",
    "    for definition in monitor_definitions:\n",
    "        if CUSTOM_MONITOR_NAME == definition.entity.name:\n",
    "            return definition\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30720ce70ddf485782ebf767a606366b"
   },
   "source": [
    "### Create the  custom monitor definition.\n",
    "\n",
    "Update the custom metric names, threshold types (`LOWER_LIMIT`, `UPPER_LIMIT`) and default values as required. You can define the threshold type as lower limit, upper limit, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18f34443-76c8-4b9c-85da-a58b10003e7c"
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Update your custom monitor metrics names in the following field. Use the same metric names for creating the \n",
    "# monitor definition and publishing the metrics to Openscale in your python function\n",
    "####################################################################\n",
    "CUSTOM_MONITOR_METRICS_NAMES = ['sensitivity','specificity', 'gender_less40_fav_prediction_ratio']\n",
    "#Update the tag values if you want to fetch the metrics by tags\n",
    "TAGS= ['region']\n",
    "TAG_DESCRIPTION =['customer geographical region']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the model input data type and  algorithm types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the input data type of your model.  \n",
    "# Supported input data types are \"structured\", \"unstructured_text\", \"unstructured_image\"\n",
    "input_data_type = [\"structured\"]\n",
    "\n",
    "# Update the algorithm types that your model supports.\n",
    "# Supported model algorithm types are \"binary\", \"multiclass\", \"regression\"\n",
    "algorithm_types = [\"binary\"]\n",
    "\n",
    "problemTypeSelection = ApplicabilitySelection(problem_type=algorithm_types)\n",
    "inputDataTypeSelection = ApplicabilitySelection(input_data_type=input_data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df7bfd7b6f274d54897133f94dc14310"
   },
   "outputs": [],
   "source": [
    "#Update the Threshold types and default values of the metrics\n",
    "def custom_metric_definitions():\n",
    "    \n",
    "    metrics = [MonitorMetricRequest(name=CUSTOM_MONITOR_METRICS_NAMES[0],applies_to=problemTypeSelection,\n",
    "                                    thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.8)]),\n",
    "              MonitorMetricRequest(name=CUSTOM_MONITOR_METRICS_NAMES[1],applies_to=problemTypeSelection,\n",
    "                                 thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.6),MetricThreshold(type=MetricThresholdTypes.UPPER_LIMIT, default=1)]),\n",
    "              MonitorMetricRequest(name=CUSTOM_MONITOR_METRICS_NAMES[2],applies_to=problemTypeSelection,\n",
    "                                 thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.6),MetricThreshold(type=MetricThresholdTypes.UPPER_LIMIT, default=1)])]\n",
    "    #Comment the below tags code if there are no tags to be created\n",
    "    tags = [MonitorTagRequest(name=TAGS[0], description=TAG_DESCRIPTION[0])]\n",
    "    \n",
    "    return metrics, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  schedule for custom monitor\n",
    "\n",
    "Creates a schedule for custom monitor with a defined repeat type and repeat interval. Supported repeat types are \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Defaults to once every hour if `repeat_interval` and  `repeat_type` values are not specified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable the schedule for your custom monitor. Update the value to false if you want to disable the schedule\n",
    "ENABLE_SCHEDULE = True\n",
    "\n",
    "#Update the repeat interval and repeat type: Default is 1 hour\n",
    "repeat_interval = 1\n",
    "repeat_type = \"hour\"\n",
    "\n",
    "#Update the delay unit and interval to trigger the first schedule run after the custom monitor instance is created. Default is 30 minutes\n",
    "delay_unit = \"minute\"\n",
    "delay_time= 30\n",
    "\n",
    "start_time= ScheduleStartTime(type = \"relative\", delay_unit= delay_unit, delay = delay_time)\n",
    "schedule = MonitorInstanceSchedule(repeat_interval=repeat_interval,repeat_unit=repeat_type, start_time=start_time)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79e2f275e13948e5834e77ec4dfadeb6"
   },
   "outputs": [],
   "source": [
    "def create_custom_monitor_definition():\n",
    "    # check if the custom monitor definition already exists or not\n",
    "    existing_definition = get_custom_monitor_definition()\n",
    "\n",
    "    # if it does not exists, then create a new one.\n",
    "    if existing_definition is None:\n",
    "        metrics, tags = custom_metric_definitions()\n",
    "        if ENABLE_SCHEDULE:\n",
    "            monitor_runtime = MonitorRuntime(type=\"custom_metrics_provider\")\n",
    "            custom_monitor_details = wos_client.monitor_definitions.add(name=CUSTOM_MONITOR_NAME, metrics=metrics, tags=tags, schedule = schedule, \n",
    "                                                                        applies_to=inputDataTypeSelection, monitor_runtime = monitor_runtime, background_mode=False).result\n",
    "        else:\n",
    "            custom_monitor_details = wos_client.monitor_definitions.add(name=CUSTOM_MONITOR_NAME, metrics=metrics, tags=tags, applies_to=inputDataTypeSelection, background_mode=False).result\n",
    "    else:\n",
    "        # otherwise, send the existing definition\n",
    "        custom_monitor_details = existing_definition\n",
    "    return custom_monitor_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d352bcf09e646d0a6d174e54078f2c6"
   },
   "outputs": [],
   "source": [
    "custom_monitor_details = create_custom_monitor_definition()\n",
    "custom_monitor_id = custom_monitor_details.metadata.id\n",
    "custom_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a0dbf50822c40c680cc9430eb88306c"
   },
   "source": [
    "### Check the existence of custom monitor instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7176b942f61f4d0381fb63b92101e76f"
   },
   "outputs": [],
   "source": [
    "def get_custom_monitor_instance(custom_monitor_id):\n",
    "    monitor_instances = wos_client.monitor_instances.list(data_mart_id = WOS_GUID, monitor_definition_id = custom_monitor_id, target_target_id = subscription_id).result.monitor_instances\n",
    "    if len(monitor_instances) == 1:\n",
    "        return monitor_instances[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32c84dce-c23a-42ef-b874-477a71b4ef36"
   },
   "outputs": [],
   "source": [
    "# Openscale MRM service invokes custom metrics deployment url during runtime and wait for the default time of 60 second's to \n",
    "# to check the run status ie finished/Failed and fetch the latest measurement. Increase the wait time, if the runtime deployment \n",
    "# takes more than 60 seconds to compute and publish the custom metrics \n",
    "\n",
    "#Update the wait time here.\n",
    "custom_metrics_wait_time = 60 #time in seconds <update the time here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c49fe8f-7921-410c-9983-6f998522d715"
   },
   "source": [
    "### Update the custom monitor instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67ed74d7-928a-4f64-a7f7-dc9bfb5d5fbc"
   },
   "outputs": [],
   "source": [
    "def update_custom_monitor_instance(custom_monitor_instance_id):\n",
    "    payload = [\n",
    "     {\n",
    "       \"op\": \"replace\",\n",
    "       \"path\": \"/parameters\",\n",
    "       \"value\": {\n",
    "           \"custom_metrics_provider_id\": integrated_system_id,\n",
    "           \"custom_metrics_wait_time\":   custom_metrics_wait_time,\n",
    "           \"enable_custom_metric_runs\": True\n",
    "       }\n",
    "     }\n",
    "    ]\n",
    "    response = wos_client.monitor_instances.update(custom_monitor_instance_id, payload, update_metadata_only = True)\n",
    "    result = response.result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "678df1de54164d6284cdf9f255affd8a"
   },
   "source": [
    "### For the custom monitor definition, create a custom monitor instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97624e1c0a8e4648af05162690c96c66"
   },
   "outputs": [],
   "source": [
    "def create_custom_monitor_instance(custom_monitor_id):\n",
    "    # Check if an custom monitor instance already exists\n",
    "    existing_monitor_instance = get_custom_monitor_instance(custom_monitor_id)\n",
    "\n",
    "    # If it does not exist, then create one\n",
    "    if existing_monitor_instance is None:\n",
    "        target = Target(\n",
    "                target_type=TargetTypes.SUBSCRIPTION,\n",
    "                target_id=subscription_id\n",
    "            )\n",
    "        parameters = {\n",
    "            \"custom_metrics_provider_id\": integrated_system_id,\n",
    "            \"custom_metrics_wait_time\":   custom_metrics_wait_time,\n",
    "            \"enable_custom_metric_runs\": True\n",
    "        }\n",
    "        # create the custom monitor instance id here.\n",
    "        custom_monitor_instance_details = wos_client.monitor_instances.create(\n",
    "                    data_mart_id=WOS_GUID,\n",
    "                    background_mode=False,\n",
    "                    monitor_definition_id=custom_monitor_id,\n",
    "                    target=target,\n",
    "                    parameters=parameters\n",
    "        ).result\n",
    "    else:\n",
    "        # otherwise, update the existing one with latest integrated system details.\n",
    "        instance_id = existing_monitor_instance.metadata.id\n",
    "        custom_monitor_instance_details = update_custom_monitor_instance(instance_id)\n",
    "    return custom_monitor_instance_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_instance_details = create_custom_monitor_instance(custom_monitor_id)\n",
    "custom_monitor_instance_id = monitor_instance_details.metadata.id\n",
    "print(monitor_instance_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associate the integrated system with custom monitor to view the custom monitor configured for the subscription in the Openscale UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an optional step to associate the custom monitor with the integrated system\n",
    "\n",
    "def update_monitor_definition_id_in_integrated_systems(custom_monitor_id):\n",
    "    payload = [\n",
    "     {\n",
    "       \"op\": \"add\",\n",
    "       \"path\": \"/parameters\",\n",
    "       \"value\": {\n",
    "           \"monitor_definition_ids\": [ custom_monitor_id ]\n",
    "     }\n",
    "     }\n",
    "    ]\n",
    "    response = wos_client.integrated_systems.update(integrated_system_id, payload)\n",
    "    result = response.result\n",
    "    return result\n",
    "\n",
    "response = update_monitor_definition_id_in_integrated_systems(custom_monitor_id)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the custom monitor <a name=\"run\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the custom metrics provider deployment\n",
    "monitor_instance_run_info = wos_client.monitor_instances.run(\n",
    "        background_mode=False,\n",
    "        monitor_instance_id=custom_monitor_instance_id\n",
    "     ).result\n",
    "\n",
    "monitor_instance_run_info\n",
    "custom_monitor_run_id = monitor_instance_run_info.metadata.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=custom_monitor_instance_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c0fca1109bf4c048fe61d6de5815179"
   },
   "source": [
    "## Recap of the steps performed in this notebook\n",
    "\n",
    "- Create a python function\n",
    "- Deploy the python function to WML\n",
    "- Create an OpenScale Integrated System pointing to the python function\n",
    "- Create a Custom Monitor Definition mentioning various custom metrics\n",
    "- Create a Custom Monitor Instance and specify the Integrated System ID in the monitor instance configuration.\n",
    "- Run the Custom monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e66248c-bbdb-4644-9dd8-b871e9aafaca"
   },
   "source": [
    "# [OPTIONAL STEP] Invoke the custom metrics deployment Python function as part of this notebook.\n",
    "\n",
    "Validate the custom metrics provider deployment by providing the correct set of paramaters to generate the custom metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_id(data_set_type: str):\n",
    "    data_sets = wos_client.data_sets.list(target_target_id= subscription_id, type = data_set_type).result.data_sets\n",
    "    feedback_data_set_id = None\n",
    "    if len(data_sets) > 0:\n",
    "        feedback_data_set_id = data_sets[0].metadata.id\n",
    "    return feedback_data_set_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fd2f8ec-bab9-4f9e-b3be-22822155fd9b"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "parameters = {\n",
    "    \"custom_metrics_provider_id\": integrated_system_id,\n",
    "    \"custom_metrics_wait_time\":   custom_metrics_wait_time\n",
    "}\n",
    "\n",
    "payload= {\n",
    "    \"data_mart_id\" : WOS_GUID,\n",
    "    \"subscription_id\" : subscription_id,\n",
    "    \"custom_monitor_id\" : custom_monitor_id,\n",
    "    \"custom_monitor_instance_id\" : custom_monitor_instance_id,\n",
    "    \"custom_monitor_run_id\":custom_monitor_run_id,\n",
    "    \"custom_monitor_instance_params\": parameters,\n",
    "    \"feedback_dataset_id\": get_dataset_id(\"feedback\")\n",
    "    \n",
    "}\n",
    "\n",
    "input_data= { \"input_data\": [ { \"values\": payload } ]\n",
    "            }\n",
    "\n",
    "\n",
    "deployment_uid=wml_client.deployments.get_uid(deployment_details)\n",
    "job_details = wml_client.deployments.score(deployment_uid, input_data)\n",
    "pprint(job_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "210c3b5835494f67812f0704bad84705"
   },
   "source": [
    "## Congratulations\n",
    "\n",
    "You have finished configuring Custom Monitor Definition and Monitor instance and executing Custom Monitor Run for your Subscription. You can also run the custom monitor from `Watson OpenScale Dashboard`(http://aiopenscale.cloud.ibm.com). Click the tile of your model and select `Evaluate Now` option from `Actions` drop down menu to run the monitor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
