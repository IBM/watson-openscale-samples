{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97648ee8",
   "metadata": {
    "id": "a81be50301fa471b8417773161ab8ca8"
   },
   "source": [
    "# Use the IBM watsonx.governance metrics toolkit to evaluate AWS Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd84d0",
   "metadata": {},
   "source": [
    "The IBM watsonx.governance metrics toolkit lets you evaluate the output of a Large Language Model (LLM) against multiple task types: Text Summarization, Content Generation, Question Answering, Text Classification, Entity Extraction, and Retrieval-Augmented Generation (RAG). \n",
    "\n",
    "This notebook will demonstrate how to evaluate output from a Text Summarization prompt run against an Amazon Web Services (AWS) Bedrock LLM. It also demonstrates how to evaluate output from Content Generation, Question Answering, and Text Classification prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97545aaa",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Create your prompt for testing against the `anthropic.claude-v2` model.\n",
    "-  Configure metrics for evaluation.\n",
    "-  Run the metrics against your prompt data.\n",
    "-  Print and review the metrics returned by the IBM watsonx.governance metrics toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa71d10d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Install the necessary packages](#packages)\n",
    "2.  [Provision services and configure credentials](#credentials)\n",
    "3.\t[Evaluate Text Summarization output from the AWS Bedrock `anthropic.claude-v2` model](#summarization)\n",
    "4.\t[Evaluate Content Generation output from the AWS Bedrock `anthropic.claude-v2` model](#contentgen)\n",
    "5.\t[Evaluate Question Answering output from the AWS Bedrock `anthropic.claude-v2` model](#question)\n",
    "6.  [Evaluate Text Classification output from the AWS Bedrock `anthropic.claude-v2` model](#textclass)\n",
    "7.\t[Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50634c8d",
   "metadata": {},
   "source": [
    "<a id=\"packages\"></a>\n",
    "## Step 1 - Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson-machine-learning   | tail -n 1\n",
    "!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1\n",
    "!pip install --upgrade ibm-metrics-plugin --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31243aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade evaluate --no-cache | tail -n 1\n",
    "!pip install --upgrade rouge_score --no-cache | tail -n 1\n",
    "!pip install --upgrade textstat --no-cache | tail -n 1\n",
    "!pip install --upgrade sacrebleu --no-cache | tail -n 1\n",
    "!pip install --upgrade sacremoses --no-cache | tail -n 1\n",
    "!pip install --upgrade datasets==2.10.0 --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 -U --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e554a-b793-4a0d-a05c-1e08ba6c3dc7",
   "metadata": {
    "id": "12308564-2875-4b49-b788-85ece1964fe5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bd36b",
   "metadata": {
    "id": "4267839e32bd48719cf4b3389e3cadb8"
   },
   "source": [
    "<a id=\"credentials\"></a>\n",
    "## Step 2 - Provision services and configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f90ef",
   "metadata": {},
   "source": [
    "### Provision an instance of IBM Watson OpenScale\n",
    "\n",
    "If you have not already done so, provision an instance of IBM Watson OpenScale using the [OpenScale link in the Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437b3cc",
   "metadata": {},
   "source": [
    "### Generate an API key\n",
    "\n",
    "You can generate a Cloud API key with IBM Cloud console or with IBM Cloud command line interface.\n",
    "\n",
    "To generate an API key by using IBM Cloud console:\n",
    "\n",
    "1. Go to the [**Users** section of the IBM Cloud console](https://cloud.ibm.com/iam#/users).\n",
    "1. Click your name, then scroll down to the **API Keys** section.\n",
    "1. Click **Create an IBM Cloud API key**.\n",
    "1. Give your key a name and click **Create**.\n",
    "1. Copy the created key - you will need to paste this key into the `CLOUD_API_KEY` variable in the \"Configure your credentials\" section below.\n",
    "\n",
    "To create an API key using the IBM Cloud [command line interface](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli):\n",
    "\n",
    "1. From the command line interface, type the following:\n",
    "\n",
    "    `bx login --sso`\n",
    "\n",
    "    `bx iam api-key-create 'my_key'`\n",
    "\n",
    "1. Copy the created key - you will need to paste this key into the `CLOUD_API_KEY` variable in the \"Configure your credentials\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909149fa",
   "metadata": {},
   "source": [
    "### Configure your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d96be6",
   "metadata": {
    "id": "f78243b20abd4b4d84313dacb4f02624"
   },
   "outputs": [],
   "source": [
    "use_cpd = False\n",
    "CLOUD_API_KEY = \"<CLOUD_API_KEY>\"\n",
    "IAM_URL = \"https://iam.ng.bluemix.net/oidc/token\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a480608",
   "metadata": {},
   "source": [
    "If you are running your notebook on a CPD cluster, uncomment and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b36609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cpd = True\n",
    "# WOS_CREDENTIALS = {\n",
    "#     \"url\": \"xxxxx\",\n",
    "#     \"username\": \"xxxxx\",\n",
    "#     \"api_key\": \"xxxxx\"\n",
    "# }\n",
    "\n",
    "# GEN_API_KEY = WOS_CREDENTIALS[\"api_key\"]\n",
    "\n",
    "# api_endpoint = WOS_CREDENTIALS[\"url\"]\n",
    "# project_id = \"<Your project id>\"\n",
    "# endpoint_url = WOS_CREDENTIALS[\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1d906",
   "metadata": {
    "id": "d5d5b8b3dd4847298dee4b065ee9c3d4"
   },
   "source": [
    "### Authenticate with IBM watsonx.governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0ce8d",
   "metadata": {
    "id": "9567c5c157ae44ecabd9fdc434f16056"
   },
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator,BearerTokenAuthenticator,CloudPakForDataAuthenticator\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "if use_cpd:\n",
    "    authenticator = CloudPakForDataAuthenticator(\n",
    "            url=WOS_CREDENTIALS['url'],\n",
    "            username=WOS_CREDENTIALS['username'],\n",
    "            password=WOS_CREDENTIALS['password'],\n",
    "            disable_ssl_verification=True\n",
    "        )\n",
    "    \n",
    "    client = APIClient(service_url=WOS_CREDENTIALS['url'],authenticator=authenticator)\n",
    "    print(client.version)\n",
    "else:\n",
    "    authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\n",
    "    client = APIClient(authenticator=authenticator)\n",
    "    print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45542ea",
   "metadata": {
    "id": "d1aa94f70d4d4f77bc87a0b06139af3c"
   },
   "source": [
    "### Import common evaluation metrics and metric groups\n",
    "\n",
    "These are the metrics used to evaluate your prompt against the selected model, based on the prompt task type â€” Summarization, Classification, Question Answering, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dc6b8",
   "metadata": {
    "id": "76620bd0a7784cb4801d10f3d93fa6a1"
   },
   "outputs": [],
   "source": [
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMTextMetricGroup\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMGenerationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMSummarizationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMQAMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMClassificationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import HAP_SCORE\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import PII_DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db10c6",
   "metadata": {
    "id": "a68e9be2a0a0438d822679c5c1728097"
   },
   "source": [
    "<a id=\"summarization\"></a>\n",
    "## Step 3 - Evaluate Text Summarization output from the AWS Bedrock `anthropic.claude-v2` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927e928",
   "metadata": {
    "id": "abab31d97a0a460a90897da08f36add0"
   },
   "source": [
    "### Download a dataset containing prompt input data for model inferencing and reference data for model output evaluation\n",
    "\n",
    "The downloaded `.csv` file contains: input, a generated summary, and two reference summaries each for 50 sample prompts. Values are then further converted to input, output, and reference panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f284b78",
   "metadata": {
    "id": "8f63311275114169ac00dc3096352dd0"
   },
   "outputs": [],
   "source": [
    "!rm -fr llm_content.csv\n",
    "!wget \"https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/watsonx/llm_content.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2169",
   "metadata": {
    "id": "c03c61b496f64edc8b9bd3fc3ff03899"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "llm_data_all = pd.read_csv(\"llm_content.csv\")\n",
    "llm_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd67ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data = llm_data_all.head(10)\n",
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea196f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6843a6",
   "metadata": {},
   "source": [
    "### Obtain your AWS security credentials\n",
    "\n",
    "Copy or create your AWS [security credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html), and paste them in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c654c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = 'xxxxxx'\n",
    "aws_secret_access_key = 'xxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a48caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77a7d7",
   "metadata": {},
   "source": [
    "### Create an AWS Bedrock service client\n",
    "\n",
    "Programmatically create a Bedrock service client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192075be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = session.client(service_name='bedrock', \n",
    "                         aws_access_key_id = aws_access_key_id, \n",
    "                         aws_secret_access_key = aws_secret_access_key, \n",
    "                         region_name = 'us-east-1',\n",
    "                         endpoint_url = 'https://bedrock.us-east-1.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a1651",
   "metadata": {},
   "source": [
    "### Select the `anthropic.claude-v2` model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the available foundation models in Bedrock\n",
    "\n",
    "fm_model_list = bedrock.list_foundation_models()\n",
    "\n",
    "fm_model_names = [x['modelId'] for x in fm_model_list['modelSummaries']]\n",
    "print(*fm_model_names, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac398d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the `anthropic.claude-v2` model\n",
    "\n",
    "modelId = 'anthropic.claude-v2'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbdc80",
   "metadata": {},
   "source": [
    "### Create a `bedrock-runtime` client\n",
    "\n",
    "The runtime client allows you to run your prompt against the `anthropic.claude-v2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = session.client(service_name='bedrock-runtime', \n",
    "                         aws_access_key_id = aws_access_key_id, \n",
    "                         aws_secret_access_key = aws_secret_access_key, \n",
    "                         region_name = 'us-east-1',\n",
    "                         endpoint_url = 'https://bedrock-runtime.us-east-1.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdc7a9",
   "metadata": {},
   "source": [
    "### Create your prompt for testing against the `anthropic.claude-v2` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(text):\n",
    "    prompt = f\"\"\"Human: Please provide a summary of the following text with maximum of 20 words.\n",
    "    \n",
    "{text}\n",
    "    \n",
    "Assistant:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50a7ab",
   "metadata": {},
   "source": [
    "### Examine the generated prompt summary result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd413ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_evaluation(text):\n",
    "    prompt = get_prompt(text)\n",
    "    body = json.dumps({\"prompt\": prompt,\n",
    "                     \"max_tokens_to_sample\":2048,\n",
    "                     \"temperature\":0.1,\n",
    "                     \"top_k\":250,\n",
    "                     \"top_p\":0.5,\n",
    "                     \"stop_sequences\":[]\n",
    "                      }) \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    completion = response_body['completion']\n",
    "    summary = completion\n",
    "    if '\\n\\n' in completion:\n",
    "        summary = completion.split(\"\\n\\n\")[1]\n",
    "    print('-----')    \n",
    "    print(summary)\n",
    "    print('-----')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45478137",
   "metadata": {},
   "source": [
    "### Append the generated prompt summary result to the model data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data['anthropic_generated_summary'] = llm_data['input_text'].apply(prompt_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aee59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8906a",
   "metadata": {},
   "source": [
    "### Get the necessary data for evaluating the prompt template metrics\n",
    "\n",
    "Metrics will be evaluated for the input, output, and reference summary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c385f",
   "metadata": {
    "id": "091c264708cc4159969a4690daf07886"
   },
   "outputs": [],
   "source": [
    "df_input = llm_data[['input_text']].copy()\n",
    "df_output = llm_data[['anthropic_generated_summary']].copy()\n",
    "df_reference = llm_data[['reference_summary_2']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7f842",
   "metadata": {
    "id": "74a9de9b2bb84c35b87fa1d2abc5770c"
   },
   "source": [
    "### Configure metrics for evaluation\n",
    "\n",
    "Select the metrics you want to evaluate; the code cell below contains 10 common Summarization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8262dfe-38b1-4d6f-88dd-c3a4c3b11996",
   "metadata": {
    "id": "95e9409a-ccb9-409b-ae04-b0bdf392faac"
   },
   "outputs": [],
   "source": [
    "metric_config = {   \n",
    "    \"configuration\": {\n",
    "        LLMTextMetricGroup.SUMMARIZATION.value: {\n",
    "            LLMSummarizationMetrics.ROUGE_SCORE.value: {},\n",
    "            LLMSummarizationMetrics.SARI.value: {},\n",
    "            LLMSummarizationMetrics.METEOR.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_RECALL.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_PRECISION.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_F1_SCORE.value: {},\n",
    "            LLMSummarizationMetrics.COSINE_SIMILARITY.value: {},\n",
    "            LLMSummarizationMetrics.JACCARD_SIMILARITY.value: {},\n",
    "            LLMSummarizationMetrics.BLEU.value: {},\n",
    "            LLMSummarizationMetrics.FLESCH.value: {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240deb44",
   "metadata": {
    "id": "f55e2339070c48608474f370abd949b7"
   },
   "source": [
    "### Summarization metrics evaluation\n",
    "\n",
    "Run the metrics against your prompt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb037b-ff41-492f-ab4e-5fdb35d4da03",
   "metadata": {
    "id": "031a7f52-ba90-46d4-aac6-09d8f76e6d3f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = client.llm_metrics.compute_metrics(metric_config,sources = df_input, predictions = df_output, references = df_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12d146",
   "metadata": {
    "id": "4bb4d7ebf78a4589b122508635c92572"
   },
   "source": [
    "### Review metrics\n",
    "\n",
    "Print and review the metrics returned by the IBM watsonx.governance metrics toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fd1da-329e-40e0-bce1-f99bb37dea3c",
   "metadata": {
    "id": "fd2fa6e4-c832-4ed6-bb10-8bae433e95f6"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(result,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32e5f6",
   "metadata": {
    "id": "9ed25962da7b418b866ff59d66a72140"
   },
   "source": [
    "<a id=\"contentgen\"></a>\n",
    "## Step 4 - Evaluate Content Generation output from the AWS Bedrock `anthropic.claude-v2` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecae67",
   "metadata": {
    "id": "851c73b2dcdb496380966cb4a57cd433"
   },
   "source": [
    "### Download a dataset containing prompt input data for model inferencing and reference data for model output evaluation\n",
    "\n",
    "The downloaded `.csv` file contains a question, generated answer text, and reference text for 50 sample prompts. Prompt values are then further converted to question, generated answer text, and reference panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a71ca",
   "metadata": {
    "id": "ba2dad47c2bd400d9d41deb03cda290b"
   },
   "outputs": [],
   "source": [
    "!rm -fr llm_content_generation.csv\n",
    "!wget \"https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/watsonx/llm_content_generation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeffc0c",
   "metadata": {
    "id": "c9de8a2fbe5642a4871bd3bf407e81b5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"llm_content_generation.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f87d99",
   "metadata": {
    "id": "b54090c5f78b4335bc0a5f449d1e8c26"
   },
   "outputs": [],
   "source": [
    "df_input = data[['question']].copy()\n",
    "df_output = data[['generated_text']].copy()\n",
    "df_reference = data[['reference_text']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49afba",
   "metadata": {
    "id": "bbede258f6cd425e8c8f60379e47dd16"
   },
   "source": [
    "### Configure metrics for evaluation\n",
    "\n",
    "Select the metrics you want to evaluate; the code cell below contains 7 common Content Generation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f09dd",
   "metadata": {
    "id": "57c6964b-def7-4fec-b5e9-c68e71a08d43"
   },
   "outputs": [],
   "source": [
    "metric_config = {   \n",
    "    #All Common parameters goes here \n",
    "    \"configuration\": {        \n",
    "        LLMTextMetricGroup.GENERATION.value: { # metric group   \n",
    "            LLMGenerationMetrics.BLEU.value: {},\n",
    "            LLMGenerationMetrics.ROUGE_SCORE.value: {},\n",
    "            LLMGenerationMetrics.FLESCH.value: {},\n",
    "            LLMGenerationMetrics.METEOR.value: {},            \n",
    "            LLMGenerationMetrics.NORMALIZED_RECALL.value: {},\n",
    "            LLMGenerationMetrics.NORMALIZED_PRECISION.value: {},\n",
    "            LLMGenerationMetrics.NORMALIZED_F1_SCORE.value: {}            \n",
    "        }    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a283cc",
   "metadata": {
    "id": "3c2039f5b47b415686a9fd5c6d7797b3"
   },
   "source": [
    "### Content Generation metrics evaluation\n",
    "\n",
    "Run the metrics against your prompt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656fc82",
   "metadata": {
    "id": "04e3359e-693c-4f4e-a0f9-4d41227af409"
   },
   "outputs": [],
   "source": [
    "result = client.llm_metrics.compute_metrics(metric_config,df_input,df_output, df_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb5446",
   "metadata": {
    "id": "9d63fa0a05584ad18d13d6924ffd8602"
   },
   "source": [
    "### Review metrics\n",
    "\n",
    "Print and review the metrics returned by the IBM watsonx.governance metrics toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe539f8",
   "metadata": {
    "id": "91fce6ecb3054e4081ab5098a2b4eaa9"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(result,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cae48d",
   "metadata": {
    "id": "e7521601-375c-43b0-84f4-7f1496d8fb03"
   },
   "source": [
    "<a id=\"question\"></a>\n",
    "## Step 5 - Evaluate Question Answering output from the AWS Bedrock `anthropic.claude-v2` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8c808",
   "metadata": {
    "id": "e06857c26fbd40629b5b619221cdfea0"
   },
   "source": [
    "### Download a dataset containing prompt input data for model inferencing and reference data for model output evaluation\n",
    "\n",
    "The downloaded `.csv` file contains question-and-answer pairs for 50 sample prompts. Values in the Question column are the input, and values in the Answer column are the prompt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ba1e2",
   "metadata": {
    "id": "65783a820d87443d9c94136b32ab5576"
   },
   "outputs": [],
   "source": [
    "!rm -fr llm_content_qa.csv\n",
    "!wget \"https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/watsonx/llm_content_qa.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea08d02",
   "metadata": {
    "id": "957ccd450d574263abd29ab7a2ab6239"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"llm_content_qa.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f630a4e",
   "metadata": {
    "id": "496b31ad4a83445b87b196501460bb3b"
   },
   "outputs": [],
   "source": [
    "df_input = data[['question']].copy()\n",
    "df_output = data[['answers']].copy()\n",
    "df_reference = data[['answers']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e4494",
   "metadata": {
    "id": "72f64a14008744a18abbaf0dffc8d175"
   },
   "source": [
    "### Configure metrics for evaluation\n",
    "\n",
    "Select the metrics you want to evaluate; the code cell below contains 3 common Question Answering metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db9e2a",
   "metadata": {
    "id": "b2e0e82f3a5e4cf787165c3c3f20d40d"
   },
   "outputs": [],
   "source": [
    "metric_config = {   \n",
    "    #All Common parameters goes here \n",
    "    \"configuration\": {        \n",
    "        LLMTextMetricGroup.QA.value: { # metric group   \n",
    "            LLMQAMetrics.EXACT_MATCH.value: {},\n",
    "            LLMQAMetrics.ROUGE_SCORE.value: {},\n",
    "            LLMQAMetrics.BLEU.value: {}          \n",
    "        }    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49768a30",
   "metadata": {
    "id": "2828f5cb24f244d38c5db891ee0afbd6"
   },
   "source": [
    "### Question and Answering metrics evaluation\n",
    "\n",
    "Run the metrics against your prompt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc9217",
   "metadata": {
    "id": "57d93e69ad2846a3aa346207ccc57af5"
   },
   "outputs": [],
   "source": [
    "result = client.llm_metrics.compute_metrics(metric_config,df_input,df_output, df_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38425f",
   "metadata": {
    "id": "cb3227d0d3c543a4807b398088b93158"
   },
   "source": [
    "### Review metrics\n",
    "\n",
    "Print and review the metrics returned by the IBM watsonx.governance metrics toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ffb4e",
   "metadata": {
    "id": "55ad771c8f9845c18ca125132da441ac"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(result,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078c15b",
   "metadata": {
    "id": "8eec33aaaad94f9db377943ebd4350b0"
   },
   "source": [
    "<a id=\"textclass\"></a>\n",
    "## Step 6 - Evaluate Text Classification output from the AWS Bedrock `anthropic.claude-v2` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6091a",
   "metadata": {
    "id": "4badf20522e44bb2a077edf95e52682a"
   },
   "source": [
    "### Download a dataset containing prompt input data for model inferencing and reference data for model output evaluation\n",
    "\n",
    "\n",
    "The downloaded `.csv` file contains label-and-text pairs for 50 sample prompts. Values in the `text` column are the input, and values in the `label` column act as both output and reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4bc249",
   "metadata": {
    "id": "757573f7fa6a4c64815e6419708ce15b"
   },
   "outputs": [],
   "source": [
    "!rm -fr llm_content_classification.csv\n",
    "!wget \"https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/watsonx/llm_content_classification.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608da47",
   "metadata": {
    "id": "69cfb80c15a34c0199bcbc3c04d7f63a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"llm_content_classification.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6257bd3",
   "metadata": {
    "id": "f3e36646c7024931ab197cfbcedc59ea"
   },
   "outputs": [],
   "source": [
    "data['label'] = data['label'].replace({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21b6b2",
   "metadata": {
    "id": "bffdedf11c8242ba9183746802dd1404"
   },
   "outputs": [],
   "source": [
    "df_input = data[['text']].copy()\n",
    "df_output = data[['label']].copy()\n",
    "df_reference = data[['label']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77893c6",
   "metadata": {
    "id": "2c50933f6d6a41bf86d8084b6904541a"
   },
   "source": [
    "### Create a reference column\n",
    "\n",
    "The reference column provides a more realistic classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b675c84",
   "metadata": {
    "id": "94a8e911986a450888c8d79d75ff3c63"
   },
   "outputs": [],
   "source": [
    "shuffled_column = df_reference['label'].sample(frac=1).reset_index(drop=True)\n",
    "df_reference['label'] = shuffled_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44ac10",
   "metadata": {
    "id": "25eb2beeed794f2a97b8eca274e1d670"
   },
   "source": [
    "### Configure metrics for evaluation\n",
    "\n",
    "Select the metrics you want to evaluate; the code cell below contains 5 common Text Classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d2011",
   "metadata": {
    "id": "d95388d5677e41819a29d4a61c490cc2"
   },
   "outputs": [],
   "source": [
    "metric_config = {   \n",
    "    #All Common parameters go here \n",
    "    \"configuration\": {        \n",
    "        LLMTextMetricGroup.CLASSIFICATION.value: { # metric group   \n",
    "            LLMClassificationMetrics.ACCURACY.value: {},\n",
    "            LLMClassificationMetrics.PRECISION.value: {},\n",
    "            LLMClassificationMetrics.RECALL.value: {},\n",
    "            LLMClassificationMetrics.F1_SCORE.value: {},\n",
    "            LLMClassificationMetrics.MATTHEWS_CORRELATION.value: {},            \n",
    "        }    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f8fb9",
   "metadata": {
    "id": "824900df910d4302b73af2e615526f32"
   },
   "source": [
    "### Text Classification metrics evaluation\n",
    "\n",
    "Run the metrics against your prompt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792b218",
   "metadata": {
    "id": "d14b65d169e7497e8f4587e944ef8924"
   },
   "outputs": [],
   "source": [
    "result = client.llm_metrics.compute_metrics(metric_config,df_input,df_output, df_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5744fe",
   "metadata": {
    "id": "9689b3b4e4f34459b8f5cf4d9c76d365"
   },
   "source": [
    "### Review metrics\n",
    "\n",
    "Print and review the metrics returned by the IBM watsonx.governance metrics toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f592c0",
   "metadata": {
    "id": "5c42d600f12142a5904f71a002ad25c3"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(result,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f62fa7",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "Congratulations, you successfully completed this notebook! You learned how to evaluate output from Text Summarization, Content Generation, Question Answering, and Text Classification prompts run against an Amazon Web Services (AWS) Bedrock LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669fb47",
   "metadata": {
    "id": "e94e539d98704ac99e31c7d88b700140"
   },
   "source": [
    "### Authors:\n",
    "\n",
    "**Kishore Patel**\n",
    "\n",
    "**Ravi Chamarthy**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
