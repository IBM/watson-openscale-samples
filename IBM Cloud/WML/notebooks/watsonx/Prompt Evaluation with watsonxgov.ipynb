{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Prompts/Prompt Template Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook should be run using with Runtime 22.2 & Python 3.10 or greater runtime environment. If you are viewing this in Watson Studio and do not see Python 3.10.x in the upper right corner of your screen, please update the runtime now. \n",
    "\n",
    "The notebook will create a summarization prompt template asset in a given project, configure OpenScale to monitor that PTA and evaluate generative quality metrics and model health metrics. The notebook then promotes the prompt template asset to space and does the same evaluation.\n",
    "\n",
    "If users wish to execute this notebook for task types other than summarization, please consult [this](https://github.com/IBM/watson-openscale-samples/blob/main/IBM%20Cloud/WML/notebooks/watsonx/README.md) document for guidance on evaluating prompt templates for the available task types.\n",
    "\n",
    "Note : User can search for `EDIT THIS` and fill the inputs needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It requires service credentials for IBM Watson OpenScale:\n",
    "* Requires a CSV file containing the test data that needs to be evaluated\n",
    "* Requires the ID of project in which you want to create the prompt template asset.\n",
    "* Requires the ID of space to which you want to promote the prompt template asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Setup](#settingup)\n",
    "- [Create Prompt template](#prompt)\n",
    "- [Prompt Setup](#ptatsetup)\n",
    "- [Risk evaluations for prompt template asset subscription](#evaluate)\n",
    "- [Display the Model Risk metrics](#mrmmetric)\n",
    "- [Display the Generative AI Quality metrics](#genaimetrics)\n",
    "- [Plot rougel and rougelsum metrics against records](#plotproject)\n",
    "- [See factsheets information](#factsheetsspace)\n",
    "- [Evaluate prompt template from space](#evaluatespace)\n",
    "- [Promote prompt template asset to space](#promottospace)\n",
    "- [Create deployment for prompt template asset in space](#ptadeployment)\n",
    "- [Setup the prompt template asset in space](#ptaspace)\n",
    "- [Score the model and configure monitors](#score)\n",
    "- [Plot rougel and rougelsum metrics against records for production subscription](#plotspace)\n",
    "- [See factsheets information from space](#factsheetsproject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"settingup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson-openscale | tail -n 1\n",
    "!pip install --upgrade ibm-watson-machine-learning | tail -n 1\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision services and configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not already, provision an instance of IBM Watson OpenScale using the [OpenScale link in the Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Cloud API key can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You can also get OpenScale `API_KEY` using IBM CLOUD CLI.\n",
    "\n",
    "How to install IBM Cloud (bluemix) console: [instruction](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli.html#install_use)\n",
    "\n",
    "How to get api key using console:\n",
    "```\n",
    "bx login --sso\n",
    "bx iam api-key-create 'my_key'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cpd = False\n",
    "IAM_URL=\"https://iam.cloud.ibm.com\"\n",
    "DATAPLATFORM_URL = \"https://api.dataplatform.cloud.ibm.com\"\n",
    "SERVICE_URL = \"https://aiopenscale.cloud.ibm.com\"\n",
    "CLOUD_API_KEY = \"<EDIT THIS>\" # YOUR_CLOUD_API_KEY\n",
    "\n",
    "\n",
    "WML_CREDENTIALS = {\n",
    "                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "                   \"apikey\": CLOUD_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code and run the below cell only if you are running your notebook on a CPD cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cpd = True\n",
    "# WOS_CREDENTIALS = {\n",
    "#     \"url\": \"xxxxx\",\n",
    "#     \"username\": \"xxxxx\",\n",
    "#     \"password\": \"xxxxx\"\n",
    "# }\n",
    "\n",
    "# WML_CREDENTIALS = {\n",
    "#                    \"url\": \"<EDIT THIS>\",\n",
    "#                    \"username\": \"<EDIT THIS>\",\n",
    "#                    \"password\" : \"<EDIT THIS>\",\n",
    "#                    \"instance_id\": \"wml_local\",\n",
    "#                    \"apikey\": \"<EDIT THIS>\",\n",
    "#                    \"version\" : \"4.8\" #If your env is CP4D 4.x.x then specify \"4.x.x\" instead of \"4.8\"\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read project id from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set up a development type subscription, the PTA must be within the project. Please supply the project ID where the PTA needs to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<EDIT THIS>\" # YOUR_PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read space id from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can use an existing space or can create a new space to promote the model. User should choose any of these options with the below variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_existing_space = True # Set it as False if user wants to create a new space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_client = APIClient(WML_CREDENTIALS)\n",
    "wml_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below details are required only if user choose to use an existing space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to use an existing space, User can directly add the space id in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_space_id = \"<EDIT THIS>\" # YOUR_SPACE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below details are required only if user choose to create a new space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_name = \"<EDIT THIS>\" # YOUR_SPACE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: `WML_INSTANCE_NAME`, `WML_CRN` and `COS_RESOURCE_CRN` are required only if you are pointing to your cloud openscale instance, Your `WML_INSTANCE_NAME` and `WML_CRN` can be read from the [Cloud console](https://cloud.ibm.com/resources). From that page, goto `Resours list`, Copy the name of your WML instance listed there. Select the row corresponding to your WML instance, and copy the CRN displayed in the popup on the right side of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Paste your WML_INSTANCE_NAME, WML CRN in the following field and then run this cell.\n",
    "######################################################################################\n",
    "WML_INSTANCE_NAME =  \"<EDIT THIS>\" # YOUR_WML_INSTANCE_NAME\n",
    "WML_CRN =  \"<EDIT THIS>\" # YOUR_WML_CRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell, you will need to paste COS_RESOURCE_CRN. If you haven't worked with COS yet please visit [getting started with COS tutorial](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-getting-started-cloud-object-storage).\n",
    "You can find COS_RESOURCE_CRN variable in Service Credentials in menu of your COS instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COS_RESOURCE_CRN = \"<EDIT THIS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_existing_space == True:\n",
    "    space_id = existing_space_id\n",
    "else:\n",
    "    if use_cpd:\n",
    "        space_meta_data = {\n",
    "            wml_client.spaces.ConfigurationMetaNames.NAME : space_name,\n",
    "            wml_client.spaces.ConfigurationMetaNames.DESCRIPTION : 'tutorial_space'\n",
    "        }\n",
    "    else:\n",
    "        space_meta_data = {\n",
    "            wml_client.spaces.ConfigurationMetaNames.NAME: space_name,\n",
    "            wml_client.spaces.ConfigurationMetaNames.STORAGE: {\"resource_crn\":COS_RESOURCE_CRN},\n",
    "            wml_client.spaces.ConfigurationMetaNames.COMPUTE: {\"name\": WML_INSTANCE_NAME, \"crn\": WML_CRN},\n",
    "            wml_client.spaces.ConfigurationMetaNames.TYPE: \"wx\"\n",
    "        }\n",
    "\n",
    "    space_id = wml_client.spaces.store(\n",
    "        meta_props=space_meta_data)[\"metadata\"][\"id\"]\n",
    "wml_client.set.default_space(space_id)\n",
    "print(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create the access token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates an IAM access token using the provided credentials. The API calls for creating and scoring prompt template assets utilize the token generated by this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "def generate_access_token():\n",
    "    headers={}\n",
    "    if not use_cpd: \n",
    "        headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n",
    "        headers[\"Accept\"] = \"application/json\"\n",
    "        data = {\n",
    "            \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n",
    "            \"apikey\": CLOUD_API_KEY,\n",
    "            \"response_type\": \"cloud_iam\"\n",
    "        }\n",
    "        response = requests.post(IAM_URL + \"/identity/token\", data=data, headers=headers)\n",
    "        json_data = response.json()\n",
    "        iam_access_token = json_data['access_token']\n",
    "    else:\n",
    "        headers[\"Content-Type\"] = \"application/json\"\n",
    "        headers[\"Accept\"] = \"application/json\"\n",
    "        data = {\n",
    "            \"username\":WOS_CREDENTIALS[\"username\"],\n",
    "            \"password\":WOS_CREDENTIALS[\"password\"]\n",
    "        }\n",
    "        data = json.dumps(data).encode(\"utf-8\")\n",
    "        url = WOS_CREDENTIALS[\"url\"] + \"/icp4d-api/v1/authorize\"\n",
    "        response = requests.post(url=url, data=data, headers=headers, ,verify=False)\n",
    "        json_data = response.json()\n",
    "        iam_access_token = json_data['token']      \n",
    "        \n",
    "    return iam_access_token\n",
    "\n",
    "iam_access_token = generate_access_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompt template <a name=\"prompt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a prompt template for a summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_cpd:\n",
    "    credentials={\n",
    "        \"apikey\": CLOUD_API_KEY,\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "    }\n",
    "\n",
    "# Uncomment the code and run the below cell only if you are running your notebook on a CPD cluster.\n",
    "# credentials={\n",
    "#     \"apikey\": WML_CREDENTIALS[\"apikey\"],\n",
    "#     \"url\": WML_CREDENTIALS[\"url\"],\n",
    "#     \"instance_id\": \"openshift\",\n",
    "#     \"username\": WML_CREDENTIALS[\"username\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.prompts import PromptTemplate, PromptTemplateManager\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "prompt_mgr = PromptTemplateManager(\n",
    "                credentials = credentials,\n",
    "                project_id = project_id\n",
    "                )\n",
    "\n",
    "prompt_template = PromptTemplate(name=\"Summarise input\",\n",
    "                                 model_id=ModelTypes.FLAN_UL2,\n",
    "                                 task_ids=[\"summarization\"],\n",
    "                                 input_prefix=\"Human:\",\n",
    "                                 output_prefix=\"Assistant:\",\n",
    "                                 input_text=\"summarize the given content {original_text}\",\n",
    "                                 input_variables=['original_text'])\n",
    "\n",
    "stored_prompt_template = prompt_mgr.store_prompt(prompt_template)\n",
    "project_pta_id = stored_prompt_template.prompt_id\n",
    "project_pta_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt setup <a name=\"ptatsetup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, CloudPakForDataAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "if use_cpd:\n",
    "    authenticator = CloudPakForDataAuthenticator(\n",
    "            url=WOS_CREDENTIALS['url'],\n",
    "            username=WOS_CREDENTIALS['username'],\n",
    "            password=WOS_CREDENTIALS['password'],\n",
    "            disable_ssl_verification=True\n",
    "        )\n",
    "    \n",
    "    wos_client = APIClient(service_url=WOS_CREDENTIALS['url'],authenticator=authenticator)\n",
    "    print(wos_client.version)\n",
    "else:\n",
    "    service_instance_id = None # Update this to refer to a particular service instance\n",
    "    authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY, url = IAM_URL)\n",
    "    wos_client = APIClient(authenticator=authenticator, service_url = SERVICE_URL, service_instance_id = service_instance_id)\n",
    "    print(wos_client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all the available datamarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_marts.show(project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mart_id = \"<EDIT_THIS>\" # YOUR_DATAMART_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openscale instance mapping with the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the authentication is on CPD then we need to add additional step of mapping the project_id/space_id to an OpenScale instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cpd:\n",
    "    wos_client.wos.add_instance_mapping(                \n",
    "                    service_instance_id=data_mart_id,\n",
    "                    project_id=project_id  # OR space_id=<SPACE_ID>\n",
    "                 )\n",
    "    wos_client.wos.add_instance_mapping(                \n",
    "                    service_instance_id=data_mart_id,\n",
    "                    project_id=space_id  # OR space_id=<SPACE_ID>\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the prompt template asset in project for evaluation with supported monitor dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template assets from project is only supported with `development` operational space ID. Running the below cell will create a development type subscription from the prompt template asset created within the project.\n",
    "\n",
    "The available parameters that can be passed for `execute_prompt_setup` function are:\n",
    "\n",
    " * `prompt_template_asset_id` : Id of prompt template asset for which subscription needs to be created.\n",
    " * `label_column` :  The name of the column containing the ground truth or actual labels.\n",
    " * `project_id` : The GUID of the project.\n",
    " * `space_id` : The GUID of the space.\n",
    " * `deployment_id` : (optional) The GUID of the deployment.\n",
    " * `operational_space_id` : The rank of the environment in which the monitoring is happening. Accepted values are `development`, `pre_production`, `production`.\n",
    " * `problem_type` : (optional) The task type to monitor for the given prompt template asset.\n",
    " * `classification_type` : The classification type `binary`/`multiclass` applicable only for `classification` problem (task) type.\n",
    " * `input_data_type` : The input data type.\n",
    " * `supporting_monitors` : Monitor configuration for the subscription to be created.\n",
    " * `background_mode` : When `True`, the promt setup operation will be executed in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"reference_summary\"\n",
    "operational_space_id = \"development\"\n",
    "problem_type= \"summarization\"\n",
    "input_data_type= \"unstructured_text\"\n",
    "\n",
    "\n",
    "monitors = {\n",
    "    \"generative_ai_quality\": {\n",
    "        \"parameters\": {\n",
    "\n",
    "            \"min_sample_size\": 10,\n",
    "            \"metrics_configuration\":{                    \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = wos_client.monitor_instances.mrm.execute_prompt_setup(prompt_template_asset_id = project_pta_id, \n",
    "                                                                   project_id = project_id,\n",
    "                                                                   label_column = label_column, \n",
    "                                                                   operational_space_id = operational_space_id, \n",
    "                                                                   problem_type = problem_type,\n",
    "                                                                   input_data_type = input_data_type, \n",
    "                                                                   supporting_monitors = monitors, \n",
    "                                                                   background_mode = False)\n",
    "\n",
    "result = response.result\n",
    "result._to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the below cell, users can  read the  prompt setup task and check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = wos_client.monitor_instances.mrm.get_prompt_setup(prompt_template_asset_id = project_pta_id,\n",
    "                                                             project_id = project_id)\n",
    "\n",
    "result = response.result\n",
    "result_json = result._to_dict()\n",
    "\n",
    "if result_json[\"status\"][\"state\"] == \"FINISHED\":\n",
    "    print(\"Finished prompt setup : The response is {}\".format(result_json))\n",
    "else:\n",
    "    print(\"prompt setup failed The response is {}\".format(result_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subscription id from prompt setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once prompt setup status is finished, Read the subscription id from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_subscription_id = result_json[\"subscription_id\"]\n",
    "dev_subscription_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all the monitor instances of the production subscription\n",
    "The following cell lists the monitors present in the development subscription along with their respective statuses and other details. Please wait for all the monitors to be in active state before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show(target_target_id = dev_subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk evaluations for PTA subscription <a name=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prompt template subscription\n",
    "\n",
    "For the risk assessment of a development type subscription the user needs to have an evaluation dataset. The risk evaluation function takes the evaluation dataset path as a parameter for evaluation of the configured metric dimensions. If there is a discrepancy between the feature columns in the subscription and the column names in the uploading CSV, users has the option to supply a mapping JSON file to associate the CSV column names with the feature column names in the subscription.\n",
    "\n",
    "\n",
    "**Note:* If you are running this notebook from Watson studio, you may first need to upload your test data to studio and run code snippet to download feedback data file from project to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download summarisation data\n",
    "!rm summarisation.csv\n",
    "!wget https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/summarization/summarisation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"summarisation.csv\"\n",
    "body = None # Please update your mapping file path here if needed\n",
    "\n",
    "# Download feedback data from project to local directory\n",
    "# Run the below code snippet only if you are running the notebook via watson studio\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "if use_cpd:\n",
    "    wslib = access_project_or_space()\n",
    "else:\n",
    "    wslib = access_project_or_space({\"token\":iam_access_token})\n",
    "wslib.download_file(test_data_path)\n",
    "if body:\n",
    "    wslib.download_file(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the MRM monitor instance id\n",
    "\n",
    "Evaluating the test data against the prompt template subscription requires the monitor instance ID of MRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"mrm\"\n",
    "target_target_id = dev_subscription_id\n",
    "result = wos_client.monitor_instances.list(data_mart_id=data_mart_id,\n",
    "                                           monitor_definition_id=monitor_definition_id,\n",
    "                                           target_target_id=target_target_id,\n",
    "                                           project_id=project_id).result\n",
    "result_json = result._to_dict()\n",
    "mrm_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "mrm_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will assess the test data with the subscription of the prompt template asset and produce relevant measurements for the configured monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set_name = \"data\"\n",
    "content_type = \"multipart/form-data\"\n",
    "\n",
    "response  = wos_client.monitor_instances.mrm.evaluate_risk(monitor_instance_id=mrm_monitor_id, \n",
    "                                                    test_data_set_name = test_data_set_name, \n",
    "                                                    test_data_path = test_data_path,\n",
    "                                                    content_type = content_type,\n",
    "                                                    body = body,\n",
    "                                                    project_id = project_id,\n",
    "                                                    background_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the risk evaluation response\n",
    "\n",
    "After initiating the risk evaluation, the evaluation results are now available for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_id, project_id = project_id)\n",
    "response.result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the Model Risk metrics <a name=\"mrmmetric\"></a>\n",
    "\n",
    "Having calculated the measurements for the Foundation Model subscription, the MRM metrics generated for this subscription are now available for your review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_id, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the Generative AI Quality metrics <a name=\"genaimetrics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Monitor instance ID of Generative ai quality metrics is required for reading its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"generative_ai_quality\"\n",
    "result = wos_client.monitor_instances.list(data_mart_id = data_mart_id,\n",
    "                                           monitor_definition_id = monitor_definition_id,\n",
    "                                           target_target_id = target_target_id,\n",
    "                                           project_id = project_id).result\n",
    "result_json = result._to_dict()\n",
    "genaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "genaiquality_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the GenAIQ monitor metrics generated through the risk evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display record level metrics for Generative AI Quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset id for generative ai quality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wos_client.data_sets.list(target_target_id = dev_subscription_id,\n",
    "                                target_target_type = \"subscription\",\n",
    "                                type = \"gen_ai_quality_metrics\").result\n",
    "\n",
    "genaiq_dataset_id = result.data_sets[0].metadata.id\n",
    "genaiq_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying record level metrics for generative ai quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.show_records(data_set_id = genaiq_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot rougel and rougelsum metrics against records <a name=\"plotproject\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wos_client.data_sets.get_list_of_records(data_set_id = genaiq_dataset_id).result\n",
    "result[\"records\"]\n",
    "x = []\n",
    "y_rougel = []\n",
    "y_rougelsum = []\n",
    "for each in result[\"records\"]:\n",
    "    x.append(each[\"metadata\"][\"id\"][-5:]) # Reading only last 5 characters to fit in the display\n",
    "    y_rougel.append(each[\"entity\"][\"values\"][\"rougel\"])\n",
    "    y_rougelsum.append(each[\"entity\"][\"values\"][\"rougelsum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot rougel metrics against records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, y_rougel, marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('X-axis - Record id (last 5 characters)')\n",
    "plt.ylabel('Y-axis - ROUGEL')\n",
    "plt.title('rougel vs record id')\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot rougelsum metrics against records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, y_rougelsum, marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('X-axis - Record id (last 5 characters)')\n",
    "plt.ylabel('Y-axis - ROUGELSUM')\n",
    "plt.title('rougelsum vs record id')\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See factsheets information <a name=\"factsheetsspace\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_cpd:\n",
    "    factsheets_url = \"https://dataplatform.cloud.ibm.com/wx/prompt-details/{}/factsheet?context=wx&project_id={}\".format(project_pta_id, project_id)\n",
    "else:\n",
    "    factsheets_url = factsheets_url = \"{}/wx/prompt-details/{}/factsheet?context=wx&project_id={}\".format(WML_CREDENTIALS[\"url\"],project_pta_id, project_id)\n",
    "print(\"User can navigate to the published facts in project {}\".format(factsheets_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Prompt template from space <a name=\"evaluatespace\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick recap of what we have done so far.\n",
    "\n",
    "1. We've created a prompt template asset in project.\n",
    "2. We've created a `development` type subscription of prompt template asset in OpenScale.\n",
    "3. Configured monitors supported by OpenScale for the subscriptions.\n",
    "4. We've performed risk evaluations against the PTA susbscription with a sample set of test data.\n",
    "5. Displayed the metrics generated with the risk evaluation.\n",
    "6. Displayed the factsheets information for the subscription.\n",
    "\n",
    "Now, we can promote the created prompt template asset to space and perform similar actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promote PTA to space <a name=\"promottospace\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell promotes the prompt template asset from the project to the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"*/*\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(iam_access_token)\n",
    "verify = True\n",
    "if use_cpd:\n",
    "    DATAPLATFORM_URL = WOS_CREDENTIALS[\"url\"]\n",
    "    verify = False\n",
    "url = \"{}/v2/assets/{}/promote\".format(DATAPLATFORM_URL ,project_pta_id)\n",
    "\n",
    "params = {\n",
    "    \"project_id\":project_id\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"space_id\": space_id\n",
    "}\n",
    "response = requests.post(url, json=payload, headers=headers, params = params, verify = verify)\n",
    "json_data = response.json()\n",
    "space_pta_id = json_data[\"metadata\"][\"asset_id\"]\n",
    "space_pta_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create deployment for prompt template asset in space <a name=\"ptadeployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a subscription from space, it is necessary to create a deployment for prompt template assets in spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENTS_URL = WML_CREDENTIALS[\"url\"] + \"/ml/v4/deployments\"\n",
    "\n",
    "serving_name = \"<EDIT_THIS>\" # eg: summary_deployment\n",
    "\n",
    "payload = {\n",
    "    \"prompt_template\": {\n",
    "      \"id\": space_pta_id\n",
    "    },\n",
    "    \"online\": {\n",
    "       \"parameters\": {\n",
    "         \"serving_name\": serving_name\n",
    "       }\n",
    "    },\n",
    "    \"base_model_id\": \"google/flan-ul2\",\n",
    "    \"description\": \"summarization deployment\",\n",
    "    \"name\": \"summarization deployment\",\n",
    "    \"space_id\": space_id\n",
    "}\n",
    "\n",
    "version = \"2023-07-07\" # The version date for the API of the form YYYY-MM-DD. Example : 2023-07-07\n",
    "params = {\n",
    "    \"version\":version,\n",
    "    \"space_id\":space_id\n",
    "}\n",
    "\n",
    "response = requests.post(DEPLOYMENTS_URL, json=payload, headers=headers, params = params, verify = verify)\n",
    "json_data = response.json()\n",
    "\n",
    "\n",
    "if \"metadata\" in json_data:\n",
    "    deployment_id = json_data[\"metadata\"][\"id\"]\n",
    "    print(deployment_id)\n",
    "else:\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the prompt template asset in space for evaluation with supported monitor dimensions <a name=\"ptaspace\"></a>\n",
    "\n",
    "The prompt template assets from space is only supported with [`pre_production` and `production`] operational space IDs. Running the below cell will create a `production` type subscription from the prompt template asset promoted to the space. The `problem_type` value should depend on the task type specified in the prompt template asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"reference_summary\"\n",
    "operational_space_id = \"production\"\n",
    "problem_type= \"summarization\"\n",
    "input_data_type= \"unstructured_text\"\n",
    "\n",
    "monitors = {\n",
    "    \"generative_ai_quality\": {\n",
    "        \"parameters\": {\n",
    "            \"min_sample_size\": 10,\n",
    "            \"metrics_configuration\":{        \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"drift_v2\": {\n",
    "        \"thresholds\": [\n",
    "            {\n",
    "                \"metric_id\": \"confidence_drift_score\",\n",
    "                \"type\": \"upper_limit\",\n",
    "                \"value\": 0.05\n",
    "            },\n",
    "            {\n",
    "                \"metric_id\": \"prediction_drift_score\",\n",
    "                \"type\": \"upper_limit\",\n",
    "                \"value\": 0.05\n",
    "            },\n",
    "            {\n",
    "                \"metric_id\": \"input_metadata_drift_score\",\n",
    "                \"specific_values\": [\n",
    "                    {\n",
    "                        \"applies_to\": [\n",
    "                            {\n",
    "                                \"type\": \"tag\",\n",
    "                                \"value\": \"subscription\",\n",
    "                                \"key\": \"field_type\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"value\": 0.05\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"upper_limit\"\n",
    "            },\n",
    "            {\n",
    "                \"metric_id\": \"output_metadata_drift_score\",\n",
    "                \"specific_values\": [\n",
    "                    {\n",
    "                        \"applies_to\": [\n",
    "                            {\n",
    "                                \"type\": \"tag\",\n",
    "                                \"value\": \"subscription\",\n",
    "                                \"key\": \"field_type\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"value\": 0.05\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"upper_limit\"\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"min_samples\": 10,\n",
    "            \"train_archive\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = wos_client.monitor_instances.mrm.execute_prompt_setup(prompt_template_asset_id = space_pta_id, \n",
    "                                                                   space_id = space_id,\n",
    "                                                                   deployment_id = deployment_id,\n",
    "                                                                   label_column = label_column, \n",
    "                                                                   operational_space_id = operational_space_id, \n",
    "                                                                   problem_type = problem_type,\n",
    "                                                                   input_data_type = input_data_type, \n",
    "                                                                   supporting_monitors = monitors, \n",
    "                                                                   background_mode = False)\n",
    "\n",
    "result = response.result\n",
    "result._to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the below cell, users can read the prompt setup task and check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = wos_client.monitor_instances.mrm.get_prompt_setup(prompt_template_asset_id = space_pta_id,\n",
    "                                                             deployment_id = deployment_id,\n",
    "                                                             space_id = space_id)\n",
    "\n",
    "result = response.result\n",
    "result_json = result._to_dict()\n",
    "result_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subscription id from prompt setup\n",
    "\n",
    "Once prompt setup status is finished, Read the subscription id from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_subscription_id = result_json[\"subscription_id\"]\n",
    "prod_subscription_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the PTA deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the scoring URL of the deployment from the subscription details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_details = wos_client.subscriptions.get(prod_subscription_id).result\n",
    "sub_details = sub_details._to_dict()\n",
    "scoring_url = sub_details[\"entity\"][\"deployment\"][\"url\"]\n",
    "if not scoring_url.find(\"?version=\") != -1:\n",
    "    scoring_url = scoring_url.strip() + \"?version=2023-09-07\"\n",
    "\n",
    "if use_cpd:\n",
    "    scoring_url = WML_CREDENTIALS[\"url\"] + \"/ml/v1-beta/deployments/\"+ deployment_id +\"/generation/text?version=2023-09-07\"\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the model so that we can configure monitors <a name=\"score\"></a>\n",
    "\n",
    "Now that the WML service has been bound and the subscription has been created, we need to score the prompt template asset. User needs to generate the test data content in JSON format from the downloaded CSV file. This is used to construct the payload for scoring the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "feature_fields = [\"original_text\"]\n",
    "prediction = \"generated_text\"\n",
    "\n",
    "headers={}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"*/*\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(iam_access_token)\n",
    "\n",
    "pl_data = []\n",
    "prediction_list = []\n",
    "with open(test_data_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        request = {\n",
    "            \"parameters\": {\n",
    "                \"template_variables\": {\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for each in feature_fields:\n",
    "            request[\"parameters\"][\"template_variables\"][each] = str(row[each])\n",
    "\n",
    "        response = requests.post(scoring_url, json=request, headers=headers).json()\n",
    "        predicted_val = response[\"results\"][0][prediction]\n",
    "        prediction_list.append(predicted_val)\n",
    "        record = {\"request\":request, \"response\":response}\n",
    "        pl_data.append(record)\n",
    "    \n",
    "pl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating additional payload data to enable drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable drift there should be minimum 100 records in the payload table. The below cell duplicates the scored records and create another 100 records for adding to the payload table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "additional_pl_data = copy.copy(pl_data)\n",
    "additional_pl_data *= 10\n",
    "print(\"Generated {} additional payload data\".format(len(additional_pl_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding payload data\n",
    "\n",
    "Below cell reads the payload data set id from the subscription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "\n",
    "time.sleep(5)\n",
    "payload_data_set_id = None\n",
    "payload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n",
    "                                                target_target_id=prod_subscription_id, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
    "if payload_data_set_id is None:\n",
    "    print(\"Payload data set not found. Please check subscription status.\")\n",
    "else:\n",
    "    print(\"Payload data set id: \", payload_data_set_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additioanl payload data to enable drift V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=additional_pl_data,background_mode=False)\n",
    "time.sleep(5)\n",
    "pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a total of 110 records should be available within the payload table. But in case if auto payload logging fails to transmit the scored records to the payload logging table, the following code can be used to manually add payload data to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from ibm_watson_openscale.supporting_classes.payload_record import PayloadRecord\n",
    "time.sleep(5)\n",
    "pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))\n",
    "if pl_records_count < 110:\n",
    "    print(\"Payload logging did not happen, performing explicit payload logging.\")\n",
    "    wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=pl_data,background_mode=False)\n",
    "    time.sleep(5)\n",
    "    pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding feedback data\n",
    "\n",
    "Below cell reads the feedback data set id from the subscription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "\n",
    "time.sleep(5)\n",
    "feedback_data_set_id = None\n",
    "feedback_data_set_id = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, \n",
    "                                                target_target_id=prod_subscription_id, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
    "if feedback_data_set_id is None:\n",
    "    print(\"Feedback data set not found. Please check subscription status.\")\n",
    "else:\n",
    "    print(\"Feedback data set id: \", feedback_data_set_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code generates feedback data based on the downloaded CSV file and the scored response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_data_content = []\n",
    "csv_file_path = \"summarisation.csv\"\n",
    "\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row, prediction_val in zip(csv_reader, prediction_list):\n",
    "\n",
    "        # Read each row from the CSV and add label and prediction values\n",
    "        result_row = []\n",
    "        result_row = [row[key] for key in feature_fields if key in row]\n",
    "        result_row.append(row[label_column])\n",
    "        result_row.append(prediction_val)\n",
    "\n",
    "        test_data_content.append(result_row)\n",
    "if len(test_data_content) == 10: # 10 records are there in the downloaded CSV\n",
    "    print(\"generated feedback data from CSV\")\n",
    "else:\n",
    "    print(\"Failed to generated feedback data from CSV, Kindly verify the CSV file content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = feature_fields\n",
    "fields.append(label_column)\n",
    "fields.append(\"_original_prediction\")\n",
    "feedback_data = [\n",
    "    {\n",
    "        \"fields\": fields,\n",
    "        \"values\": test_data_content\n",
    "    }\n",
    "]\n",
    "feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to manually add feedback data to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.store_records(data_set_id=feedback_data_set_id, request_body=feedback_data,background_mode=False)\n",
    "time.sleep(5)\n",
    "fb_records_count = wos_client.data_sets.get_records_count(feedback_data_set_id)\n",
    "# Adding time delay to enable drift\n",
    "time.sleep(10)\n",
    "print(\"Number of records in the feedback logging table: {}\".format(fb_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all the monitor instances of the production subscription\n",
    "The following cell lists the monitors present in the production subscription along with their respective statuses and other details. Please wait for all the monitors to be in active state before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show(target_target_id = prod_subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the MRM monitor instance ID of PTA subscription deployed in space\n",
    "\n",
    "Evaluating the test data against the prompt template subscription requires the monitor instance ID of MRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"mrm\"\n",
    "target_target_id = prod_subscription_id\n",
    "result = wos_client.monitor_instances.list(data_mart_id=data_mart_id,\n",
    "                                           monitor_definition_id=monitor_definition_id,\n",
    "                                           target_target_id=target_target_id,\n",
    "                                           space_id=space_id).result\n",
    "result_json = result._to_dict()\n",
    "mrm_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "mrm_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prompt template subscription from space\n",
    "\n",
    "The following cell will assess subscription of the prompt template asset and produce relevant measurements for the configured monitor. The data to be evaluated are already uploaded to payload and feedback table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response  = wos_client.monitor_instances.mrm.evaluate_risk(monitor_instance_id=mrm_monitor_id, \n",
    "                                                    body = body,\n",
    "                                                    space_id = space_id,\n",
    "                                                    evaluation_tests = [\"model_health\", \"drift_v2\", \"generative_ai_quality\"],\n",
    "                                                    background_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the risk evaluation response\n",
    "\n",
    "After initiating the risk evaluation, the evaluation results of PTA from space are now available for review,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_id, space_id = space_id)\n",
    "response.result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Model Risk metrics\n",
    "\n",
    "Having calculated the measurements for the Foundation Model subscription, the MRM metrics generated for this subscription are now available for your review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_id, space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Generative AI Quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Generative ai quality monitor instance id\n",
    "\n",
    "Monitor instance ID of Generative ai quality metrics is required for reading its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"generative_ai_quality\"\n",
    "result = wos_client.monitor_instances.list(data_mart_id = data_mart_id,\n",
    "                                           monitor_definition_id = monitor_definition_id,\n",
    "                                           target_target_id = target_target_id,\n",
    "                                           space_id = space_id).result\n",
    "result_json = result._to_dict()\n",
    "genaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "genaiquality_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the monitor metrics of GenAIQ generated through the risk evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display record level metrics for Generative AI Quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset id for generative ai quality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wos_client.data_sets.list(target_target_id = prod_subscription_id,\n",
    "                                target_target_type = \"subscription\",\n",
    "                                type = \"gen_ai_quality_metrics\").result\n",
    "\n",
    "genaiq_dataset_id = result.data_sets[0].metadata.id\n",
    "genaiq_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying record level metrics for generative ai quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.show_records(data_set_id = genaiq_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot rougel and rougelsum metrics against records <a name=\"plotspace\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wos_client.data_sets.get_list_of_records(data_set_id = genaiq_dataset_id).result\n",
    "result[\"records\"]\n",
    "x = []\n",
    "y_rougel = []\n",
    "y_rougelsum = []\n",
    "for each in result[\"records\"]:\n",
    "    x.append(each[\"metadata\"][\"id\"][-5:]) # Reading only last 5 characters to fit in the display\n",
    "    y_rougel.append(each[\"entity\"][\"values\"][\"rougel\"])\n",
    "    y_rougelsum.append(each[\"entity\"][\"values\"][\"rougelsum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot rougel metrics against records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, y_rougel, marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('X-axis - Record id (last 5 characters)')\n",
    "plt.ylabel('Y-axis - ROUGEL')\n",
    "plt.title('rougel vs record id')\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot rougelsum metrics against records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, y_rougelsum, marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('X-axis - Record id (last 5 characters)')\n",
    "plt.ylabel('Y-axis - ROUGELSUM')\n",
    "plt.title('rougelsum vs record id')\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Drift V2 metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Drift V2 monitor instance id\n",
    "\n",
    "Monitor instance ID of Drift V2 metrics is required for reading its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"drift_v2\"\n",
    "result = wos_client.monitor_instances.list(data_mart_id = data_mart_id,\n",
    "                                           monitor_definition_id = monitor_definition_id,\n",
    "                                           target_target_id = target_target_id,\n",
    "                                           space_id = space_id).result\n",
    "result_json = result._to_dict()\n",
    "drift_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "drift_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the monitor metrics of Drift V2 generated through the risk evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=drift_monitor_id, space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User can navigate to see the published facts in space <a name=\"factsheetsproject\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_cpd:\n",
    "    factsheets_url = \"https://dataplatform.cloud.ibm.com/ml-runtime/deployments/{}/details?space_id={}&context=wx&flush=true\".format(deployment_id, space_id)\n",
    "else:\n",
    "    factsheets_url = \"{}/ml-runtime/deployments/{}/details?space_id={}&context=wx&flush=true\".format(WML_CREDENTIALS[\"url\"], deployment_id, space_id)\n",
    "    \n",
    "print(\"User can navigate to the published facts in space {}\".format(factsheets_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have finished the hands-on lab for IBM Watson OpenScale. You can now navigate to the prompt template asset in your project / spaceand click on the Evaluate tab to visualise the results on the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
