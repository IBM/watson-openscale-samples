{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97648ee8",
   "metadata": {
    "id": "a81be50301fa471b8417773161ab8ca8"
   },
   "source": [
    "# Use the IBM watsonx.governance metrics toolkit to evaluate Google Vertex AI\n",
    "\n",
    "The IBM watsonx.governance metrics toolkit lets you evaluate the output of a Large Language Model (LLM) against multiple task types: Text Summarization, Content Generation, Question Answering, Text Classification, Entity Extraction, and Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "This notebook will demonstrate how to evaluate output from a Text Summarization prompt run against a Google Vertex AI LLM.\n",
    "\n",
    "- The prompt is generated against a Vertex AI Gemini Pro Predictions model endpoint.\n",
    "- Prompt output is evaluated using IBM's watsonx.governance metrics toolkit.\n",
    "- Evaluated metrics are published to IBM OpenPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9eebc6",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Create your prompt for testing against the `Gemini Pro Predictions` model.\n",
    "-  Configure metrics for evaluation.\n",
    "-  Run the metrics against your prompt data.\n",
    "-  Print and review the metrics returned by the IBM watsonx.governance metrics toolkit. \n",
    "-  Publish the computed metrics to IBM OpenPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eab566",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Install the necessary packages](#packages)\n",
    "2.  [Provision services and configure credentials](#credentials)\n",
    "3.\t[Evaluate Text Summarization output from the Google Vertex AI `Gemini Pro Predictions` model](#summarization)\n",
    "4.\t[Publish computed metrics to an OpenPages foundation model](#openpages)\n",
    "5.  [Navigate to the OpenPages UI to verify your metrics](#verify)\n",
    "6.\t[Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63949a7",
   "metadata": {},
   "source": [
    "<a id=\"packages\"></a>\n",
    "## Step 1 - Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d069ed-a362-466d-9059-897e80a95169",
   "metadata": {
    "id": "6e9cd5a7-9437-47d0-a1fb-b3d6f963cec9"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146235d-f447-4cbe-b2ea-69d58cd9b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ ibm-watson-openscale==3.0.34.8  | tail -n 1\n",
    "!pip install -i https://test.pypi.org/simple/ ibm-metrics-plugin==5.0.1.17  | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade evaluate --no-cache | tail -n 1\n",
    "!pip install --upgrade textstat --no-cache | tail -n 1\n",
    "!pip install --upgrade sacrebleu --no-cache | tail -n 1\n",
    "!pip install --upgrade sacremoses --no-cache | tail -n 1\n",
    "!pip install --upgrade datasets==2.10.0 --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b6e41-d67e-4b56-831e-4893eece1cdd",
   "metadata": {},
   "source": [
    "Optional `pip` installs, as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe6913-c27c-46ad-a5ea-6409f712a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18055bc6-c18f-4422-af5d-19a2997c8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm_db_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b6c23-91e6-4fcf-91ee-588dc0e72eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2081f-98bc-403f-aac3-0164c3a40a10",
   "metadata": {
    "id": "4267839e32bd48719cf4b3389e3cadb8"
   },
   "source": [
    "<a id=\"credentials\"></a>\n",
    "## Step 2 - Provision services and configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0e2e6-c441-42ae-84dc-70f809aa27b3",
   "metadata": {
    "id": "3e34c78b4df9420d84eb4aeaf887ebd5"
   },
   "source": [
    "### Provision an instance of IBM Watson OpenScale\n",
    "\n",
    "If you have not already done so, provision an instance of IBM Watson OpenScale using the [OpenScale link in the Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312b7ec-4624-460a-8080-639255c6df40",
   "metadata": {
    "id": "0565e9abc066411ea4d2aff075c434f2"
   },
   "source": [
    "### Generate an API key\n",
    "\n",
    "You can generate a Cloud API key with IBM Cloud console or with IBM Cloud command line interface.\n",
    "\n",
    "To generate an API key by using IBM Cloud console:\n",
    "\n",
    "1. Go to the [**Users** section of the IBM Cloud console](https://cloud.ibm.com/iam#/users).\n",
    "1. Click your name, then scroll down to the **API Keys** section.\n",
    "1. Click **Create an IBM Cloud API key**.\n",
    "1. Give your key a name and click **Create**.\n",
    "1. Copy the created key - you will need to paste this key into the `CLOUD_API_KEY` variable in the \"Configure your credentials\" section below.\n",
    "\n",
    "To create an API key using the IBM Cloud [command line interface](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli):\n",
    "\n",
    "1. From the command line interface, type the following:\n",
    "\n",
    "    `bx login --sso`\n",
    "\n",
    "    `bx iam api-key-create 'my_key'`\n",
    "\n",
    "1. Copy the created key - you will need to paste this key into the `CLOUD_API_KEY` variable in the \"Configure your credentials\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb3bbf",
   "metadata": {},
   "source": [
    "### Configure your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d96be6",
   "metadata": {
    "id": "f78243b20abd4b4d84313dacb4f02624"
   },
   "outputs": [],
   "source": [
    "use_cpd = False\n",
    "CLOUD_API_KEY = \"<Your IBM API Key>\"\n",
    "IAM_URL=\"https://iam.ng.bluemix.net/oidc/token\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f09a6-41ae-40d2-9661-f9c71d2565e3",
   "metadata": {},
   "source": [
    "If you are running your notebook on a CPD cluster, uncomment and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c46ce-4958-45be-90fa-ec7d95c6d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cpd = True\n",
    "# WOS_CREDENTIALS = {\n",
    "#     \"url\": \"xxxxx\",\n",
    "#     \"username\": \"xxxxx\",\n",
    "#     \"api_key\": \"xxxxx\"\n",
    "# }\n",
    "\n",
    "# GEN_API_KEY = WOS_CREDENTIALS[\"api_key\"]\n",
    "\n",
    "# api_endpoint = WOS_CREDENTIALS[\"url\"]\n",
    "# project_id = \"<Your project id>\"\n",
    "# endpoint_url = WOS_CREDENTIALS[\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d249c",
   "metadata": {},
   "source": [
    "<a id=\"summarization\"></a>\n",
    "## Step 3 - Evaluate Text Summarization output from the Google Vertex AI `Gemini Pro Predictions` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cb9dc-0a26-4320-8a27-476f0a4dadcd",
   "metadata": {
    "id": "abab31d97a0a460a90897da08f36add0"
   },
   "source": [
    "### Download a dataset containing prompt input data for model inferencing and reference data for model output evaluation\n",
    "\n",
    "The downloaded `.csv` file contains: input, a generated summary, and two reference summaries each for 50 sample prompts. Values are then further converted to input, output, and reference panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f284b78",
   "metadata": {
    "id": "8f63311275114169ac00dc3096352dd0"
   },
   "outputs": [],
   "source": [
    "!rm -fr llm_content.csv\n",
    "!wget \"https://raw.githubusercontent.com/IBM/watson-openscale-samples/main/IBM%20Cloud/WML/assets/data/watsonx/llm_content.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2169",
   "metadata": {
    "id": "c03c61b496f64edc8b9bd3fc3ff03899"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "llm_data_all = pd.read_csv(\"llm_content.csv\")\n",
    "llm_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data = llm_data_all.tail(10)\n",
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bcc4a-1691-4adc-a440-d1b8902cd060",
   "metadata": {},
   "source": [
    "### Configure Vertex AI\n",
    "\n",
    "**Note**: There are multiple ways to invoke Vertex AI model predictions and multiple ways to generate the Vertex AI token. This notebook demostrates how to generate the service account token and call the LLM predictions API using a REST API. You may also consider using the `gcloud` toolkit, specifically using the `gcloud auth print-access-token` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972a998-0805-4173-b7b8-02103e9cc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the project id from your google cloud account\n",
    "PROJECT_ID = \"<Your GCP Project ID>\"\n",
    "\n",
    "# the location of the project\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "# the large language model - model id\n",
    "MODEL_ID = \"gemini-pro\"\n",
    "\n",
    "# service account credentials obtained from GCP Credentials Page\n",
    "gcp_json_credentials_dict = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"brave-healer-xxxx\",\n",
    "  \"private_key_id\": \"xxxx\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\xxxx==\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"xxxx@brave-healer-xxxx.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"xxxx\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/xxxxx.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "# predictions URL\n",
    "vertexai_predictions_url = 'https://us-central1-aiplatform.googleapis.com/v1/projects/{0}/locations/us-central1/publishers/google/models/{1}:streamGenerateContent'.format(PROJECT_ID, MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53402447-38d6-4fed-a281-609ee851ed3c",
   "metadata": {},
   "source": [
    "#### Initialize the Google Cloud AI platform with the project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bf5d8-3a0c-4f8e-952a-a482de2fb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfb694-3ceb-4af9-a7eb-8012cf5a9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b57331-7727-4d40-9e5a-f68838b66e68",
   "metadata": {},
   "source": [
    "#### Generate the Vertex AI token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc6d18-369a-4972-96c3-a02fd46c6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertexai_token():\n",
    "    from google.oauth2 import service_account\n",
    "    import google.auth.transport.requests\n",
    "    import google\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        gcp_json_credentials_dict, \n",
    "        scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "    request = google.auth.transport.requests.Request()\n",
    "    credentials.refresh(request)\n",
    "    token = credentials.token\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98527327-e25c-47a9-b7fe-b17123775876",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai_token = get_vertexai_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2314cdd-f742-48ec-880b-ee24c74ce5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(vertexai_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef6e8a-0dc2-4d99-92e6-3d41bbf10adb",
   "metadata": {},
   "source": [
    "### Create your prompt for testing against the `Gemini Pro Predictions` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495edbc-26b3-4471-9386-d3f2a80d3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(text):\n",
    "    prompt = f\"\"\"Please provide a summary of the following text with maximum of 20 words.\n",
    "    \n",
    "{text}\n",
    "    \n",
    "Summary:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bdd58-3cdb-428f-935a-6e3337b8af6f",
   "metadata": {},
   "source": [
    "### Run the prompt evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9ba65-b6bc-46ea-8292-2e5053931904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "def get_completion(prompt_text):\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": get_prompt(prompt_text)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"generation_config\": {\n",
    "            \"temperature\": 0.2,\n",
    "            \"maxOutputTokens\": 20\n",
    "          }    \n",
    "    }\n",
    "    response = requests.post(vertexai_predictions_url, headers=headers, json=payload, verify=False)\n",
    "    json_data = response.json()\n",
    "    prompt_output = json_data[0]['candidates'][0]['content']['parts'][0]['text']\n",
    "    return prompt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7257e2-2cc7-4138-bb1b-9f2eb29df423",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Scientists have discovered a new species of deep-sea fish that emits a soft, soothing light. This bioluminescent fish could inspire advancements in low-light underwater exploration.\"\n",
    "'''\n",
    "output = get_completion(text)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782dc07-295e-4124-aebb-3df5ec3e32b6",
   "metadata": {},
   "source": [
    "### Set the generated prompt summary with the summary from the Vertex AI `Gemini Pro Predictions` prompt evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddb55b-3444-43e4-bca3-1be2b0bfdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data['vertexai_gemini_pro_generated_summary'] = llm_data['input_text'].apply(get_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b51704-6f49-4783-9eb8-505a3b0086a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c789ea-083c-4c5c-bf80-9dc1f3f92280",
   "metadata": {},
   "source": [
    "#### Sample generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6e740-fa99-43eb-a8ce-5b9b3ce7626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data['vertexai_gemini_pro_generated_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1d906",
   "metadata": {
    "id": "d5d5b8b3dd4847298dee4b065ee9c3d4"
   },
   "source": [
    "### Authenticate with IBM watsonx.governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0ce8d",
   "metadata": {
    "id": "9567c5c157ae44ecabd9fdc434f16056"
   },
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator,BearerTokenAuthenticator,CloudPakForDataAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "if use_cpd:\n",
    "    authenticator = CloudPakForDataAuthenticator(\n",
    "            url=WOS_CREDENTIALS['url'],\n",
    "            username=WOS_CREDENTIALS['username'],\n",
    "            password=WOS_CREDENTIALS['password'],\n",
    "            disable_ssl_verification=True\n",
    "        )\n",
    "    \n",
    "    client = APIClient(service_url=WOS_CREDENTIALS['url'],authenticator=authenticator)\n",
    "    print(client.version)\n",
    "else:\n",
    "    authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\n",
    "    client = APIClient(authenticator=authenticator)\n",
    "    print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45542ea",
   "metadata": {
    "id": "d1aa94f70d4d4f77bc87a0b06139af3c"
   },
   "source": [
    "### Import common evaluation metrics and metric groups\n",
    "\n",
    "These are the metrics used to evaluate your prompt against the selected model, based on the prompt task type. For example, Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dc6b8",
   "metadata": {
    "id": "76620bd0a7784cb4801d10f3d93fa6a1"
   },
   "outputs": [],
   "source": [
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMTextMetricGroup\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMGenerationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMSummarizationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMQAMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMClassificationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import HAP_SCORE\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import PII_DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db10c6",
   "metadata": {
    "id": "a68e9be2a0a0438d822679c5c1728097"
   },
   "source": [
    "### Get the necessary data for evaluating the prompt template metrics\n",
    "\n",
    "Metrics will be evaluated for the input, output, and reference summary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c385f",
   "metadata": {
    "id": "091c264708cc4159969a4690daf07886"
   },
   "outputs": [],
   "source": [
    "df_input = llm_data[['input_text']].copy()\n",
    "df_output = llm_data[['vertexai_gemini_pro_generated_summary']].copy()\n",
    "df_reference = llm_data[['reference_summary_2']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874eaf3-338a-4ec5-851d-2526b2820044",
   "metadata": {},
   "source": [
    "### Evaluate custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977f2ca-48d9-40b6-89d9-b9383f5d5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938696f-906e-4306-ae8e-bc9623dd9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def extract_key_words(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb898e8-708a-4746-80ee-83f335e221ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(reference_keywords, generated_keywords):\n",
    "    common_keywords = set(reference_keywords) & set(generated_keywords)\n",
    "\n",
    "    precision = len(common_keywords) / len(generated_keywords) if len(generated_keywords) > 0 else 0\n",
    "    recall = len(common_keywords) / len(reference_keywords) if len(reference_keywords) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7812aa0-f85c-430b-961a-77db067d6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_averages_f1_score(precisions, recalls, f1_scores):\n",
    "    import numpy as np\n",
    "    precision = round(np.min(precisions), 4)\n",
    "    recall = round(np.min(recalls), 4)\n",
    "    f1_score = round(np.min(f1_scores), 4)\n",
    "\n",
    "    keyword_inclusions = {\n",
    "        \"keyword_inclusions\" : {\n",
    "            \"precision\": {\n",
    "                \"metric_value\": precision\n",
    "            },\n",
    "            \"recall\": {\n",
    "                \"metric_value\": recall\n",
    "            },\n",
    "            \"f1_score\": {\n",
    "                \"metric_value\": f1_score\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return keyword_inclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcbdef-646c-4a44-9337-a18540b4012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_word_inclusions(df_input, df_output, df_reference):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for input_text, generated_summary in zip(df_input['input_text'], df_output['vertexai_gemini_pro_generated_summary']):\n",
    "    \n",
    "        input_text_keywords = extract_key_words(input_text)\n",
    "        print('Input Text Keywords: '+ str(input_text_keywords))\n",
    "    \n",
    "        generated_summary_keywords = extract_key_words(generated_summary)\n",
    "        print('Generated Summary Keywords: '+ str(generated_summary_keywords))\n",
    "        \n",
    "        precision, recall, f1_score = compute_f1_score(input_text_keywords, generated_summary_keywords)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        print('\\n')\n",
    "    \n",
    "    keyword_inclusions = compute_averages_f1_score(precisions, recalls, f1_scores)\n",
    "    return keyword_inclusions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7f842",
   "metadata": {
    "id": "74a9de9b2bb84c35b87fa1d2abc5770c"
   },
   "source": [
    "### Configure metrics for evaluation\n",
    "\n",
    "Select the metrics you want to evaluate; the code cell below contains 10 common Summarization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8262dfe-38b1-4d6f-88dd-c3a4c3b11996",
   "metadata": {
    "id": "95e9409a-ccb9-409b-ae04-b0bdf392faac"
   },
   "outputs": [],
   "source": [
    "metric_config = {   \n",
    "    \"configuration\": {\n",
    "        LLMTextMetricGroup.SUMMARIZATION.value: {\n",
    "            LLMSummarizationMetrics.ROUGE_SCORE.value: {},\n",
    "            LLMSummarizationMetrics.SARI.value: {},\n",
    "            LLMSummarizationMetrics.METEOR.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_RECALL.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_PRECISION.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_F1_SCORE.value: {},\n",
    "            LLMSummarizationMetrics.COSINE_SIMILARITY.value: {},\n",
    "            LLMSummarizationMetrics.JACCARD_SIMILARITY.value: {},\n",
    "            LLMSummarizationMetrics.BLEU.value: {},\n",
    "            LLMSummarizationMetrics.FLESCH.value: {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240deb44",
   "metadata": {
    "id": "f55e2339070c48608474f370abd949b7"
   },
   "source": [
    "### Summarization metrics evaluation\n",
    "\n",
    "Run the metrics against your prompt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb037b-ff41-492f-ab4e-5fdb35d4da03",
   "metadata": {
    "id": "031a7f52-ba90-46d4-aac6-09d8f76e6d3f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = client.llm_metrics.compute_metrics(metric_config, \n",
    "                                            sources = df_input, \n",
    "                                            predictions = df_output, \n",
    "                                            references = df_reference, \n",
    "                                            custom_evaluators = [key_word_inclusions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12d146",
   "metadata": {
    "id": "4bb4d7ebf78a4589b122508635c92572"
   },
   "source": [
    "### Review metrics\n",
    "\n",
    "Print and review the metrics returned by the IBM watsonx.governance toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fd1da-329e-40e0-bce1-f99bb37dea3c",
   "metadata": {
    "id": "fd2fa6e4-c832-4ed6-bb10-8bae433e95f6"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(result,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6477f",
   "metadata": {},
   "source": [
    "<a id=\"openpages\"></a>\n",
    "## Step 4 - Publish computed metrics to an OpenPages foundation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b637a3-c1af-41c7-abe5-fb06ec2cc078",
   "metadata": {},
   "source": [
    "### Construct a key/value dictionary of the metrics to be published to OpenPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7597d-9ffd-4751-8558-06e13c0457c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(result):\n",
    "    metrics = {}\n",
    "    metrics['rouge1'] = round(result['rouge_score']['rouge1']['metric_value'], 4)\n",
    "    metrics['rouge2'] = round(result['rouge_score']['rouge2']['metric_value'], 4)\n",
    "    metrics['rougeL'] = round(result['rouge_score']['rougeL']['metric_value'], 4)\n",
    "    metrics['rougeLsum'] = round(result['rouge_score']['rougeLsum']['metric_value'], 4)\n",
    "    metrics['meteor'] = round(result['meteor']['metric_value'], 4)\n",
    "    metrics['sari'] = round(result['sari']['metric_value'], 4)\n",
    "    metrics['cosine_similarity'] = round(result['cosine_similarity']['metric_value'], 4)\n",
    "    metrics['keyword_inclusions_f1_score'] = round(result['keyword_inclusions']['f1_score']['metric_value'], 4)\n",
    "    # metrics['jaccard_similarity'] = round(result['jaccard_similarity']['metric_value'], 4)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9089ad-03d8-46eb-a40d-9c7c262a2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =  get_metrics(result)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba3063-ef89-4d12-b593-e9c126bf0f0e",
   "metadata": {},
   "source": [
    "### Create an authorization token for OpenPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd986296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_auth_token(username, password):\n",
    "    token = base64.b64encode(bytes('{0}:{1}'.format(username, password), 'utf-8')).decode(\"ascii\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7c85d-0365-46a3-bbef-c11385308e05",
   "metadata": {},
   "source": [
    "### For a given model name, get the OpenPages model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154e903-6691-4e6a-be29-adef889b4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_op_model_id(header, model_name):\n",
    "    openpages_url = OP_URL.rstrip(\"/\") + \"/grc/api/query\"\n",
    "    # Prepare post payload\n",
    "    get_id_payload = {\n",
    "        \"statement\": \"SELECT [Model].[Resource ID] FROM [Model] WHERE [Model].[Name] IN ('{0}')\".format(model_name),\n",
    "        \"skipCount\": 0\n",
    "    }\n",
    "    response = requests.post(openpages_url, json=get_id_payload, headers=header, verify=False).json()\n",
    "\n",
    "    model_id = None\n",
    "    if response is not None:\n",
    "        if response.get(\"rows\") is not None:\n",
    "            rows = response.get(\"rows\")\n",
    "            if len(rows) != 0:\n",
    "                fields = rows[0].get(\"fields\")\n",
    "                if fields is not None:\n",
    "                    field = fields.get(\"field\")\n",
    "                    if len(field) != 0:\n",
    "                        model_id = field[0][\"value\"]\n",
    "\n",
    "    if model_id is None:\n",
    "        print(\"Model ID not found.\")\n",
    "    else:\n",
    "        print(\"Model ID fetched: \" + model_id)\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7efe2b-6be9-4aee-9e6b-b19cb485b720",
   "metadata": {},
   "source": [
    "### For a given model ID, get the corresponding OpenPages metrics definitions map containing metric ID and its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0baa1a-f7f7-49af-805c-5c7ec0656f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_op_model_metrics_definitions(header, model_id):\n",
    "    openpages_url = OP_URL.rstrip(\"/\") + \"/grc/api/query\"    \n",
    "    get_metrics_payload = {\n",
    "        \"statement\": \"SELECT [Metric].[Resource ID], [Metric].[Name], [Metric].[Description] FROM [Model] JOIN [Metric] ON PARENT([Model]) WHERE [Model].[Resource ID]='{0}'\".format(model_id),\n",
    "        \"skipCount\": 0\n",
    "    }\n",
    "    print(\"Sending request to fetch all metrics associated with the model.\")\n",
    "    response = requests.post(openpages_url, json=get_metrics_payload, headers=header, verify=False).json()\n",
    "\n",
    "    metrics_map = []\n",
    "\n",
    "    if response is not None:\n",
    "        if response.get(\"rows\") is not None:\n",
    "            rows = response.get(\"rows\")\n",
    "            if len(rows) != 0:\n",
    "                for i in range(len(rows)):\n",
    "                    fields = rows[i].get(\"fields\")\n",
    "                    if fields is not None:\n",
    "                        field = fields.get(\"field\")\n",
    "                        metric_id_desc = {}\n",
    "                        metric_id = None\n",
    "                        metric_desc = None\n",
    "                        for row in field:\n",
    "                            if row.get('name') == 'Resource ID':\n",
    "                                metric_id = row.get('value')\n",
    "                            if row.get('name') == 'Description':\n",
    "                                metric_desc = row.get('value')\n",
    "                        metric_id_desc['metric_desc'] = metric_desc\n",
    "                        metric_id_desc['metric_id'] = metric_id\n",
    "                        metrics_map.append(metric_id_desc)\n",
    "        print(\"Completed fetching, if any, all metrics associated with the model.\")\n",
    "        return metrics_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b29a45-4aab-47a1-8e9f-a1c2fee2fff9",
   "metadata": {},
   "source": [
    "### Construct the Metrics Object Payload for metrics creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee167db-2ecb-4335-b753-04c963895ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_object_payload(primaryParentId, metric_name):\n",
    "    metric_description = \"watsonx.governance metric for '\" + metric_name + \"'\"\n",
    "    metric_object_payload = {\n",
    "    \t\"name\": metric_name,\n",
    "    \t\"description\": metric_description,\n",
    "    \t\"typeDefinitionId\": \"Metric\",\n",
    "        \"primaryParentId\": primaryParentId,\n",
    "    \t\"fields\":\n",
    "    \t{\n",
    "    \t\t\"field\":\n",
    "    \t\t[\n",
    "    \t\t\t{\n",
    "                    \"name\": \"MRG-Metric:Data Source\",\n",
    "                    \"dataType\": \"STRING_TYPE\",\n",
    "                    \"value\": \"watsonx.governance\"\n",
    "                },\n",
    "                {\n",
    "            \t\t\"name\": \"MRG-Metric:Frequency\",\n",
    "            \t\t\"dataType\": \"ENUM_TYPE\",\n",
    "            \t\t\"enumValue\": {\n",
    "                \t\t\"name\": \"Multiple times a day\"\n",
    "                \t}\n",
    "            \t},\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Breach Status\",\n",
    "                    \"dataType\": \"ENUM_TYPE\",\n",
    "                    \"enumValue\": {\n",
    "                        \"name\": \"Green\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Direction Information\",\n",
    "                    \"dataType\": \"ENUM_TYPE\",\n",
    "                    \"enumValue\": {\n",
    "                        \"name\": \"Increase means better performance\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Yellow Threshold\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": 0.6\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Red Threshold\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": 0.5\n",
    "                },\n",
    "                \n",
    "    \t\t]\n",
    "    \t}\n",
    "    }\n",
    "    return metric_object_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fd68d-8153-4ace-8b48-0934511b61f3",
   "metadata": {},
   "source": [
    "### Construct the Metrics Value Payload for creating and associating a metric value to a metric of a given model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046690b0-0b12-43d9-9411-a43998c8b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_value_payload(primaryParentId, metric_name, metric_value):\n",
    "    metric_description = \"watsonx.governance metric for '\" + metric_name + \"'\"\n",
    "    metric_value_payload = {\n",
    "        \"typeDefinitionId\": \"MetricValue\",\n",
    "        \"primaryParentId\": primaryParentId,\n",
    "        \"description\": metric_description,\n",
    "        \"fields\": {\n",
    "            \"field\": [\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Breach Status\",\n",
    "                    \"dataType\": \"ENUM_TYPE\",\n",
    "                    \"enumValue\": {\n",
    "                        \"name\": \"Green\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Direction Information\",\n",
    "                    \"dataType\": \"ENUM_TYPE\",\n",
    "                    \"enumValue\": {\n",
    "                        \"name\": \"Increase means better performance\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Yellow Threshold\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": 0.6\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Red Threshold\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": 0.5\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-MetricVal:Value\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": metric_value\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    return metric_value_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e057c-3b93-44ae-9ff1-8908d94c3071",
   "metadata": {},
   "source": [
    "### Create a Metrics Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ff0eb-3ddb-422d-8b89-d52eaf14c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_object(metric_object_payload):\n",
    "    openpages_metric_object_creation_url = OP_URL + \"/grc/api/contents\"\n",
    "    response = requests.post(openpages_metric_object_creation_url, json=metric_object_payload, headers=header, verify=False).json()\n",
    "    metric_id = response['id']\n",
    "    return metric_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd866669-a8ce-4548-90a6-8a7cb7f06fd5",
   "metadata": {},
   "source": [
    "### Add Metric Value to the Metric Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3e73b-fa6a-44a8-873f-2333509daa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metric_value_to_metric_object(metric_value_payload):\n",
    "    openpages_metric_value_creation_url = OP_URL + \"/grc/api/contents\"\n",
    "    response = requests.post(openpages_metric_value_creation_url, json=metric_value_payload, headers=header, verify=False).json()\n",
    "    metric_value_id = response['id']\n",
    "    return metric_value_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed823be5-cbcc-4e25-9a76-9dc029707d69",
   "metadata": {},
   "source": [
    "### Check for the metric's existence in the metrics map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3700230-7023-480c-8381-b443dbbf7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_metric_id(metrics_map, metric_name):\n",
    "    for item in metrics_map:\n",
    "        if 'metric_desc' in item and metric_name in item['metric_desc']:\n",
    "            return item['metric_id']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee1ec6-2cee-44fb-833e-59f2323ccb43",
   "metadata": {},
   "source": [
    "### Configure your OpenPages connection details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35447cc7-1fc3-4376-8866-e5ad0860a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_URL = \"<OpenPages Url>\"\n",
    "OP_USERNAME = \"<OpenPages Username>\"\n",
    "OP_PASSWORD = \"<OpenPages User Password>\"\n",
    "model_name = '<OpenPages Model>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_basic_auth_token(OP_USERNAME, OP_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Basic {0}\".format(token)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a189dd-7857-469d-a802-7f3a806d5281",
   "metadata": {},
   "source": [
    "### Fetch the Model ID for a given OpenPages model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e38308-de1d-4a8b-a6fd-50739f2f830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = get_op_model_id(header, model_name)\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857fe25-acc6-4968-84c7-3f5d877a826d",
   "metadata": {},
   "source": [
    "### Publish the metrics to OpenPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb30dd7-ea9f-4f39-bd6e-4e70f96f0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetch the existing, if any, OP Model Metrics for a given OP Model ID\n",
    "metrics_map = get_op_model_metrics_definitions(header, model_id)\n",
    "print(metrics_map)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Iterate over the given metrics to be published..\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    \n",
    "    # check if the metric exists by the given name, and if, get its metric_id\n",
    "    metric_id = get_existing_metric_id(metrics_map, metric_name)\n",
    "\n",
    "    # if the metric does not exists, then create it\n",
    "    if metric_id is None:\n",
    "        print(metric_name + ': Metric Object does not exists, hence creating it..')\n",
    "\n",
    "        # construct the metric object to be published\n",
    "        metric_object_payload = get_metric_object_payload(model_id, metric_name)\n",
    "\n",
    "        # now, create the metric object\n",
    "        metric_id = create_metrics_object(metric_object_payload)\n",
    "\n",
    "    # Add the metric value to metric object\n",
    "\n",
    "    # construct the metric value object to be published\n",
    "    metric_value_payload = get_metric_value_payload(metric_id, metric_name, metric_value)\n",
    "\n",
    "    # create the metric value - basically add the metric value to the metric object\n",
    "    metric_value_id = add_metric_value_to_metric_object(metric_value_payload)\n",
    "    \n",
    "    print(str(metric_name) + ': Metric Object ID: ' + str(metric_id) + ', Metric Value Object ID: '+ str(metric_value_id) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8c17ebd",
   "metadata": {},
   "source": [
    "<a id=\"verify\"></a>\n",
    "## Step 5 - Navigate to the OpenPages UI to verify your metrics\n",
    "\n",
    "To locate your metrics, first navigate to your model:\n",
    " 1. From the **Menu** on the left, click on **Inventory**, and go to **Models**. \n",
    " 2. Choose your model from the list. \n",
    " 3. Go to the **Admin** tab:\n",
    " 4. To find your metrics, scroll down to **Associations**. They are under **Model Metrics**. \n",
    " 5. To view the metrics in a seperate tab, go to the icon in the top right corner and click **Launch Grid page**. You can see your metrics as shown in the following image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2262c33",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations, you successfully completed this notebook! You learned how to evaluate output from a Text Summarization prompt run against a Google Vertex AI LLM and publish the computed metrics to IBM OpenPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669fb47",
   "metadata": {
    "id": "e94e539d98704ac99e31c7d88b700140"
   },
   "source": [
    "### Author:\n",
    "\n",
    "**Ravi Chamarthy**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
