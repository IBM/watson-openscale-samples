{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97648ee8",
   "metadata": {
    "id": "a81be50301fa471b8417773161ab8ca8"
   },
   "source": [
    "# Use the IBM watsonx.governance metrics toolkit to evaluate watsonx.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899130e",
   "metadata": {},
   "source": [
    "The IBM watsonx.governance metrics toolkit lets you evaluate the output of a Large Language Model (LLM) against multiple task types: Text Summarization, Content Generation, Question Answering, Text Classification, Entity Extraction, and Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "This notebook will demonstrate how to evaluate output from a Text Summarization prompt run against an IBM watsonx.ai LLM. Evaluated metrics are then published to IBM OpenPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d7202",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Create your prompt for testing against the watsonx.ai model. \n",
    "-  Configure metrics for evaluation.\n",
    "-  Run the metrics against your prompt data.\n",
    "-  Print and review the metrics returned by the IBM watsonx.governance metrics toolkit. \n",
    "-  Publish the computed metrics to IBM OpenPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ba7c7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Install the necessary packages](#packages)\n",
    "2.  [Provision services and configure credentials](#credentials)\n",
    "3.\t[Evaluate Text Summarization output from the watsonx.ai model](#summarization)\n",
    "4.\t[Publish computed metrics to an OpenPages foundation model](#openpages)\n",
    "5.  [Navigate to the OpenPages UI to verify your metrics](#verify)\n",
    "6.\t[Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d4525",
   "metadata": {},
   "source": [
    "<a id=\"packages\"></a>\n",
    "## Step 1 - Install the necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae3a4e-8933-48fe-8add-7380726128da",
   "metadata": {
    "id": "6e9cd5a7-9437-47d0-a1fb-b3d6f963cec9"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson-machine-learning   | tail -n 1\n",
    "!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ddc47-aaec-4dae-b2d8-58c94093ebc3",
   "metadata": {},
   "source": [
    "**Note**: If you are running this notebook in a Jupyter environment in a Linux install, you must install the `yum install postgresql-devel` library by running the upgrade code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e42d76-0dce-4fd6-b73e-4c8a36d91dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-metrics-plugin --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade evaluate --no-cache | tail -n 1\n",
    "!pip install --upgrade rouge_score --no-cache | tail -n 1\n",
    "!pip install --upgrade textstat --no-cache | tail -n 1\n",
    "!pip install --upgrade sacrebleu --no-cache | tail -n 1\n",
    "!pip install --upgrade sacremoses --no-cache | tail -n 1\n",
    "!pip install --upgrade datasets==2.10.0 --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a0c26-584d-4817-ae94-a4c936a857f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy==1.4.47\n",
    "!pip install datasets==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e554a-b793-4a0d-a05c-1e08ba6c3dc7",
   "metadata": {
    "id": "12308564-2875-4b49-b788-85ece1964fe5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8203c66d",
   "metadata": {},
   "source": [
    "<a id=\"credentials\"></a>\n",
    "## Step 2 - Provision services and configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bd36b",
   "metadata": {
    "id": "4267839e32bd48719cf4b3389e3cadb8"
   },
   "source": [
    "### Provision an instance of IBM Watson OpenScale\n",
    "\n",
    "If you have not already done so, provision an instance of IBM Watson OpenScale using the [OpenScale link in the Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d77180",
   "metadata": {},
   "source": [
    "### Generate an API key\n",
    "\n",
    "You can generate a Cloud API key with IBM Cloud console or with IBM Cloud command line interface.\n",
    "\n",
    "To generate an API key by using IBM Cloud console:\n",
    "\n",
    "1. Go to the [**Users** section of the IBM Cloud console](https://cloud.ibm.com/iam#/users).\n",
    "1. Click your name, then scroll down to the **API Keys** section.\n",
    "1. Click **Create an IBM Cloud API key**.\n",
    "1. Give your key a name and click **Create**.\n",
    "1. Copy the created key - you will need to paste this key into the `CLOUD_API_KEY` variable in the \"Configure your credentials\" section below.\n",
    "\n",
    "To create an API key using the IBM Cloud [command line interface](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli):\n",
    "\n",
    "1. From the command line interface, type the following:\n",
    "\n",
    "    `bx login --sso`\n",
    "\n",
    "    `bx iam api-key-create 'my_key'`\n",
    "\n",
    "1. Copy the created key - you will need to paste this key into the `CLOUD_API_KEY` variable in the \"Configure your credentials\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c8fba",
   "metadata": {},
   "source": [
    "### Configure your watsonx.ai credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d96be6",
   "metadata": {
    "id": "f78243b20abd4b4d84313dacb4f02624"
   },
   "outputs": [],
   "source": [
    "GEN_API_KEY = \"<Your API Key>\"\n",
    "CLOUD_API_KEY = GEN_API_KEY\n",
    "api_endpoint = \"<IBM watsonx.ai model inferencing endpoint>\"\n",
    "project_id = \"<Your IBM watsonx.ai project id>\"\n",
    "endpoint_url = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec785b",
   "metadata": {},
   "source": [
    "If you are running your notebook on a CPD cluster, uncomment and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cpd = True\n",
    "# WOS_CREDENTIALS = {\n",
    "#     \"url\": \"xxxxx\",\n",
    "#     \"username\": \"xxxxx\",\n",
    "#     \"api_key\": \"xxxxx\"\n",
    "# }\n",
    "\n",
    "# GEN_API_KEY = WOS_CREDENTIALS[\"api_key\"]\n",
    "\n",
    "# api_endpoint = WOS_CREDENTIALS[\"url\"]\n",
    "# project_id = \"<Your project id>\"\n",
    "# endpoint_url = WOS_CREDENTIALS[\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e24f8",
   "metadata": {},
   "source": [
    "### Configure your OpenPages connection details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883033d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_URL = \"<OpenPages URL>\"\n",
    "OP_USERNAME = \"OpenPages Username\"\n",
    "OP_PASSWORD = \"OpenPages User Password\"\n",
    "model_name = 'OpenPages FM Model Id to which metrics needs to be published'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7783e6-04fe-4487-aea5-4fb853d9d5a0",
   "metadata": {
    "id": "abab31d97a0a460a90897da08f36add0"
   },
   "source": [
    "<a id=\"summarization\"></a>\n",
    "## Step 3 - Evaluate Text Summarization output from the watsonx.ai model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb74ee",
   "metadata": {},
   "source": [
    "### Download a dataset containing prompt input data for model inferencing and reference data for model output evaluation\n",
    "\n",
    "The downloaded `.csv` file contains: input, a generated summary, and two reference summaries each for 50 sample prompts. Values are then further converted to input, output and reference panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f284b78",
   "metadata": {
    "id": "8f63311275114169ac00dc3096352dd0"
   },
   "outputs": [],
   "source": [
    "!rm -fr llm_content.csv\n",
    "!wget \"https://raw.githubusercontent.com/ravichamarthy/custom_metrics/main/llm_content.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2169",
   "metadata": {
    "id": "c03c61b496f64edc8b9bd3fc3ff03899"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "llm_data_all = pd.read_csv(\"llm_content.csv\")\n",
    "llm_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data = llm_data_all.head(10)\n",
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216f5c4",
   "metadata": {},
   "source": [
    "### Create your prompt for testing against the watsonx.ai model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af80776",
   "metadata": {},
   "source": [
    "Use the following code to build the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe957522-3180-4d0f-89b1-5d425612a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2c721-7631-43f1-affd-e9d274dd0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_params = {\n",
    "    GenParams.MAX_NEW_TOKENS: 75,\n",
    "    GenParams.MIN_NEW_TOKENS: 10,\n",
    "    GenParams.TEMPERATURE: 0.0\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id=ModelTypes.FLAN_UL2,\n",
    "    params=generate_params,\n",
    "    credentials={\n",
    "        \"apikey\": GEN_API_KEY,\n",
    "        \"url\": endpoint_url\n",
    "    },\n",
    "    project_id=project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111326-629e-4a81-963c-ecd7ce78be84",
   "metadata": {},
   "source": [
    "### Run the prompt evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff972cd0",
   "metadata": {},
   "source": [
    "Evaluate the prompt that you built against the IBM watsonx.ai model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ec0b3-8e63-41cc-b72d-1caeee61507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(text):\n",
    "    prompt = f\"\"\"Please provide a summary of the following text with maximum of 20 words.\n",
    "    \n",
    "{text}\n",
    "    \n",
    "Summary:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a8bf9-002a-4222-86b3-293103cac959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(input_text):\n",
    "    prompt_text = get_prompt(input_text)\n",
    "    model_response = model.generate_text(prompt=prompt_text)\n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe805b94",
   "metadata": {},
   "source": [
    "### Set the generated prompt summary with the summary from the watsonx.ai prompt evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa0397-93e8-40bc-b0ee-a12c67abc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data['watsonx_ai_generated_summary'] = llm_data['input_text'].apply(get_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0b52a-f224-41c0-a900-6e4a20d41bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf28d2",
   "metadata": {},
   "source": [
    "#### Sample generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980af2c-58a3-4f6c-950b-3ebec6181708",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data['watsonx_ai_generated_summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1d906",
   "metadata": {
    "id": "d5d5b8b3dd4847298dee4b065ee9c3d4"
   },
   "source": [
    "### Authenticate with IBM watsonx.governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0ce8d",
   "metadata": {
    "id": "9567c5c157ae44ecabd9fdc434f16056"
   },
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator,BearerTokenAuthenticator,CloudPakForDataAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "if use_cpd:\n",
    "    authenticator = CloudPakForDataAuthenticator(\n",
    "            url=WOS_CREDENTIALS['url'],\n",
    "            username=WOS_CREDENTIALS['username'],\n",
    "            password=WOS_CREDENTIALS['password'],\n",
    "            disable_ssl_verification=True\n",
    "        )\n",
    "    \n",
    "    client = APIClient(service_url=WOS_CREDENTIALS['url'],authenticator=authenticator)\n",
    "    print(client.version)\n",
    "else:\n",
    "    authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\n",
    "    client = APIClient(authenticator=authenticator)\n",
    "    print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45542ea",
   "metadata": {
    "id": "d1aa94f70d4d4f77bc87a0b06139af3c"
   },
   "source": [
    "### Import common evaluation metrics and metric groups\n",
    "\n",
    "These are the metrics used to evaluate your prompt against the selected model, based on the prompt task type. For example, Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dc6b8",
   "metadata": {
    "id": "76620bd0a7784cb4801d10f3d93fa6a1"
   },
   "outputs": [],
   "source": [
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMTextMetricGroup\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMGenerationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMSummarizationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMQAMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import LLMClassificationMetrics\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import HAP_SCORE\n",
    "from ibm_metrics_plugin.metrics.llm.utils.constants import PII_DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db10c6",
   "metadata": {
    "id": "a68e9be2a0a0438d822679c5c1728097"
   },
   "source": [
    "### Get the necessary data for evaluating the prompt template metrics\n",
    "\n",
    "Metrics will be evaluated for the input, output, and reference summary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e28376-4d70-4477-978a-2181461c0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = llm_data[['input_text']].copy()\n",
    "df_output = llm_data[['watsonx_ai_generated_summary']].copy()\n",
    "df_reference = llm_data[['reference_summary_2']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7f842",
   "metadata": {
    "id": "74a9de9b2bb84c35b87fa1d2abc5770c"
   },
   "source": [
    "### Configure metrics for evaluation\n",
    "\n",
    "Select the metrics you want to evaluate; the below code cell contains 10 common Summarization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_config = {   \n",
    "    \"configuration\": {\n",
    "        LLMTextMetricGroup.SUMMARIZATION.value: {\n",
    "            LLMSummarizationMetrics.ROUGE_SCORE.value: {},\n",
    "            LLMSummarizationMetrics.SARI.value: {},\n",
    "            LLMSummarizationMetrics.METEOR.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_RECALL.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_PRECISION.value: {},\n",
    "            LLMSummarizationMetrics.NORMALIZED_F1_SCORE.value: {},\n",
    "            LLMSummarizationMetrics.COSINE_SIMILARITY.value: {},\n",
    "            LLMSummarizationMetrics.JACCARD_SIMILARITY.value: {},\n",
    "            LLMSummarizationMetrics.BLEU.value: {},\n",
    "            LLMSummarizationMetrics.FLESCH.value: {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240deb44",
   "metadata": {
    "id": "f55e2339070c48608474f370abd949b7"
   },
   "source": [
    "### Summarization metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c26ffc",
   "metadata": {},
   "source": [
    "Run the metrics against your prompt data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb037b-ff41-492f-ab4e-5fdb35d4da03",
   "metadata": {
    "id": "031a7f52-ba90-46d4-aac6-09d8f76e6d3f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = client.llm_metrics.compute_metrics(metric_config,df_input,df_output,df_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12d146",
   "metadata": {
    "id": "4bb4d7ebf78a4589b122508635c92572"
   },
   "source": [
    "### Review metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fd1da-329e-40e0-bce1-f99bb37dea3c",
   "metadata": {
    "id": "fd2fa6e4-c832-4ed6-bb10-8bae433e95f6"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(result,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c127e0",
   "metadata": {},
   "source": [
    "<a id=\"openpages\"></a>\n",
    "## Step 4 - Publish computed metrics to an OpenPages foundation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d8be4-b020-457e-a5ae-76a4da06c0d5",
   "metadata": {},
   "source": [
    "### Construct a key/value dictionary of the metrics to be published to OpenPages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(result):\n",
    "    metrics = {}\n",
    "    metrics['rouge1'] = round(result['rouge_score']['rouge1']['metric_value'], 4)\n",
    "    metrics['rouge2'] = round(result['rouge_score']['rouge2']['metric_value'], 4)\n",
    "    metrics['rougeL'] = round(result['rouge_score']['rougeL']['metric_value'], 4)\n",
    "    metrics['rougeLsum'] = round(result['rouge_score']['rougeLsum']['metric_value'], 4)\n",
    "    metrics['meteor'] = round(result['meteor']['metric_value'], 4)\n",
    "    metrics['sari'] = round(result['sari']['metric_value'], 4)\n",
    "    metrics['cosine_similarity'] = round(result['cosine_similarity']['metric_value'], 4)\n",
    "    metrics['jaccard_similarity'] = round(result['jaccard_similarity']['metric_value'], 4)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99873178-0736-4d5b-8647-81345dbd89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =  get_metrics(result)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fa64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf51499",
   "metadata": {},
   "source": [
    "### Get an authorization token for OpenPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_auth_token(username, password):\n",
    "    token = base64.b64encode(bytes('{0}:{1}'.format(username, password), 'utf-8')).decode(\"ascii\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74cb4e",
   "metadata": {},
   "source": [
    "### For a given model name, get the OpenPages model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac5ae914-598a-47a3-be05-b047b827ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_op_model_id(header, model_name):\n",
    "    openpages_url = OP_URL.rstrip(\"/\") + \"/grc/api/query\"\n",
    "    # Prepare post payload\n",
    "    get_id_payload = {\n",
    "        \"statement\": \"SELECT [Model].[Resource ID] FROM [Model] WHERE [Model].[Name] IN ('{0}')\".format(model_name),\n",
    "        \"skipCount\": 0\n",
    "    }\n",
    "    response = requests.post(openpages_url, json=get_id_payload, headers=header, verify=False).json()\n",
    "\n",
    "    model_id = None\n",
    "    if response is not None:\n",
    "        if response.get(\"rows\") is not None:\n",
    "            rows = response.get(\"rows\")\n",
    "            if len(rows) != 0:\n",
    "                fields = rows[0].get(\"fields\")\n",
    "                if fields is not None:\n",
    "                    field = fields.get(\"field\")\n",
    "                    if len(field) != 0:\n",
    "                        model_id = field[0][\"value\"]\n",
    "\n",
    "    if model_id is None:\n",
    "        print(\"Model ID not found.\")\n",
    "    else:\n",
    "        print(\"Model ID fetched: \" + model_id)\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b1d36",
   "metadata": {},
   "source": [
    "### For a given model ID, get the corresponding OpenPages metrics definitions map containing metric ID and its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91b977f6-64fc-4185-9707-04069534a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_op_model_metrics_definitions(header, model_id):\n",
    "    openpages_url = OP_URL.rstrip(\"/\") + \"/grc/api/query\"    \n",
    "    get_metrics_payload = {\n",
    "        \"statement\": \"SELECT [Metric].[Resource ID], [Metric].[Name], [Metric].[Description] FROM [Model] JOIN [Metric] ON PARENT([Model]) WHERE [Model].[Resource ID]='{0}'\".format(model_id),\n",
    "        \"skipCount\": 0\n",
    "    }\n",
    "    print(\"Sending request to fetch all metrics associated with the model.\")\n",
    "    response = requests.post(openpages_url, json=get_metrics_payload, headers=header, verify=False).json()\n",
    "\n",
    "    metrics_map = []\n",
    "\n",
    "    if response is not None:\n",
    "        if response.get(\"rows\") is not None:\n",
    "            rows = response.get(\"rows\")\n",
    "            if len(rows) != 0:\n",
    "                for i in range(len(rows)):\n",
    "                    fields = rows[i].get(\"fields\")\n",
    "                    if fields is not None:\n",
    "                        field = fields.get(\"field\")\n",
    "                        metric_id_desc = {}\n",
    "                        metric_id = None\n",
    "                        metric_desc = None\n",
    "                        for row in field:\n",
    "                            if row.get('name') == 'Resource ID':\n",
    "                                metric_id = row.get('value')\n",
    "                            if row.get('name') == 'Description':\n",
    "                                metric_desc = row.get('value')\n",
    "                        metric_id_desc['metric_desc'] = metric_desc\n",
    "                        metric_id_desc['metric_id'] = metric_id\n",
    "                        metrics_map.append(metric_id_desc)\n",
    "        print(\"Completed fetching, if any, all metrics associated with the model.\")\n",
    "        return metrics_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7441db",
   "metadata": {},
   "source": [
    "### Construct the Metrics Object Payload for metrics creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "584b0f66-31b0-4ca3-ae00-191af18ad677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_object_payload(primaryParentId, metric_name):\n",
    "    metric_description = \"watsonx.governance metric for '\" + metric_name + \"'\"\n",
    "    metric_object_payload = {\n",
    "    \t\"name\": metric_name,\n",
    "    \t\"description\": metric_description,\n",
    "    \t\"typeDefinitionId\": \"Metric\",\n",
    "        \"primaryParentId\": primaryParentId,\n",
    "    \t\"fields\":\n",
    "    \t{\n",
    "    \t\t\"field\":\n",
    "    \t\t[\n",
    "    \t\t\t{\n",
    "                    \"name\": \"MRG-Metric:Data Source\",\n",
    "                    \"dataType\": \"STRING_TYPE\",\n",
    "                    \"value\": \"watsonx.governance\"\n",
    "                },\n",
    "                {\n",
    "            \t\t\"name\": \"MRG-Metric:Frequency\",\n",
    "            \t\t\"dataType\": \"ENUM_TYPE\",\n",
    "            \t\t\"enumValue\": {\n",
    "                \t\t\"name\": \"Multiple times a day\"\n",
    "                \t}\n",
    "            \t}\n",
    "    \t\t]\n",
    "    \t}\n",
    "    }\n",
    "    return metric_object_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4b898",
   "metadata": {},
   "source": [
    "### Construct the Metrics Value Payload for creating and associating a metric value to a metric of a given model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11449656-b252-43e8-a668-c7aa10050a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_value_payload(primaryParentId, metric_name, metric_value):\n",
    "    metric_description = \"watsonx.governance metric for '\" + metric_name + \"'\"\n",
    "    metric_value_payload = {\n",
    "        \"typeDefinitionId\": \"MetricValue\",\n",
    "        \"primaryParentId\": primaryParentId,\n",
    "        \"description\": metric_description,\n",
    "        \"fields\": {\n",
    "            \"field\": [\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Breach Status\",\n",
    "                    \"dataType\": \"ENUM_TYPE\",\n",
    "                    \"enumValue\": {\n",
    "                        \"name\": \"Green\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-Metric-Shared:Red Threshold\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": 0.5\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MRG-MetricVal:Value\",\n",
    "                    \"dataType\": \"FLOAT_TYPE\",\n",
    "                    \"value\": metric_value\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    return metric_value_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f7791",
   "metadata": {},
   "source": [
    "### Create a Metrics Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be13053b-e43b-434b-9d7f-16f0bd77912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_object(metric_object_payload):\n",
    "    openpages_metric_object_creation_url = OP_URL + \"/grc/api/contents\"\n",
    "    response = requests.post(openpages_metric_object_creation_url, json=metric_object_payload, headers=header, verify=False).json()\n",
    "    metric_id = response['id']\n",
    "    return metric_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268756d4",
   "metadata": {},
   "source": [
    "### Add Metric Value to the Metric Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daa4cf64-8424-43a4-931d-6b9421e804b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metric_value_to_metric_object(metric_value_payload):\n",
    "    openpages_metric_value_creation_url = OP_URL + \"/grc/api/contents\"\n",
    "    response = requests.post(openpages_metric_value_creation_url, json=metric_value_payload, headers=header, verify=False).json()\n",
    "    metric_value_id = response['id']\n",
    "    return metric_value_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a1fe5",
   "metadata": {},
   "source": [
    "### Check for the metric's existence in the metrics map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e9c9304-0b9c-438e-8c5d-9ab31bd36541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_metric_id(metrics_map, metric_name):\n",
    "    for item in metrics_map:\n",
    "        if 'metric_desc' in item and metric_name in item['metric_desc']:\n",
    "            return item['metric_id']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e539d13",
   "metadata": {},
   "source": [
    "### Get your OpenScale authorization token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0607efb1-d327-4828-be05-9f3cf391dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_basic_auth_token(OP_USERNAME, OP_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Basic {0}\".format(token)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e988cb7",
   "metadata": {},
   "source": [
    "### Fetch the Model ID for a given OpenPages model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e6019-7319-49a4-8410-c7b828bee8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = get_op_model_id(header, model_name)\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187312ae",
   "metadata": {},
   "source": [
    "### Publish the metrics to OpenPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf081041-3772-4e4e-8654-c031ce3b258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetch the existing, if any, OP Model Metrics for a given OP Model ID\n",
    "metrics_map = get_op_model_metrics_definitions(header, model_id)\n",
    "print(metrics_map)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Iterate over the given metrics to be published..\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    \n",
    "    # check if the metric exists by the given name, and if, get its metric_id\n",
    "    metric_id = get_existing_metric_id(metrics_map, metric_name)\n",
    "\n",
    "    # if the metric does not exists, then create it\n",
    "    if metric_id is None:\n",
    "        print(metric_name + ': Metric Object does not exists, hence creating it..')\n",
    "\n",
    "        # construct the metric object to be published\n",
    "        metric_object_payload = get_metric_object_payload(model_id, metric_name)\n",
    "\n",
    "        # now, create the metric object\n",
    "        metric_id = create_metrics_object(metric_object_payload)\n",
    "\n",
    "    # Add the metric value to metric object\n",
    "\n",
    "    # construct the metric value object to be published\n",
    "    metric_value_payload = get_metric_value_payload(metric_id, metric_name, metric_value)\n",
    "\n",
    "    # create the metric value - basically add the metric value to the metric object\n",
    "    metric_value_id = add_metric_value_to_metric_object(metric_value_payload)\n",
    "    \n",
    "    print(str(metric_name) + ': Metric Object ID: ' + str(metric_id) + ', Metric Value Object ID: '+ str(metric_value_id) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad8552e0",
   "metadata": {},
   "source": [
    "<a id=\"verify\"></a>\n",
    "## Step 5 - Navigate to the OpenPages UI to verify your metrics\n",
    "\n",
    "To locate your metrics, first navigate to your model:\n",
    " 1. From the **Menu** on the left, click on **Inventory**, and go to **Models**. \n",
    " 2. Choose your model from the list. \n",
    " 3. Go to the **Admin** tab\n",
    " 4. To find your metrics, scroll down to **Associations**. They are under **Model Metrics**. \n",
    " 5. To view the metrics in a seperate tab, go to the icon in the top right corner and click **Launch Grid page**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd46321",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "Congratulations, you successfully completed this notebook! You learned how to evaluate output from a Text Summarization prompt run against an IBM watsonx.ai LLM and publish the computed metrics to IBM OpenPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e547ff",
   "metadata": {},
   "source": [
    "### Author:\n",
    "\n",
    "**Ravi Chamarthy**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
